{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a random name for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = string.ascii_lowercase\n",
    "\n",
    "def name():\n",
    "    return 'X_'+ ''.join(random.choices(cc,k=3)) + '.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_dti.html'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a random adjacency matrix\n",
    "\n",
    "we want the graph to be connected so \n",
    "\n",
    "- we start  with a [cycle](https://en.wikipedia.org/wiki/Cycle_graph#:~:text=In%20graph%20theory%2C%20a%20cycle,vertices%20is%20called%20Cn.)\n",
    "- add a random matrix of 1,0\n",
    "- clamp the values to 1,0\n",
    "\n",
    "the matrix is not necessarily symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 7\n",
    "M = np.identity(n,dtype =np.int)\n",
    "# this slides everything over by 1\n",
    "M = np.roll(M,1, axis=1)\n",
    "\n",
    "M = M + np.random.randint(2,size=(n,n))\n",
    "M[M>1] = 1\n",
    "\n",
    "# don't allow self references\n",
    "for k, r in enumerate(M):\n",
    "    r[k] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [name() for k in range(n)]\n",
    "lks = dict(zip(sites,M))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, ll in lks.items():\n",
    "    lks[x] = [ x for x,y in zip(sites,ll) if y >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lks = {'X_lzx.html': ['X_ask.html', 'X_hvc.html', 'X_lnh.html', 'X_dbo.html'],\n",
    " 'X_ask.html': ['X_lzx.html',\n",
    "  'X_ask.html',\n",
    "  'X_hvc.html',\n",
    "  'X_lnh.html',\n",
    "  'X_cgg.html',\n",
    "  'X_ifl.html'],\n",
    " 'X_hvc.html': ['X_hvc.html', 'X_lnh.html', 'X_cgg.html', 'X_dbo.html'],\n",
    " 'X_lnh.html': ['X_hvc.html', 'X_cgg.html', 'X_ifl.html'],\n",
    " 'X_cgg.html': ['X_ask.html', 'X_hvc.html', 'X_lnh.html', 'X_dbo.html'],\n",
    " 'X_dbo.html': ['X_ask.html', 'X_hvc.html', 'X_lnh.html', 'X_ifl.html'],\n",
    " 'X_ifl.html': ['X_lzx.html', 'X_hvc.html', 'X_lnh.html', 'X_cgg.html']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, ll in lks.items():\n",
    "    with open(x,'w') as fp:\n",
    "        fp.write('# ' + x + '<br>\\n'*3 )\n",
    "        body = ''.join(['<a href=\"./{}\"> {}</a> <br>\\n'.format(y,y) for  y in ll])\n",
    "        fp.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 3057a2c] web\n",
      " 1 file changed, 61 insertions(+), 37 deletions(-)\n",
      "Counting objects: 4, done.\n",
      "Delta compression using up to 12 threads.\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 911 bytes | 911.00 KiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/macbuse/macbuse.github.io.git\n",
      "   2c602f2..3057a2c  master -> master\n"
     ]
    }
   ],
   "source": [
    "! ../.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./X_hvc.html', './X_lnh.html', './X_cgg.html', './X_dbo.html']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "pax = re.compile('<a href=\"(.*?)\">', re.DOTALL)\n",
    "\n",
    "base = 'https://macbuse.github.io/PROG/'\n",
    "root = 'X_aoy.html'\n",
    "\n",
    "todo = [root]\n",
    "done, nodes = [], {}\n",
    "\n",
    "while todo:\n",
    "    parent = todo.pop()\n",
    "    if parent in done: continue\n",
    "    done.append(parent)\n",
    "    r = requests.get(base + parent)\n",
    "    children = [ x[2:] for x in pax.findall(r.text)]\n",
    "    nodes[parent] = children\n",
    "\n",
    "    todo.extend(children)\n",
    "    todo = list(set(todo))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_aoy.html': ['X_bqe.html', 'X_lbv.html', 'X_suk.html', 'X_wwv.html'],\n",
       " 'X_suk.html': ['X_bqe.html', 'X_aoy.html', 'X_wwv.html'],\n",
       " 'X_wwv.html': ['X_fyp.html', 'X_lbv.html', 'X_mpn.html', 'X_aoy.html'],\n",
       " 'X_bqe.html': ['X_fyp.html',\n",
       "  'X_lbv.html',\n",
       "  'X_suk.html',\n",
       "  'X_aoy.html',\n",
       "  'X_wwv.html'],\n",
       " 'X_lbv.html': ['X_mpn.html'],\n",
       " 'X_fyp.html': ['X_bqe.html', 'X_suk.html'],\n",
       " 'X_mpn.html': ['X_suk.html']}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(nodes.keys())\n",
    "\n",
    "n = len(pages)\n",
    "M = np.zeros((n,n))\n",
    "\n",
    "for k, p in enumerate(pages):\n",
    "    M[k, [pages.index(x) for x in nodes[p] ] ]  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia trail\n",
    "\n",
    "- Look at the page\n",
    "[Wikipedia:Getting_to_Philosophy](https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy)\n",
    "on the right you can see a script doing the crawl.\n",
    "- Do a Google search for the script ```scraper.py```\n",
    "\n",
    "You should get to [this page](https://medium.com/tech-insider/task-crawling-first-link-of-wikipedia-recursively-to-reach-the-philosophy-page-9f5bcc770a60).\n",
    "Now copy the script and try to get it to work!!!\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Web pages are created from databases by robots.\n",
    "Sometimes the format of the web page changes \n",
    "and so the crawler won't find what you are looking for.\n",
    "This script no longer works but it should be easy\n",
    "for you to fix it.\n",
    "\n",
    "---\n",
    "\n",
    "What-back-end-infrastructure for [wikipedia](https://www.quora.com/What-back-end-infrastructure-technologies-does-Wikipedia-use)\n",
    "\n",
    "Probably Wikimedia servers is a good place to start looking at how the backend works. Its pretty complex. Some of the components include\n",
    "\n",
    "- Master SQL database running MariaDB.\n",
    "- Several Slave databases also running MariaDB.\n",
    "- Application Servers running Apache and MediaWiki.\n",
    "- HTTP Cache servers which serve pre compiled pages. Running Varnish HTTP Cache\n",
    "- Load balancing systems. running LVS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://en.wikipedia.org/wiki/Species\n",
      "https://en.wikipedia.org/wiki/Biology\n",
      "https://en.wikipedia.org/wiki/Natural_science\n",
      "https://en.wikipedia.org/wiki/Branches_of_science\n",
      "https://en.wikipedia.org/wiki/Science\n",
      "https://en.wikipedia.org/wiki/Latin\n",
      "https://en.wikipedia.org/wiki/Help:IPA/Latin\n",
      "https://en.wikipedia.org/wiki/International_Phonetic_Alphabet\n",
      "https://en.wikipedia.org/wiki/Alphabet\n",
      "https://en.wikipedia.org/wiki/Symbols\n",
      "https://en.wikipedia.org/wiki/Word\n",
      "https://en.wikipedia.org/wiki/Linguistics\n",
      "We've arrived at an article we've already seen, aborting search!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import urllib\n",
    "\n",
    "#import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "target_url = \"https://en.wikipedia.org/wiki/Narendra_Modi\"\n",
    "\n",
    "# I changed so I don't use bs4\n",
    "pp = re.compile('<p>(.*?)</p>', re.DOTALL)\n",
    "pax = re.compile(r'<a href=\"(.*?)\"', re.DOTALL)\n",
    "\n",
    "def find_first(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    \n",
    "    paras = pp.findall(html)\n",
    "    if not paras: return\n",
    "    \n",
    "    lks = pax.findall(''.join(paras))\n",
    "    if not lks: return\n",
    "    \n",
    "    article_link = lks[0]\n",
    "    \n",
    "    #I wouldn't import urllib just to do this\n",
    "    first_link = urllib.parse.urljoin('https://en.wikipedia.org/', article_link)\n",
    "    return first_link\n",
    "\n",
    "def continue_crawl(search_history, target_url, max_steps=25):\n",
    "    if search_history[-1] == target_url:\n",
    "        print(\"We've found the target article!\")\n",
    "        return False\n",
    "    elif len(search_history) > max_steps:\n",
    "        print(\"The search has gone on suspiciously long, aborting search!\")\n",
    "        return False\n",
    "    elif search_history[-1] in search_history[:-1]:\n",
    "        print(\"We've arrived at an article we've already seen, aborting search!\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "article_chain = [start_url]\n",
    "\n",
    "while continue_crawl(article_chain, target_url):\n",
    "    print(article_chain[-1])\n",
    "\n",
    "    first_link = find_first(article_chain[-1])\n",
    "    if not first_link:\n",
    "        print(\"We've arrived at an article with no links, aborting search!\")\n",
    "        break\n",
    "\n",
    "    article_chain.append(first_link)\n",
    "\n",
    "    time.sleep(2) # slow down otherwise wiki server will block you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = start_url\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Vine',\n",
       " '/wiki/Apocynaceae',\n",
       " '/wiki/Indomalaya',\n",
       " '/wiki/Australasia',\n",
       " '/wiki/Melanesia',\n",
       " '#cite_note-u-1',\n",
       " '/wiki/Leaf',\n",
       " '#cite_note-NSW-2',\n",
       " '/wiki/Latex',\n",
       " '#cite_note-NSW-2',\n",
       " '#cite_note-NSW-2',\n",
       " '/wiki/Robert_Brown_(botanist,_born_1773)',\n",
       " '/wiki/Wernerian_Natural_History_Society',\n",
       " '#cite_note-APNI-3',\n",
       " '/wiki/James_Parsons_(physician)',\n",
       " '/wiki/Royal_Society',\n",
       " '#cite_note-4',\n",
       " '/wiki/Conserved_name',\n",
       " '/wiki/Homonym',\n",
       " '/wiki/Patrick_Browne',\n",
       " '/wiki/Lythraceae',\n",
       " '/wiki/Cuphea',\n",
       " '#cite_note-GRIN-5',\n",
       " '/wiki/Artia_(plant)',\n",
       " '/wiki/Prestonia_(plant)',\n",
       " '#cite_note-6',\n",
       " '#cite_note-7',\n",
       " '#cite_note-u-1',\n",
       " '#cite_note-18',\n",
       " '#cite_note-FONZ-11']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = ''.join(pp.findall(html))\n",
    "pax.findall(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 96adb8d] web\n",
      " 2 files changed, 427 insertions(+), 44 deletions(-)\n",
      "Counting objects: 5, done.\n",
      "Delta compression using up to 12 threads.\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 3.49 KiB | 3.49 MiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/macbuse/macbuse.github.io.git\n",
      "   6d7c30b..96adb8d  master -> master\n"
     ]
    }
   ],
   "source": [
    "! ../.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
