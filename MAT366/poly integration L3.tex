\documentclass[11pt,a4paper]{article}
\usepackage{amsmath,amssymb,amsthm,graphics}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,mathabx}
\usepackage{oldgerm}
\usepackage{mathtools}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{enumitem}
\usepackage[shortlabels,inline]{enumitem}
%\usepackage{stmaryrd}

\usepackage{tikz} % Importation de TikZ
\usetikzlibrary{math} % Pour utiliser \tikzmath
\usepackage{pgfplots}

\usepackage{t1enc}
\pagestyle{empty}

\newcommand{\rk}[1]{\textbf{\color{red}#1}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\real}{\RR}
\newcommand{\R}{\RR}

\newcommand{\indic}{{\it Indication : }}

\def\ds{\displaystyle}

\newenvironment{preuve}[1][]
{\vskip 2mm  \noindent\emph{\bf Proof#1. }}{$\Box$ \vskip 2mm}

\setlength{\textwidth}{17.6cm} \setlength{\evensidemargin}{-0.4cm}
\setlength{\oddsidemargin}{-0.4cm} \setlength{\textheight}{21cm}
\setlength{\topmargin}{-1cm}

\def\Jac{\mathrm{Jac}}


\everymath{\displaystyle}

\let\geq\geqslant
\let\leq\leqslant
\let\ge\geqslant
\let\le\leqslant

\newcommand{\sgn}{\hbox{\rm sgn}\,}

\newcommand{\Class}{C}

\newcommand{\de}{\mathrm{d}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
%\newcommand{\R}{\mathbb{R}}
%\newcommand{\real}{\R}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\corps}{\mathbb{K}}

\def\M{\mbox{M}}
\newcommand{\vect}{\ensuremath{\mathrm{vect\,}}}
\newcommand{\Ker}{\ensuremath{\mathrm{Ker\,}}}
\newcommand{\tr}{\ensuremath{\mathrm{tr\,}}}
\newcommand{\Sp}{\ensuremath{\mathrm{Sp\,}}}

\def\soluce#1{}


\newcounter{exercice}
\def\exo#1{\vspace{.2cm}\textbf{Exercice\addtocounter{exercice}{1}
		\arabic{exercice} {\it #1}} }
\setcounter {exercice}{0}

\topmargin-1cm
\oddsidemargin0cm%-0.4mm
\textwidth15cm
\textheight24cm
\parskip=3pt
\parindent=0pt

\usepackage[colorlinks]{hyperref}

\begin{document}
	
	{\bfseries
		\noindent Université Grenoble Alpes\hfill Année 2024-2025\break
		Licence de mathématiques, 3e année\hfill \break  Parcours mathématiques \hfill\break
		
		\vskip 5pt
		\centerline{\LARGE{Exercices de théorie de la mesure \rk{sûr ?} MAT366}}
		
	}
	\vskip5mm
	
	\tableofcontents

\newpage \section{Révisions}


\subsection{Fonctions élémentaires}

\exo{La fonction exponentielle)}
La fonction exponentielle est définie comme la (seule) fonction $f : \R \rightarrow \R$ différentiable qui satisfait que $f' = f$
et $f(0) = 1$. 
\begin{enumerate}
\item Soit $f$ comme ci-dessus. Montrer que $f(x) f(-x) = 1$, pour tout $x \in \R$. 
En particulier, $f(x) \neq 0$ et $f(-x) = f(x)^{-1}$, pour tout $x \in \R$. 
\\
\textbf{Indication :} calculer la dérivée de l'application $x \mapsto f(x) f(-x)$. 

\item Soit $g: \R \rightarrow \R$ une fonction différentiable qui satisfait que $g' = g$. 
Montrer qu'il existe $C \in \R$ tel que $g(x) = C f(x)$, pour tout $x \in \R$. 
En déduire l'unicité de $f$. 
\\
\textbf{Indication :} considérer la fonction $x \mapsto g(x)/f(x)$. 

\item En déduire que $f''(x) = f'(x) = f(x) > 0$, pour tout $x \in \R$. 
En particulier, $f$ est strictement croissante et concave. 

\item Montrer que $f(x+y) = f(x) f(y)$, pour tous $x, y \in \R$. 
En déduire que 
\[     f(n x) = f(x)^{n},     \] 
pour tout $n \in \Z$. 
\\
\textbf{Indication :} considérer la fonction $g(x) = f(x + y)$, avec $y$ fixe. 

\item On définit $e = f(1)$. 
Noter que $e = f(1) > f(0) = 1$. 
On écrira en plus $f(x) = e^{x}$. 
Montrer que la série dans  
\begin{equation}
\label{eq:exp}
\tag{EXP}
     x \mapsto \sum_{n=0}^{\infty} \frac{x^{n}}{n!}
\end{equation}          
converge uniformément sur tout intervalle borné de $\R$ et définit par conséquent une fonction différentiable $\operatorname{exp}$. 
Comme $\operatorname{exp}' = \operatorname{exp}$ et $\operatorname{exp}(0) = 1$, elle coïncide avec 
la fonction exponentielle. 
Cela démontre l'existence. 
En particulier, 
\begin{equation}
\label{eq:e}
\tag{E}
     e = \sum_{n=0}^{\infty} \frac{1}{n!}.
\end{equation}    
\end{enumerate}

\begin{preuve}
La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

\exo{Les fonctions trigonométriques)}
Les fonctions \emph{sinus} et \emph{cosinus} sont définies comme les (seules) fonctions $f, g : \R \rightarrow \R$ différentiables qui satisfont que $f' = g$, $g' = -f$, $f(0) = 0$ et $g(0) = 1$, respectivement. 
%On admettra leur existence. 
\begin{enumerate}
\item Soient $f$ et $g$ comme ci-dessus. Montrer que $f(x)^{2} + g(x)^{2} = 1$, pour tout $x \in \R$. 
\\
\textbf{Indication :} calculer la dérivée de l'application $x \mapsto f(x)^{2} + g(x)^{2}$. 

\item Soient $f_{1}, g_{1}: \R \rightarrow \R$ des fonction différentiables qui satisfont que $f'_{1} = g_{1}$, $g'_{1} = -f_{1}$. 
Montrer qu'il existe $a, b \in \R$ tels que 
\begin{equation}
\label{eq:scgen}
   f_{1} = b f - a g \text{ et } g_{1} = a f + b g.
\end{equation}          
En déduire l'unicité de $f$ et de $g$, que l'on appelle \emph{sinus} et \emph{cosinus}, et que l'on note $\sin$ et $\cos$, respectivement. 
\\
\textbf{Indication :} montrer que les dérivées des fonctions $f g_{1} - f_{1} g$ et $f f_{1} + g g_{1}$ valent zéro. 
Après, calculer la différence entre le produit de la première expression par $f$ et le produit de la deuxième par $g$. 

\item Montrer que $\sin(-x) = - \sin(x)$ et $\cos(-x) = \cos(x)$, pour tout $x \in \R$. 
\\
\textbf{Indication :} utiliser \eqref{eq:scgen} avec $f_{1}(x) = \cos(-x)$ et $g_{1}(x) = \sin(-x)$. 

\item Montrer que 
\begin{align*}     
   \sin(x+y) &= \sin(x) \cos(y) + \cos(x) \sin(y), 
   \\
   \cos(x+y) &= \cos(x) \cos(y) - \sin(x) \sin(y),     
\end{align*}   
pour tous $x, y \in \R$. 
\\
\textbf{Indication :} utiliser \eqref{eq:scgen} avec $f_{1}(x) = \sin(x+y)$ et $g_{1}(x) = \cos(x+y)$, où $y$ est fixe. 

\item Montrer que les séries dans
\begin{equation}
\label{eq:sc}
\tag{SC}
     x \mapsto \sum_{n=0}^{\infty} (-1)^{n} \frac{x^{2n+1}}{(2n+1)!} \text{ et } x \mapsto \sum_{n=0}^{\infty} (-1)^{n} \frac{x^{2n}}{(2n)!}
\end{equation}          
convergent uniformément sur tout intervalle borné de $\R$ et définissent par conséquent des fonctions différentiables $\operatorname{SIN}$ et $\operatorname{COS}$. 
Elles vérifient les conditions $\operatorname{SIN}' = \operatorname{COS}$, $\operatorname{COS}' = -\operatorname{SIN}$, $\operatorname{SIN}(0) = 0$ et $\operatorname{COS}(0) = 1$, donc elles coïncident avec les fonctions sinus et cosinus, respectivement. 
Cela démontre l'existence.
\end{enumerate}

\begin{preuve}
La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

\exo{La définition de $\pi$)}
On définit $\pi \in \R$ comme le plus petit nombre positif qui satisfait que $\cos(\pi/2) = 0$. 
Le but de cet exercice est de démontrer que ce nombre existe. 
\begin{enumerate}
\item Supposons qu'il n'existe aucun $x > 0$ tel que $\cos(x/2) = 0$. 
Comme $\cos(0)=1$, $\cos(x) > 0$, pour tout $x \in \R_{>0}$. 
En déduire que $\sin$ (resp., $\cos$) est une fonction strictement croissante  (resp., décroissante) sur $\R_{>0}$. 

\item On fixe $a > 0$. 
D'après l'item précédent, $0 < \cos(a) < 1$. 
Noter que
\[     0 < \cos (2 a) = \cos^{2}(a) - \sin^{2}(a) < \cos^{2}(a)     \]
et en particulier $\cos(2^{n} a) < (\cos (a))^{2^{n}}$, pour tout $n \in \N$. 
En déduire que la suite $(\cos(2^{n}a))_{n \in \N}$ converge vers $0$ et, en conséquence, $(\sin(2^{n}a))_{n \in \N}$ converge vers $1$. 

\item Montrer que $\cos(x)$ (resp., $\sin(x)$) converge vers $0$ (resp., $1$) quand $x$ tend vers $+\infty$. 
En particulier, il existe $b > 0$ tel que $\cos(b) < 1/4$ et $\sin(b) > 1/2$. 
En déduire que $\cos (2b) = \cos^{2} (b) - \sin^{2} (b) < 0$, pour trouver un absurde. 
Conclure qu'il existe $x> 0$ tel que $\cos(x/2) = 0$. 

\item Noter que l'ensemble $\mathcal{P} = \{ x > 0 : \cos(x/2) = 0\}$ est non vide et minoré. 
On définit $\pi = \inf \mathcal{P}$. 
Montrer que $\pi \in \mathcal{P}$. 
\end{enumerate}

\begin{preuve}
La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

\subsection{Primitives basiques}

%%%%%%%%%%%%%%%
\exo{Primitives usuelles)} 
Compléter le tableau suivant
\begin{center}
\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|c|c|}
\hline
fonction $f$ & une primitive de $f$ & ensemble de définition \\ \hline
$x^{\alpha},\ \alpha \in \R \setminus \{ -1\}$ &\   & \   \\ \hline
$1/x$& &\\ \hline
$1/(ax+b),\ a \in \R \setminus \{ 0 \}$& &\\ \hline
$1/(a^2+x^2),\ a \in \R \setminus \{ 0 \}$& &\\ \hline
$1/\sqrt{a^2-x^2},\ a \in \R \setminus \{ 0 \}$& &\\ \hline
 $e^{\lambda x},\ \lambda \in \R \setminus \{ 0 \}$ &  &   \\ \hline
 $\cos (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
 $\sin (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
 $\cosh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
  $\sinh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
   $\tan (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
   $\tanh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& &\\ \hline
\end{tabular}
\end{center}

\begin{preuve}
\begin{center}
\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|c|c|}
\hline
fonction $f$ & une primitive de $f$ & ensemble de définition \\ \hline
$x^{\alpha},\ \alpha \in \R \setminus \{ -1\}$ & $x^{\alpha+1}/(\alpha+1)$   &  $\R_{>0}$ ($\R$, si $\alpha \in \Z_{\geq 0}$,   \\ 
&&  $\R \setminus \{ 0\}$, si $\alpha \in \Z_{<-1}$\\
&& et $\R_{\geq 0}$, si $\alpha \in ]-1,+\infty[ \setminus \Z$) \\ \hline
$1/x$& $\ln(|x|)$ & $\R \setminus \{ 0 \}$ \\ \hline
$1/(ax+b),\ a \in \R \setminus \{ 0 \}$& $\frac1a\ln(|ax+b|)$ & $\R \setminus \{-b/a\}$\\ \hline
$1/(a^2+x^2),\ a \in \R \setminus \{ 0 \}$& $\arctan(x/a)/a$ & $\R$ \\ \hline
$1/\sqrt{a^2-x^2},\ a \in \R \setminus \{ 0 \}$& $\arcsin(x/|a|)$ & $]-|a|,|a|[$\\ \hline
 $e^{\lambda x},\ \lambda \in \R \setminus \{ 0 \}$ & $e^{\lambda x}/\lambda$ & $\R$  \\ \hline
 $\cos (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $\sin (\omega x+a)/\omega$ & $\R$ \\ \hline
 $\sin (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $- \cos (\omega x+a)/\omega$ & $\R$ \\ \hline
 $\cosh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $\sinh (\omega x+a)/\omega$ & $\R$ \\ \hline
  $\sinh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $\cosh (\omega x+a)/\omega$ & $\R$ \\ \hline
   $\tan (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $- \ln(|\cos(\omega x + a)|)/\omega$ & $\R \setminus \{\frac1\omega(\frac\pi2+k\pi -a),\ k\in \Z \} $\\ \hline
   $\tanh (\omega x+a),\ \omega \in \R \setminus \{ 0 \}$& $\ln(\cosh(\omega x + a))/\omega$ & $\R$ \\ \hline
\end{tabular}
\end{center}
\end{preuve}

%%%%%%%%%%%%

\exo{} Soit $a \in \R$ et $n\in\Z$. Calculer
\[     \int \frac{dx}{(x-a)^n}.     \]
\begin{preuve}
C'est clair que, si $n \neq 1$, alors
\[     \int \frac{dx}{(x-a)^n} = -\frac{1}{(n-1)(x-a)^{(n-1)}}+C,     \] 
et si $n = 1$, alors 
\[     \int \frac{dx}{(x-a)^n} = \ln(|x-a|)+C,     \]
avec domaine de définition $\R \setminus \{ a \}$, où $C \in \R$ dénote une constante quelconque. 
\end{preuve}


%%%%%%%%%%
\exo{} Calculer les primitives et donner leur domaine de définition des fonctions suivantes :
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int x\sqrt{x^2+4}dx$,
\item $\int \frac{x}{\sqrt[3]{x^2+1}}dx$, 
\item $\int x e^{-x^2}dx$,
\item $\int \frac{dx}{x^2+16}$,  \item $\int\frac{e^xdx}{1+e^{2x}}$,  \\
\item $\int\frac{xdx}{\sqrt{1-x^4}}$, 
\item $\int\frac{dx}{\sqrt{1-x}}$, 
\item $\int\frac{e^xdx}{\sqrt{16-e^{2x}}}$. 
\end{enumerate*}
\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item $\int x\sqrt{x^2+4}dx =  \sqrt{(x^{2}+4)^3}/3 + C$, avec domaine de définition $\R$. 
\item $\int \frac{x}{\sqrt[3]{x^2+1}}dx =\frac34 \sqrt[3]{(x^2+1)^{2}} + C$, avec domaine de définition $\R$.
\item $\int x e^{-x^2}dx = -e^{-x^{2}}/2 + C$, avec domaine de définition $\R$.
\item $\int \frac{dx}{x^2+16} =  \operatorname{arctan}(x/4)/4 + C$, avec domaine de définition $\R$. 
\item $\int\frac{e^xdx}{1+e^{2x}} = \operatorname{arctan}(e^{x}) + C$, avec domaine de définition $\R$.
\item $\int\frac{xdx}{\sqrt{1-x^4}} = \operatorname{arcsin}(x^{2})/2 + C$, avec domaine de définition $\hskip 0.6mm ] \hskip 0.6mm -1, 1\hskip 0.6mm[\hskip 0.6mm$.
\item $\int\frac{dx}{\sqrt{1-x}}=  -2 \sqrt{1-x} + C$, avec domaine de définition $\R_{<1}$. 
\item $\int\frac{e^xdx}{\sqrt{16-e^{2x}}} = \operatorname{arcsin}(e^{x}/4) + C$, avec domaine de définition $\R_{< \ln(4)}$.
\end{enumerate}
\end{preuve}

%\newpage

\subsection{Calcul de primitives} 

\noindent\textbf{Pour chaque exercice suivant on calculera une formule exacte pour les primitives des fonctions données et on donnera leur domaine de définition.}

%%%%%%%%%%
\exo{Intégration par parties)}

Écrire la formule d'intégration par parties. 

\begin{preuve} Si $f$ et $g$ sont deux fonctions définies sur (un intervalle de) $\R$ de classe $C^{1}$, alors 
\[     \int f(x) g'(x) dx = f(x) g(x) - \int f'(x) g(x) dx.     \]
\end{preuve}

%%%%%%%%%%%%
\exo{} Calculer
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int x^n\ln(x)dx$ ($n \in \Z$), 
\item $\int e^{-x} \cos^{2} (x) dx$,
\item $\int \arctan(\sqrt{1-x^2})dx$.	
\end{enumerate*}

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item Si $n \neq -1$, on applique une intégration par parties avec $u = \ln(x)$ et $v' = x^{n}$, \textit{i.e.} $v = x^{n+1}/(n+1)$: 
\[     \int x^n\ln(x)dx =  \ln(x)\frac{x^{n+1}}{n+1}- \int \frac{x^{n}}{n+1}dx =
\frac{x^{n+1} \big( (n+1) \ln(x) - 1\big)}{(n+1)^{2}} + C,     \] 
avec domaine de définition $\R_{>0}$ .

Si $n=-1$, on applique aussi une intégration par parties avec $u = \ln(x)$ et $v' = 1/x$, \textit{i.e.} $v = \ln x$: 
\[     \int \frac{\ln(x)}{x}dx = \ln^2 x - \int \frac{\ln(x)}{x}dx \Rightarrow \int \frac{\ln(x)}{x}dx = \frac{\ln^{2}(x)}{2} + C,     \]
avec domaine de définition $\R_{>0}$.
\item Si l'on utilise $\cos^{2} (x) = (1 + \cos(2x))/2$, on trouve que 
\[
 \int e^{-x} \cos^{2} (x) dx = \frac{1}{2} \bigg( \int e^{-x} dx + \int e^{-x} \cos (2x) dx \bigg).
\]
Avec deux int\'egrations par parties, on calcule
\begin{align*}
     \int e^{-x}\cos (2x) dx&=  -e^{-x} \cos (2x) - 2\int e^{-x} \sin(2x) dx\\
      &= -e^{-x} \cos (2x) +2e^{-x} \sin (2x) -4\int e^{-x} \cos(2x) dx
\end{align*}     
d'o\`u
\[
\int e^{-x}\cos (2x) dx = \frac{-e^{-x} \cos (2x) +2e^{-x} \sin (2x)}5
\]
et on conclue
\[      
 \int e^{-x} \cos^{2} (x) dx     = - \frac{e^{-x}}{10} \big(5+\cos(2x)-2 \sin(2x)\big) + C,
\]
avec domaine de définition $\R$.

\item Si l'on fait une intégration par parties avec $u = \arctan(\sqrt{1-x^2})$ et $v' = 1$ (\textit{i.e.} $v = x$), on trouve
\[     \int \arctan(\sqrt{1-x^2})dx= x \arctan(\sqrt{1-x^2}) + \int \frac{x^{2} dx}{(2-x^{2}) \sqrt{1 - x^{2}}}.     \]
Si l'on fait la substitution $x = \sqrt{2} z/\sqrt{1+2 z^{2}}$ (\textit{i.e.} $z = x/\sqrt{2-2x^{2}}$) , la dernière intégrale devient 
\begin{align*}     
\int &\frac{x^{2} dx}{(2-x^{2}) \sqrt{1 - x^{2}}} = \sqrt{2} \int \frac{z^{2} dz}{(1+z^{2})(1+2z^{2})} = \sqrt{2} \int dz\bigg(\frac{1}{z^{2}+1}-\frac{1}{2z^{2}+1} \bigg) 
\\
&= \sqrt{2} \arctan(z) - \arctan (\sqrt{2} z) + C 
\\
&= \sqrt{2} \arctan\bigg(\frac{x}{\sqrt{2-2 x^2}}\bigg) - \arctan\bigg(\frac{x}{\sqrt{1-x^2}}\bigg) + C.
\end{align*} 
Si l'on utilise l'identité $\tan(\arcsin(x)) = x/\sqrt{1-x^{2}}$, alors  
\[     \int \arctan(\sqrt{1-x^2})dx= \sqrt{2} \arctan\bigg(\frac{x}{\sqrt{2-2 x^2}}\bigg) + x \arctan(\sqrt{1-x^2}) - \arcsin(x) + C     \]
avec domaine de définition $\hskip 0.6mm ]\hskip 0.6mm -1,1\hskip 0.6mm[\hskip 0.6mm$.
\end{enumerate}
\end{preuve}


%%%%%%%%%%
\subsection{(Fractions rationnelles)}
La méthode est de décomposer la fraction rationnelle en éléments simples. 
On est donc ramené à calculer des intégrales de l'un de ces deux types :
\[\int \frac{1}{(x-a)^n}dx,\ \ \int \frac{ax+b}{(x^2+cx+d)^n}dx,\ \text{ avec } n \in \N^{*} \text{ et } c^2-4d<0.\]


%%%%%%%%%%%%
\exo{}
\label{exo:17} 
Pour $n\in\N$, on définit la fonction $I_{n} : \R \rightarrow \R$ via
\[     I_{n}(x) = \int \frac{dx}{(1+x^2)^n}     \]
telle que $I_{n}(0) = 0$. 
Noter que $I_{0}(x) = x$.  
Montrer que, pour tout entier $n \geq 2$, 
\[     I_{n}(x) = \frac{x}{2(n-1) (1+x^{2})^{n-1}} + \frac{2n-3}{2n-2} I_{n-1}(x).     \]

\begin{preuve}
Soit $n\in\N^{*}$. On int\`egre par partie $u=1/(1+x^2)^{n-1}$ et $v'=1$ :
\begin{align*}
I_{n-1}(x)&=  \frac{x}{(1+x^2)^{n-1}} + 2(n-1)\int \frac{x^{2} dx}{(1+x^2)^n} =  \frac{x}{(1+x^2)^{n-1}} + 2(n-1)\int \frac{(1+x^{2}) dx}{(1+x^2)^n}- 2(n-1)\int \frac{1 dx}{(1+x^2)^n}\\
&=\frac{x}{(1+x^2)^{n-1}} + 2(n-1)I_{n-1}- 2(n-1)I_{n}
\end{align*}
ce qui implique le résultat demandé. 
\end{preuve}


%%%%%%%%%%%%
\exo{}
En utilisant la décomposition en éléments simples, calculer les primitives suivantes :
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int \frac{x^3dx}{x^2+1}$, 
\item $\int \frac{dx}{x(1+x)^2}$, 
\item $\int \frac{dx}{4x^2-3x+2}$,
\item $\int\frac{x^2dx}{x^4-1}$,
\item $\int \frac{(x-1)dx}{(1+x)^3(x-2)}$.
\end{enumerate*}

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item C'est clair que
\[     \int \frac{x^3dx}{x^2+1} =  \int x dx - \frac{x dx}{x^2+1} = \frac{x^{2}}{2} - \frac{\ln(x^{2}+1)}{2} + C     \]

\item On voit que 
\[     \int \frac{dx}{x(1+x)^2} = \int \bigg( \frac{1}{x} - \frac{1}{x+1} - \frac{1}{(x+1)^{2}}  \bigg) dx = \ln(|x|) - \ln(|x+1|) + \frac{1}{(x+1)} + C,     \]
avec domaine de définition $\R \setminus \{ 0, -1\}$. 

\item C'est clair que 
\[     \int \frac{dx}{4x^2-3x+2} = \frac{1}{4} \int \frac{dx}{(x - \frac{3}{8})^{2} + \frac{23}{64}} = \frac{2 \arctan\bigg( \frac{8x-3}{\sqrt{23}} \bigg)}{\sqrt{23}} + C,     \]
avec domaine de définition $\R$. 

\item On voit que 
\begin{align*}     
\int\frac{x^2dx}{x^4-1} &= \int \bigg( \frac{1}{2(x^{2}+1)} - \frac{1}{4(x+1)} + \frac{1}{4(x-1)}  \bigg) dx 
\\
&= \frac{1}{4} \bigg(\ln(|x-1|) - \ln(|x+1|) + 2 \arctan(x) \bigg) + C,     
\end{align*}
avec domaine de définition $\R \setminus \{ \pm 1\}$. 

\item On voit que 
\begin{align*}
     \int \frac{(x-1)dx}{(1+x)^3(x-2)} &= \int \bigg(- \frac{1}{27(x+1)} -\frac{1}{9(x+1)^{2}} + \frac{2}{3(x+1)^{3}}+ \frac{1}{27(x-2)}  \bigg) dx 
     \\ 
     &=  \frac{1}{9(x+1)} -\frac{1}{3(x+1)^2}  + \frac1{27}\ln(|x-2|) -\frac1{27} \ln (|x+1|)  + C,     
     \end{align*}
avec domaine de définition $\R \setminus \{ 2 \}$. 
\end{enumerate}
\end{preuve}

%%%%%%%%%%%%
\exo{}
Calculer 
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int\frac{dx}{49-4x^2}$,
\item $\int\frac{(5x-12)dx}{x(x-4)}$,
\item $\int\frac{(37-11x)dx}{(x+1)(x-2)(x-3)}$, 
\item $\int\frac{(2x^2-15x+33)dx}{(x+1)(x-5)}$,
\item $\int\frac{(x-1)dx}{x^2+x+1}$,
\item $\int\frac{dx}{(x^2+4x+5)^2}$.
\end{enumerate*}

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item C'est clair que
\begin{align*}
     \int\frac{dx}{49-4x^2} =  \frac{1}{28} \int \bigg( \frac{1}{7+2x} + \frac{1}{7-2x} \bigg) dx = \frac{1}{28} \ln\bigg(\bigg| \frac{2x+7}{2x-7} \bigg|\bigg) + C,     
     \end{align*}
avec domaine de définition $\R \setminus \{ \pm 7/2 \}$. 

\item On voit que 
\begin{align*}     \int\frac{(5x-12)dx}{x(x-4)} &= \int \bigg( \frac{3}{x} + \frac{2}{x-4}   \bigg) dx 
\\ 
&= 3 \ln(|x|) + 2\ln(|x-4|)  + C,     
\end{align*}
avec domaine de définition $\R \setminus \{ 0, 4 \}$. 


\item On voit que 
\begin{align*}     \int\frac{(37-11x)dx}{(x+1)(x-2)(x-3)} &= \int \bigg( -\frac{5}{x-2} + \frac{4}{x+1} + \frac{1}{x-3}  \bigg) dx 
\\ 
&= -5 \ln(|x-2|) + \ln(|x-3|) + 4  \ln(|x+1|) + C,     
\end{align*}
avec domaine de définition $\R \setminus \{ -1, 2, 3 \}$. 

\item C'est clair que 
\begin{align*}
    \int\frac{(2x^2-15x+33)dx}{(x+1)(x-5)} &= \int \bigg( 2+ \frac{4}{3(x-5)} - \frac{25}{3(x+1)}  \bigg) dx 
\\    
   &= 
2x + \frac{4}{3} \ln(|x-5|) - \frac{25}{3} \ln(|x+1|) + C,     
\end{align*}
avec domaine de définition $\R \setminus \{ -1, 5 \}$. 

\item On voit que 
\begin{align*}     
\int\frac{(x-1)dx}{x^2+x+1} &= \int \bigg( \frac{2x+1}{2(x^2+x+1)} - \frac{3}{2(x^2+x+1)}  \bigg) dx 
\\
&= \int \bigg( \frac{2x+1}{2(x^2+x+1)} - \frac{6}{(2 x+1)^{2}+3}  \bigg) dx 
\\
&= \frac{1}{2} \bigg(\ln(x^2+x+1) - 2 \sqrt{3} \arctan\bigg(\frac{2x+1}{\sqrt{3}}\bigg) \bigg) + C,     
\end{align*}
avec domaine de définition $\R$. 

\item Si l'on fait la substitution $y = x + 2$, on voit que 
\begin{align*}
     \int\frac{dx}{(x^2+4x+5)^2} &= \int\frac{dx}{\big((x+2)^2+1\big)^2} = \int \frac{dy}{(y^2+1)^2} 
     \\
     &= \int \frac{1+y^{2}}{(y^2+1)^2} dy - \int \frac{y^{2} dy}{(y^2+1)^2} = \arctan(y) - \int \frac{y^{2} dy}{(y^2+1)^2}.
    \end{align*}
La dernière intégrale peut se calculer à partir d'une intégration par parties avec $u = y$ et $v' = - y/(y^{2}+1)^{2}$ (\textit{i.e.} $v = 1/(2(y^{2}+1))$)  
\[     - \int \frac{y^{2} dy}{(y^2+1)^2} = \frac{1}{2}\frac{y}{y^{2}+1} - \frac{1}{2} \int \frac{dy}{y^2+1} = \frac{1}{2}\bigg(\frac{y}{y^{2}+1} - \arctan(y)\bigg).   \]
Finalement, la primitive est 
\[     \int\frac{dx}{(x^2+4x+5)^2} = \frac{1}{2} \bigg( \frac{x+2}{x^2+4x+5} + \arctan(x+2)\bigg) + C,     \]
avec domaine de définition $\R$. 
\end{enumerate}
\end{preuve}


%%%%%%%%%%
 \subsubsection{Polynômes en sinus et cosinus}

La méthode est de linéariser sauf dans le cas de $\cos^m (x) \sin^n (x)$ avec $m \in \N$ ou $n \in \N$ impair. 
Dans ce cas on fait un changement de variables. 

\exo{} \label{exo:6}Écrire les primitives de $\cos^m (x) \sin^n (x)$ avec $m$ ou $n$ impair.  

\begin{preuve} Si $m$ est impair de la forme $m = 2 m' + 1$ avec $m' \in \N$, alors
\begin{align*} 
   \hskip -0.9cm \int \cos^m (x) &\sin^n (x) dx = \int \cos^{2m'} (x) \sin^n (x) \cos(x) dx = \int \big(1 - \sin^{2}(x)\big)^{m'} \sin^n (x) \cos(x) dx 
    \\ 
    &= \sum_{k=0}^{m'} (-1)^{k} \begin{pmatrix}m'\\k \end{pmatrix} \int \sin^{n+2k}(x) \cos(x) dx = \sum_{k=0}^{m'} (-1)^{k} \begin{pmatrix}m'\\k \end{pmatrix} \frac{\sin^{n+2k+1}(x)}{n+2k+1}.     
\end{align*}
De façon analogue, si $n$ est impair avec $n = 2 n' +1$ et $n' \in \N$, on a 
\begin{align*} 
   \hskip -0.9cm \int \cos^m (x) &\sin^n (x) dx = \int \cos^{m} (x) \sin^{2n'} (x) \sin(x) dx = \int \big(1 - \cos^{2}(x)\big)^{n'} \cos^m (x) \sin(x) dx 
    \\ 
    &= \sum_{k=0}^{n'} (-1)^{k} \begin{pmatrix}n'\\k \end{pmatrix} \int \cos^{m+2k}(x) \sin(x) dx = \sum_{k=0}^{n'} (-1)^{k+1} \begin{pmatrix}n'\\k \end{pmatrix} \frac{\cos^{m+2k+1}(x)}{m+2k+1}.     
\end{align*}
\end{preuve}

%%%%%%%%%%%%
\exo{}
Pour $n\in\N$, calculer 
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int \cos^n(\theta)d\theta$, 
\item $\int \sin^n(\theta)d\theta$.
\end{enumerate*}

%\newpage


%%%%%%%%%%
\subsubsection{Changement de variables}

\exo{} 
\begin{enumerate}
\item Écrire le théorème de changement de variables.
\item Écrire le cas particulier des fractions rationnelles en sinus et cosinus (règles de \emph{C. Bioche}).
\item Donner des primitives de $\sqrt{x^2+1}$, de $\sqrt{x^2-1}$ et de $\sqrt{1-x^2}$, en utilisant les changements de variable $x=\sinh (u)$ ou $x=\cosh (u)$ ou $x=\sin (u)$.
\end{enumerate}

\begin{preuve}
Il s'agit des résultats du cours. 
\end{preuve}





%%%%%%%%%%%%
\exo{} 
Calculer 
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int \sin^{3} (x)dx$, 
\item $\int \frac{\sin (x)}{(2+\cos (x))^2}dx$, 
\item $\int \sin(x/2)\cos(x/3) dx$, 
\item $\int \frac{1}{1+\sin^2 (x)}dx$, 
\end{enumerate*}
\\
\begin{enumerate*}[label=(\alph*)]
\setcounter{enumi}{4}
\item $\int \frac{dx}{a+b\cos (x)}$, 
\end{enumerate*}
avec $a,b\in\R$ tels que $a>|b|>0$.

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item D'après l'exercice \ref{exo:6}, on voit que 
\[     \int \sin^{3} (x)dx = \frac{1}{3}\big( \cos^{3}(x) - 3 \cos(x) \big) + C,      \]
avec domaine de définition $\R$. 

\item C'est clair que 
\[     \int \frac{\sin (x)}{(2+\cos (x))^2}dx = \frac{1}{2+\cos (x)} + C,     \]
avec domaine de définition $\R$. 

\item On voit que 
\[     \int \sin\bigg(\frac{x}{2}\bigg)\cos\bigg(\frac{x}{3}\bigg) dx = \int \bigg(\sin\bigg(\frac{x}{6}\bigg) + \sin\bigg(\frac{5x}{6}\bigg)\bigg) \frac{dx}{2} = - \frac{3}{5} \bigg(5 \cos\bigg(\frac{x}{6}\bigg) + \cos\bigg(\frac{5 x}{6}\bigg)\bigg) + C,  \]
avec domaine de définition $\R$, où l'on a utilisé l'identité 
\[     \sin(\alpha) \cos(\beta) = \big(\sin(\alpha-\beta) + \sin(\alpha+\beta)\big)/2.     \] 

\item Si l'on fait la substitution $y= \tan(x)$, alors $\sin^{2}(x) = y^{2}/(1+y^{2})$ et $dx = dy/(1+y^{2})$. 
Cela implique que  
\begin{align*}     \int \frac{1}{1+\sin^2 (x)}dx &=  \int \frac{1}{1+2 y^{2}}dy = \frac{1}{\sqrt{2}} \arctan( \sqrt{2} y ) + C 
\\
&= \frac{1}{\sqrt{2}} \arctan\big( \sqrt{2} \tan(x) \big) + C,     
\end{align*}
avec domaine de définition $\hskip 0.6mm ]\hskip 0.6mm -\pi/2,\pi/2\hskip 0.6mm[\hskip 0.6mm$.

\item Si l'on fait la substitution $y = \tan(x/2)$ (\textit{i.e.} $\cos(x) = (1-y^{2})/(1+y^{2})$), alors $dx = 2 dy/(1+y^{2})$ et 
\[     \int \frac{dx}{a+b\cos (x)} = \int \frac{2 dy}{(a+b) + (a-b) y^{2}} = \frac{2}{\sqrt{a^{2}-b^{2}}} \arctan\bigg( \sqrt{\frac{a-b}{a+b}} \tan(x/2)\bigg) + C,     \] 
avec domaine de définition $\hskip 0.6mm ]\hskip 0.6mm -\pi,\pi\hskip 0.6mm[\hskip 0.6mm$. 

\end{enumerate}
\end{preuve}

\exo{} 
Calculer 
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int\frac{dx}{(1+x)\sqrt{x}}$, 
\item $\int \frac{\sqrt{x}dx}{(x-1)^2}$, 
\item $\int \frac{x\sqrt{x}dx}{(x+1)^2}$.
\end{enumerate*}

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item $\int\frac{dx}{(1+x)\sqrt{x}} = 2 \operatorname{arctan}(\sqrt{x}) + C$, avec domaine de définition $\R_{>0}$ (appliquer la substitution $u = \sqrt{x}$).
\item On utilise la substitution $y = \sqrt{x}$. 
Cela nous donne 
\begin{align*}
     \int \frac{\sqrt{x}dx}{(x-1)^2} &= 2 \int \frac{y^{2} dx}{(y^{2}-1)^2} 
     \\
     &= 2 \int \bigg( \frac{1}{4(y+1)^{2}} - \frac{1}{4(y+1)} + \frac{1}{4(y-1)^{2}} + \frac{1}{4(y-1)} \bigg) dy 
     \\
     &= \frac{1}{2}\bigg(\ln\bigg(\bigg|\frac{y-1}{y+1}\bigg|\bigg) - \frac{2y}{y^{2}-1} \bigg) + C
     \\
     &= \frac{1}{2}\bigg(\ln\bigg(\bigg|\frac{\sqrt{x}-1}{\sqrt{x}+1}\bigg|\bigg) - \frac{2\sqrt{x}}{x-1} \bigg) + C,
\end{align*}
avec domaine de définition $\R_{>0} \setminus \{ 1 \}$. 

\item On utilise la substitution $y = \sqrt{x}$. 
Cela nous donne 
\begin{align*}
     \int \frac{x\sqrt{x}dx}{(x+1)^2} &= 2 \int \frac{y^{4} dx}{(y^{2}+1)^2} 
     = 2 \int \bigg( 1+ \frac{1}{(y^{2}+1)^{2}} - \frac{2}{(y^{2}+1)} \bigg) dy 
          \\
     &= 2 y + \bigg(\frac{y}{1+y^{2}} +  \arctan(y) \bigg) - 4 \arctan(y) + C
          \\
     &= \frac{\sqrt{x} (2x+3)}{1+x} - 3 \arctan(\sqrt{x}) + C, 
\end{align*}
où l'on a utilisé l'exercice \ref{exo:17}. 
Le domaine de définition respectif est $\R_{>0}$. 
\end{enumerate}
\end{preuve}


%%%%%%%%%%%%
\exo{Primitives de fonctions du type $f(x,\sqrt{ax^2+bx+c})$}
Par un changement de variables, se ramener a une forme canonique du type $f(x,\sqrt{x^2+1})$, $f(x,|x-1|)$, $f(x,\sqrt{x^2-1})$ ou $f(x,\sqrt{1-x^2})$, puis effectuer un autre changement de variables avec les fonctions sinus/cosinus trigonométriques ou hyperboliques. 
\\
Calculer
\\
\begin{enumerate*}[label=(\alph*)]
\item $\int \frac{dx}{(1-x)\sqrt{1-x^2}}$, 
\item $\int \sqrt{x^2-3x+2} dx$,
\end{enumerate*}
\\
\begin{enumerate*}[label=(\alph*)]
\setcounter{enumi}{2}
\item $\int\sqrt{x^2+x+1} dx$,
\item $\int\sqrt{-x^2+x+1} dx$.
\end{enumerate*}

\begin{preuve}
Dans les résultats suivants $C \in \R$ dénote une constante quelconque. 
\begin{enumerate}
\item Le changement de variable $x  = \sin y$ nous dit que 
\[     \int \frac{dx}{(1-x)\sqrt{1-x^2}} =  \int \frac{\cos y dy}{(1-\sin y)\cos y} = \int \frac{\frac{2dt}{1+t^2}}{1-\frac{2t}{1+t^2}}\]
où l'on a fait le changement de variable $t=\tan\frac{y}2$ (règle de Bioche). On a donc
\[     \int \frac{dx}{(1-x)\sqrt{1-x^2}} = \int \frac{2dt}{(1-t)^2} = \frac2{1-t}+C=  \frac2{1-\tan\frac{\arcsin x}2}+C  \]
avec domaine de définition $\hskip 0.6mm ]\hskip 0.6mm -1,1\hskip 0.6mm[\hskip 0.6mm$. Ceci peut se transformer en écrivant:
\begin{align*}
 \tan\frac{\arcsin x}2 &= \frac{\sin\frac{\arcsin x}2 \cos\frac{\arcsin x}2}{\cos^2\frac{\arcsin x}2}=\frac{\sin \arcsin x}{\cos \arcsin x+1}=\frac{x}{\sqrt{1-\sin^2 \arcsin x }+1}\\
 &=\frac{x}{\sqrt{1- x ^2}+1}.
\end{align*}


\item Le changement de variable $2x-3  = \cosh y$ nous dit que 
\begin{align*}     
\int \sqrt{x^2-3x+2} dx &= \int \sqrt{ (x-\frac32)^2-\frac14} dx = \frac14\int \sqrt{ (2x-3)^2-1} (2dx) =\frac14\int \sinh^2 y dy 
\\
&=\frac{e^{2y}-e^{-2y}}{32} -\frac{y}8+C = \frac{\sinh 2{\rm arcosh} (2x-3)}{16}- \frac{{\rm arcosh} (2x-3)}{8} +C
\end{align*}
avec domaine de définition $[2,+\infty[$. Pour avoir une primitive sur $]-\infty,1[$ il faut poser $-2x+3  = \cosh y$.

\item Le changement de variable $\frac{2x+1}{\sqrt3}  = \sinh y$ nous dit que 
\begin{align*}     \int \sqrt{x^2+x+1} dx &=  \int \sqrt{ (x+\frac12)^2+\frac34} dx = \frac{3}4\int \sqrt{ (\frac{2x+1}{\sqrt3})^2+1} (\frac{2dx}{\sqrt3})\\
& =\frac34\int \cosh^2 y dy =3\frac{e^{2y}-e^{-2y}}{32} +\frac{3y}8+C \\
&= \frac{3\sinh 2{\rm arsinh} \frac{2x+1}{\sqrt3} }{16}+ \frac{3{\rm arsinh} \frac{2x+1}{\sqrt3} }{8} +C
\end{align*}
avec domaine de définition $\R$. 

\item Le changement de variable $\frac{2x-1}{\sqrt 5}  = \sin y$ nous dit que 
\begin{align*}    
 \int \sqrt{-x^2+x+1} dx &=  \int \sqrt{\frac54 -(x-\frac12)^2} dx =\frac54 \int \sqrt{1 -(\frac{2x-1}{\sqrt 5})^2} \frac{2dx}{\sqrt5} = 
\frac54 \int \cos^2 y dy \\
&=\frac5{16} \sin2y +\frac58y+C  =\frac5{16} \sin2\arcsin\frac{2x-1}{\sqrt 5} +\frac58\arcsin\frac{2x-1}{\sqrt 5}+C  ,     
\end{align*}
avec domaine de définition $[1-\sqrt{5})/2,(1+\sqrt{5})/2]$. 
\end{enumerate}
\end{preuve}

%%%%%%%%%%%%

%%%%%%%%%%%%
\exo{}
\begin{enumerate} 
\item Montrer qu'une primitive de $x\mapsto P(x) e^{ax}$, où $P \in \R[X]$ est un polynôme et $a$ un réel, est de la forme $x\mapsto Q(x)e^{ax}+C$, où $Q \in \R[X]$ est un polynôme et $C$ est une constante.

\item Montrer qu'une primitive de $x\mapsto P(x) \cos(\alpha x)$, où $P\in \R[X]$ est un polynôme et $\alpha$ un réel, est de la forme $x\mapsto Q_1(x)\cos(\alpha x)+Q_2(x)\sin(\alpha x)+ C$, où $Q_1$ et $Q_2$ sont des polynômes dans $\R[X]$ et $C$ est une constante.
\end{enumerate}


\begin{preuve}
\begin{enumerate} 
\item On procède par récurrence sur le degré de $P$. 
Si $\deg(P) \leq 0$, alors $P = c \in \R$ et on pose $Q(x) = c/a$. 
On suppose que l'énoncé est vrai pour tout polynôme $P$ de degré strictement inférieur à $d \in \N^{*}$. 
On va le démontrer pour le cas où $P$ a degré $d$. 
Comme $(Q(x) e^{ax} + C)' = (Q'(x) + a Q(x)) e^{ax}$, il suffit de montrer qu'il existe un polynôme $Q \in \R[X]$ 
tel que $Q'(x) + a Q(x) = P(x)$. 
Soit $P = c x^{d} + \bar{P}$, avec $c \neq 0$ et $\bar{P}$ de degré strictement inférieur à $d$. 
On pose $R = c x^{d}/a$. 
C'est clair que $R' + a R - P$ est un polynôme de degré strictement inférieur à $d$. 
Par l'hypothèse de la récurrence, on sait qu'il existe un polynôme $T$ tel que $T' + a T = R' + a R - P$. 
Alors $Q = R - T$ satisfait la propriété demandée. 

\item Le même argument que celui donné pour l'item précédent est aussi valable dans ce cas. 
\end{enumerate}
\end{preuve}

\newpage \section{Algèbres de Boole de parties d'un ensemble et mesures}



\exo{} 
Soit $X$ un ensemble. 
On rappelle qu'une \textbf{algèbre de Boole de parties} de $X$ (ou simplement \textbf{algèbre de Boole}, ou \textbf{algèbre d'ensembles}) est une paire $(X, \mathscr{A})$, où $\mathscr{A} \subseteq \mathscr{P}(X)$ est une famille de parties de $X$ qui satisfait que $\emptyset, X \in \mathscr{A}$ et que, pour tous $A, B \in \mathscr{A}$, $A \cup B \in \mathscr{A}$, $A \cap B \in \mathscr{A}$ et $A \setminus B \in \mathscr{A}$. \footnote{Plusieurs auteurs n'imposent pas la condition $X \in \mathscr{A}$ dans la définition d'algèbre d'ensembles, car ils veulent considérer après seulement des parties de mesure finie.}
Soit $(X,\mathscr{A})$ une algèbre de Boole. 
On définit $A + B = (A \setminus B) \cup (B \setminus A) = (A \cup B) \setminus (A \cap B)$ et $A \cdot B = A \cap B$, pour tous $A, B \in \mathscr{A}$. 
Montrer que $(\mathscr{A},+,\cdot)$ est un anneau (unitaire) commutatif.

\begin{preuve}
	C'est clair que $+, \cdot : \mathscr{A} \times \mathscr{A} \rightarrow \mathscr{A}$ sont deux lois internes commutatives. 
	Comme l'intersection d'ensembles est associative, le produit $\cdot$ est associatif aussi. 
	En outre, $+$ est associative. 
	En effet, cela suit du fait que 
	\[     A + (B + C) = \big(A \setminus (B \cup C) \big) \cup \big(B \setminus (A \cup C) \big) \cup \big(C \setminus (A \cup B) \big) \cup \big(A \cap B \cap C \big),     \]
	pour tous $A, B, C \in \mathscr{A}$. 
	Cette dernière égalité suit de 
	\begin{align*} 
	A \setminus (B + C) &= A \setminus \big((B \cup C) \setminus (B \cap C) \big) = A \cap \big((B \cup C)^{c} \cup (B \cap C) \big) 
	\\
	&= \big(A \cap (B \cup C)^{c}\big) \cup \big(A \cap (B \cap C) \big) =\big(A \setminus (B \cup C) \big) \cup \big(A \cap B \cap C \big),                        
	\end{align*}
	où $D^{c} = X \setminus D$, pour tout $D \subseteq X$, et 
	\begin{align*} 
	(B + C) \setminus A &= \big((B \setminus C) \cup (C \setminus B)\big) \setminus A = \big(B \setminus (A \cup C)\big) \cup \big(C \setminus (A \cup B)\big).                        
	\end{align*}
	
	On voit bien que $\emptyset$ est l'élément neutre pour la somme $+$ et l'ensemble $X$ est l'unité pour le produit $\cdot$. 
	Il reste à vérifier la propriété distributive, \textit{i.e.} 
	\[     A \cdot (B + C) = (A \cdot B) + (A \cdot C),     \]
	pour tous $A, B, C \in \mathscr{A}$. 
	En effet, 
	\begin{align*} 
	A \cdot (B + C) &= A \cap \big((B \setminus C) \cup (C \setminus B)\big) 
	= \big(A \cap (B \setminus C)\big) \cup \big(A \cap (C \setminus B)\big)
	\\
	&=  \big((A \cap B) \setminus C\big) \cup \big((A \cap C) \setminus B)\big)
	=  \big((A \cap B) \setminus (A \cap C)\big) \cup \big((A \cap C) \setminus (A \cap B)\big)
	\\
	&= (A \cap B) + (A \cap C) = (A \cdot B) + (A \cdot C), 
	\end{align*}
	où l'on a utilisé la distributivité de l'intersection par rapport à l'union dans la deuxième égalité. 
\end{preuve}

\exo{} 
Donner une algèbre de Boole $(\RR, \mathscr{A})$ telle que les singletons ne soient pas inclus dans $\mathscr{A}$.

\begin{preuve}
	On peut prendre $\mathscr{A} = \{ \emptyset, \RR \}$. 
\end{preuve}

\exo{} 
Soient $(X,\mathscr{A})$ et $(Y,\mathscr{B})$ deux algèbres de Boole. 
On rappelle qu'un \textbf{morphisme} $f : (X,\mathscr{A}) \rightarrow (Y,\mathscr{B})$ est une application $f : X \rightarrow Y$ qui satisfait que $f^{-1}(B) \in \mathscr{A}$, pour tout $B \in \mathscr{B}$.
\begin{enumerate}
	\item Soient $(X , \mathscr{A})$ une algèbre de Boole et $f : X \rightarrow Y$ une application. 
	On définit $f_{*}(\mathscr{A}) = \{ B \subseteq Y : f^{-1}(B) \in \mathscr{A} \}$. 
	Montrer que $(Y,f_{*}(\mathscr{A}))$ est une algèbre de Boole, appelée \textbf{l'image directe} de $\mathscr{A}$, 
	et que $f$ est un morphisme d'algèbres de Boole de $(X,\mathscr{A})$ dans $(Y , f_{*}(\mathscr{A}))$. 
	\item Soient $(Y , \mathscr{B})$ une algèbre de Boole et $f : X \rightarrow Y$ une application. 
	On définit $f^{*}(\mathscr{B}) = \{ f^{-1}(B) : B \in \mathscr{B} \}$. 
	Montrer que $(X,f^{*}(\mathscr{B}))$ est une algèbre de Boole, appelée \textbf{l'image inverse} de $\mathscr{B}$, 
	et que $f$ est un morphisme d'algèbres de Boole de $(X,f^{*}(\mathscr{B}))$ dans $(Y , \mathscr{B})$. 
	\item Soient $(X , \mathscr{A})$ et $(Y , \mathscr{B})$ deux algèbres de Boole et $f : X \rightarrow Y$ une application. 
	Montrer que $f^{*}(\mathscr{B}) \subseteq \mathscr{A}$ si et seulement si $f$ est un morphisme d'algèbres de Boole de $(X , \mathscr{A})$ dans $(Y , \mathscr{B})$, si et seulement si $\mathscr{B} \subseteq f_{*}(\mathscr{A})$.
\end{enumerate}

\begin{preuve}
	\begin{enumerate}
		\item C'est clair que $\emptyset = f^{-1}(\emptyset) \in \mathscr{A}$, ce qui implique que $\emptyset \in f_{*}(\mathscr{A})$. 
		De même, la condition $X \in \mathscr{A}$ avec $X = f^{-1}(Y)$ nous disent que $Y \in f_{*}(\mathscr{A})$. 
		En outre, soient $B, B' \in f_{*}(\mathscr{A})$. 
		Alors, $f^{-1}(B \cup B')= f^{-1}(B) \cup f^{-1}(B')$, $f^{-1}(B \cap B')= f^{-1}(B) \cap f^{-1}(B')$ et $f^{-1}(B \setminus B')= f^{-1}(B) \setminus f^{-1}(B')$ 
		sont des éléments de $\mathscr{A}$, ce qui implique que $B \cup B', B \cap B', B \setminus B' \in f_{*}(\mathscr{A})$, comme on voulait démontrer. 
		Le fait que $f$ est un morphisme d'algèbres de Boole est immédiat. 
		
		\item C'est clair que $\emptyset = f^{-1}(\emptyset) \in f^{*}(\mathscr{B})$, vu que $\emptyset \in \mathscr{B}$. 
		De même, la condition $Y \in \mathscr{B}$ avec $X = f^{-1}(Y)$ nous disent que $X \in f^{*}(\mathscr{B})$. 
		En outre, soient $A = f^{-1}(B)$ et $A' = f^{-1}(B')$, avec $B, B' \in \mathscr{B}$. 
		Alors, $A \cup A' = f^{-1}(B) \cup f^{-1}(B') = f^{-1}(B \cup B')$, $A \cap A' = f^{-1}(B) \cap f^{-1}(B') = f^{-1}(B \cap B')$ et $A \setminus A' = f^{-1}(B) \setminus f^{-1}(B') = f^{-1}(B \setminus B')$ sont des éléments de $f^{*}(\mathscr{B})$, comme on voulait démontrer. 
		Le fait que $f$ est un morphisme d'algèbres de Boole est immédiat.
		
		\item Il s'agit d'une conséquence directe des définitions. 
	\end{enumerate}
\end{preuve}

\exo{}
Soient $(X,\mathscr{A})$ et $(Y,\mathscr{B})$ deux algèbres de Boole. 
On définit $\mathscr{A} \boxtimes \mathscr{B} \subseteq \mathscr{P}(X \times Y)$ comme l'ensemble formé des unions des familles finies et disjointes des parties de la forme $A \times B$, avec $A \in \mathscr{A}$ et $B \in \mathscr{B}$. 
Montrer que $(X \times Y , \mathscr{A} \boxtimes \mathscr{B})$ est une algèbre de Boole. 

\begin{preuve}
	C'est clair que $\emptyset = \emptyset \times \emptyset \in \mathscr{A} \boxtimes \mathscr{B}$. 
	En outre, les conditions $X \in \mathscr{A}$ et $Y \in \mathscr{B}$ nous disent que $X \times Y \in \mathscr{A} \boxtimes \mathscr{B}$. 
	Comme
	\[     (A \times B) \cap (A' \times B') = (A \cap A') \times (B \cap B'),     \]
	pour tous $A, A' \in \mathscr{A}$ et $B, B' \in \mathscr{B}$, on conclut que l'intersection de deux éléments dans $\mathscr{A} \boxtimes \mathscr{B}$ appartient à $\mathscr{A} \boxtimes \mathscr{B}$. 
	Il reste à montrer que la différence entre deux éléments dans $\mathscr{A} \boxtimes \mathscr{B}$ appartient à $\mathscr{A} \boxtimes \mathscr{B}$. 
	Pour cela, il suffit de démontrer que $(A \times B) \setminus (A' \times B') \in \mathscr{A} \boxtimes \mathscr{B}$, pour tous $A, A' \in \mathscr{A}$ et $B, B' \in \mathscr{B}$. 
	Cette dernière propriété suit de l'égalité 
	\[     (A \times B) \setminus (A' \times B') = \big( (A \setminus A') \times B\big) \sqcup \big( (A \cap A') \times (B \setminus B')\big),     \]
	et l'on remarque que l'union précédente est disjointe. 
	C'est clair que la réunion finie d'éléments dans $\mathscr{A} \boxtimes \mathscr{B}$ est aussi dans $\mathscr{A} \boxtimes \mathscr{B}$, puisque, si $P$ et $Q$ sont deux éléments de  $\mathscr{A} \boxtimes \mathscr{B}$, alors $P \cup Q = (P \setminus Q) \sqcup Q$, où l'union précédente est disjointe. 
\end{preuve}


\exo{}
Soit $(X, \mathscr{A})$ une algèbre de Boole. 
On rappelle qu'une \textbf{mesure} sur $\mathscr{A}$ est une application $\mu : \mathscr{A} \rightarrow [0,+\infty]$ telle que 
$\nu(\emptyset) = 0$ et 
\[     \mu\bigg( \bigcup_{i \in I} A_{i} \bigg) = \sum_{i \in I} \mu(A_{i}),     \]
pour toute famille disjointe $(A_{i})_{i \in I} \in \mathscr{A}^{I}$ (\textit{i.e.} $A_{i} \cap A_{i'} = \emptyset$ si $i \neq i'$) avec $I$ au plus dénombrable telle que $\cup_{i \in I} A_{i} \in \mathscr{A}$. 
Si dans la définition précédente $I$ est seulement fini, on dit que $\mu$ est une application \textbf{additive}.  
Soient  $(X,\mathscr{A})$ et $(Y,\mathscr{B})$ deux algèbres de Boole et $f : X \rightarrow Y$ un morphisme d'algèbres de Boole. 
Montrer que, si $\mu : \mathscr{A} \rightarrow [0,+\infty]$ est une mesure sur $\mathscr{A}$, l'application $f_{*}\mu : \mathscr{B} \rightarrow [0,+\infty]$ donnée par $B \mapsto \mu (f^{-1} (B))$ est une mesure sur $\mathscr{B}$, appelée \textbf{l'image directe} de $\mu$. 

\begin{preuve}
	On voit bien que 
	\[     f_{*}\mu (\emptyset) = \mu\big(f^{-1}(\emptyset)\big) = \mu(\emptyset) = 0.     \]
	En plus, soit $(B_{i})_{i \in \NN} \in (f_{*}\mathscr{A})^{\NN}$ qui satisfait que $B_{i} \cap B_{i'} = \emptyset$ si $i \neq i'$ et que 
	$\cup_{i \in \NN} B_{i} \in f_{*}\mathscr{A}$. 
	Alors, $(f^{-1}(B_{i}))_{i \in \NN} \in \mathscr{A}^{\NN}$ satisfait que $f^{-1}(B_{i}) \cap f^{-1}(B_{i'}) = f^{-1}(B_{i} \cap B_{i'}) = \emptyset$ si $i \neq i'$ et que $\cup_{i \in \NN} f^{-1}(B_{i}) = f^{-1}(\cup_{i \in \NN} B_{i}) \in \mathscr{A}$. 
	En plus, 
	\[     f_{*}\mu\bigg( \bigcup_{i \in \NN} B_{i} \bigg) = \mu\bigg( f^{-1}\Big(\bigcup_{i \in \NN} B_{i}\Big)\bigg) 
	= \mu\bigg(\bigcup_{i \in \NN}f^{-1}(B_{i}) \bigg) 
	= \sum_{i \in \NN} \mu\big(f^{-1}(A_{i})\big) = \sum_{i \in \NN} f_{*}\mu(B_{i}),      \]
	comme on voulait démontrer. 
\end{preuve}

\exo{} 
Soient $(X,\mathscr{A})$ et $(Y,\mathscr{B})$ deux algèbres de Boole, munies des applications additives $\mu$ et $\nu$, respectivement. 
Montrer qu'il existe une unique application additive $\mu \boxtimes \nu : \mathscr{A} \boxtimes \mathscr{B} \rightarrow [0,+\infty]$ telle que 
\begin{equation}
\label{eq:mu}
(\mu \boxtimes \nu)(A \times B) = \mu(A) \nu(B),     
\end{equation}
pour tous $A \in \mathscr{A}$ et $B \in \mathscr{B}$. 

\begin{preuve}
	Soit $P \in \mathscr{A} \boxtimes \mathscr{B}$. 
	Si $P = \emptyset$, on pose $\mu \boxtimes \nu (P) = 0$. 
	Cette condition est compatible avec \eqref{eq:mu}, car $A \times B = \emptyset$, avec $A \in \mathscr{A}$ et $B \in \mathscr{B}$, si $A = \emptyset$ ou $B = \emptyset$. 
	Si $P \neq \emptyset$, on peut l'écrire de la forme comme une union disjointe 
	\[     P = \bigsqcup_{i \in I} A_{i} \times B_{i},     \] 
	avec $I$ fini, $A_{i} \in \mathscr{A}$ et $B_{i} \in \mathscr{B}$. 
	On pose dans ce cas 
	\[     (\mu \boxtimes \nu) (P) = \sum_{i \in I} \mu(A_{i}) \nu(B_{i}).     \]
	On va montrer que cette expression est bien définie. 
	En effet, on suppose que 
	\[     P = \bigsqcup_{j \in J} C_{j} \times D_{j},     \] 
	avec $J$ fini, $C_{j} \in \mathscr{A}$ et $D_{j} \in \mathscr{B}$. 
	Alors, 
	\begin{align*}     P &= P \cap P = \bigg( \bigsqcup_{i \in I} A_{i} \times B_{i} \bigg) \cap \bigg( \bigsqcup_{j \in J} C_{j} \times D_{j} \bigg) 
	= \bigsqcup_{(i,j) \in I \times J} \big((A_{i} \times B_{i}) \cap (C_{j} \times D_{j})\big) 
	\\
	&
	= \bigsqcup_{(i,j) \in I \times J} \big((A_{i} \cap C_{j}) \times (B_{i} \cap D_{j})\big).    
	\end{align*}
	On voit bien que 
	\[     \sum_{i \in I} \mu(A_{i}) \nu(B_{i}) = \sum_{(i,j) \in I \times J} \mu(A_{i} \cap C_{j}) \nu(B_{i} \cap D_{j}) = \sum_{j \in J} \mu(C_{j}) \nu(D_{j}),     \]
	comme on voulait démontrer. 
	C'est clair que $\mu \boxtimes \nu$ est additive. 
\end{preuve}

\subsection{Intégration basique}

\exo{}
Étant donnés $a < b$ nombres réels, on pose $\operatorname{St}([a,b],\C)$ l'espace vectoriel de \emph{fonctions en escalier}, 
\textit{i.e.} les fonctions $f : [a,b] \rightarrow \C$ telles qu'il existe une \emph{subdivision} $P = \{ a=a_{0} < \dots < a_{n} = b \} \subseteq [a,b]$ avec $n \in \NN^{*}$ tel que $f|_{\hskip 0.1mm ] \hskip 0.1mm a_{i},a_{i+1 \hskip 0.1mm}[ \hskip 0.1mm}$ soit une constante $c_{i}$, pour tout $i \in \{0, \dots, n-1\}$. 
On regarde $\operatorname{St}([a,b],\C)$ comme un sous-espace vectoriel de l'espace vectoriel normé complet $\operatorname{B}([a,b],\C)$ formé des fonctions bornées, muni de la norme infini $|| \hskip 1.2mm ||_{\infty}$. 
\begin{enumerate}
	\item Si $f$ est une fonction en escalier avec subdivision $P = \{ a=a_{0} < \dots a_{n} = b \}$ 
	on définit la valeur $I(f) = \sum_{i=0}^{n-1} c_{i} (a_{i+1}-a_{i}) \in \C$, où $c_{i} = f|_{\hskip 0.1mm ] \hskip 0.1mm a_{i},a_{i+1 \hskip 0.1mm}[ \hskip 0.1mm}$, pour tout $i \in \{0, \dots, n-1\}$.
	Montrer que $I(f)$ est indépendante de la subdivision de $f$ 
	et que l'application $I : \operatorname{St}([a,b],\C) \rightarrow \C$ est linéaire. 
	
	\item Montrer que 
	\[     | I(f)| \leq (b-a). || f ||_{\infty},     \] 
	pour tout $f \in \operatorname{St}([a,b],\C)$, \textit{i.e.} l'application linéaire 
	$I : \operatorname{St}([a,b],\C) \rightarrow \C$ est continue pour la norme infini. 
	En conséquence, $I$ s'étend en une unique application linéaire continue $\bar{I} : \overline{\operatorname{St}([a,b],\C)} \rightarrow \C$ telle que $| \bar{I}(f)| \leq (b-a).|| f ||_{\infty}$, pour tout $f \in \overline{\operatorname{St}([a,b],\C)}$, où 
	$\overline{\operatorname{St}([a,b],\C)}$ est l'adhérence de $\operatorname{St}([a,b],\C)$ dans $\operatorname{B}([a,b],\C)$. 
\end{enumerate} 

On appelle $\overline{\operatorname{St}([a,b],\C)}$ \emph{l'espace des fonctions réglées} ou \emph{intégrables (au sens réglé)}, et on écrit $\bar{I}(f) = \int_{a}^{b} f(x) dx$.\footnote{Cette notion d'intégrabilité n'est pas précisément celle de Riemann (\textit{i.e.} les fonctions $f \in \operatorname{B}([a,b],\C)$ telles que, pour tout $\epsilon > 0$, il existe $g, h \in \operatorname{St}([a,b],\C)$ telles que $|f(x) - g(x)| \leq h(x)$, pour tout $x \in [a,b]$, et $I(h) \leq \epsilon$). Plus précisément, toute fonction réglée est intégrable au sens de Riemann, mais la réciproque n'est pas vraie (\textit{e.g.}, $f : [0,2] \rightarrow \RR$ telle que $f(1+1/n) =1$, si $n \in \NN^{*}$, et zéro sinon), mais la théorie de fonctions réglées satisfait des propriétés plus ``raisonnables'' que celles de la théorie standard de Riemann. 
	C'est pour cela que plusieurs mathématiciens proposent que la notion d'intégrale réglée remplace celle de Riemann 
	(voir Berberian, S. K. Classroom Notes: Regulated Functions: Bourbaki's Alternative to the Riemann Integral. Amer. Math. Monthly {\bf 86} (1979), no. 3, 208--211).} 

\begin{preuve}
	Il s'agit d'un résultat du cours. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{Fonctions en escalier}
Soit $f : [a,b] \rightarrow\RR$ telle que $f$ ne prenne qu'un nombre fini de valeurs. 
On note $\Gamma=f([a,b])$. 
On suppose $|\Gamma|\geq2$.

On suppose de plus que $f$ admet en tout point une limite à droite et une limite à gauche.
\begin{enumerate}
	\item Soit $\epsilon_0=\min\{|y-y'| :  y,y'\in\Gamma, y \neq y' \}$. 
	Montrer que $\epsilon_0>0$.
	
	%\item Montrer que pour tout $x\in [a,b[$, $f(x+)$ appartient \`a $\Gamma$.
	
	\item Montrer que pour tout $x\in [a,b \hskip 0.6mm [\hskip 0.6mm$, il existe $\eta>0$ tel que la restriction de $f$ à $\hskip 0.6mm ] \hskip 0.6mmx ,x+\eta \hskip 0.6mm [ \hskip 0.6mm \cap [a,b]$ est constante de valeur $f(x+)$.
	
	\item Si $x\in \hskip 0.6mm]\hskip 0.6mm a,b\hskip 0.6mm [ \hskip 0.6mm$, on définit $I_x=\{y\in \hskip 0.6mm ] \hskip 0.6mm x,b] : f(y) \neq f(x+) \}$. 
	Si $I_x \neq \emptyset$, on note $\bar{x}$ la borne inférieure de $I_x$. 
	Si $I_x=\emptyset$, on note $\bar{x}=b$. 
	Montrer que $\bar{x}>x$ et que la restriction de $f$ à $\hskip 0.6mm ] \hskip 0.6mm x,\bar{x} \hskip 0.6mm [ \hskip 0.6mm$ est constante. 
	Montrer que si $\bar{x} \neq b$, $f(\bar{x}) \neq f(x+)$ ou $f(\bar{x}+) \neq f(x+)$.
	
	\item On définit par récurrence la suite $(x_n)_{n\in \NN}$ par $x_0=a$ et si $x_n \neq b$, $x_{n+1}=\bar{x}_n$. 
	Si $x_n=b$,  $x_{n+1}=b$. 
	Montrer qu'il existe $N\in\NN$ tel que $x_N=b$. 
	En déduire que $f$ est une fonction en escalier.
	
	\item Montrer que la conclusion de la question précédente est fausse si on enlève l'hypothèse d'existence de limites.
\end{enumerate}

\begin{preuve}
	Il s'agit d'un résultat du cours. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{Une fonction continue est limite uniforme d'une suite croissante de fonctions en escalier} 
Soit $f : [0,1] \rightarrow\RR$ une fonction continue.
\begin{enumerate}
	\item Soit $\omega_n(f)=\sup\{|f(x)-f(y)| : x,y\in[0,1], |x-y|\leq 1/n \}$. 
	Montrer que $\omega_n(f)$ tend vers $0$ quand $n$ tend vers $+\infty$.
	
	\item Soit $\sigma_n$ la subdivision donnée par $\{ j/2^n : j \in \{ 0, \dots, 2^n\} \}$. 
	Montrer que $\sigma_{n+1}$ est plus fine que $\sigma_n$.
	
	\item On définit la suite $g_n$ de fonctions en escalier de $[0,1]$ dans $\RR$ de la façon suivante. 
	Pour $0\leq k<2^n$ et $x \in [k/2^n,(k+1)/2^n \hskip 0.6mm [\hskip 0.6mm$, on pose 
	\[     g_n(x) = \inf \bigg\{ f(y) : y \in \bigg[\frac{k}{2^n},\frac{k+1}{2^n}\bigg] \bigg\}.     \] 
	En plus, on définit $g_n(1)=f(1)$. 
	Montrer que $g_n\leq g_{n+1}$.
	
	\item Montrer que pour tout $n \in \NN$ et pour tout $x\in [0,1]$, $|g_n(x)-f(x)|\leq \omega_{2^n}(f)$.
	En déduire que  la suite $(g_n)_{n \in \NN}$ converge uniformément vers $f$.
	
	\item Comment feriez-vous pour construire une suite décroissante de fonctions en escalier convergeant uniformément vers $f$ ?
\end{enumerate}

\begin{preuve}
	Il s'agit d'un résultat du cours. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{} Soit $f:[0,1]\rightarrow \RR$ la fonction définie par 
\[     f(x)=\begin{cases}
0, &\text{si $x \in ([0,1] \setminus \Q) \cup \{ 0\}$,}
\\
1/q, &\text{si $x=p/q$ avec $p, q \in \NN^{*}$, $p \leq q$ et $\operatorname{PGCD}(p,q) = 1$.}
\end{cases}
\] 
\begin{enumerate}
	\item Soit $x \notin\Q$ et soit $(p_n/q_n)_{n \in \NN}$ une suite de rationnels tendant vers $x$. 
	Soit $\mathcal{F}_N = \{ p/q \in[0,1] : 0 < q \leq N, p \in \NN \}$.
	\begin{enumerate}[label=(\roman*)]
		\item Montrer que $\mathcal{F}_N$ est un ensemble fini.
		\item En déduire qu'il existe $\epsilon>0$ tel que $|x-y|>\epsilon$, pour tout $y\in \mathcal{F}_N$.
		\item En déduire qu'il existe $n_0\in\NN$ tel que $q_n>N$, pour tout $n\geq n_0$.
		\item En déduire que la suite $(q_n)_{n \in \NN}$ tend vers $+\infty$.
	\end{enumerate}
	
	\item Montrer que $f$ est continue en tout point de $[0,1] \setminus\Q$ et en $0$, et elle est discontinue en tout point de $\Q \hskip 0.6mm \cap \hskip 0.6mm ] \hskip 0.6mm 0,1]$. 
	
	\item Soit $\epsilon >0$ et $\mathcal{F}'_\epsilon=\{x \in [0,1] : f(x)>\epsilon \}$.
	\begin{enumerate}[label=(\roman*)]
		\item Montrer que $\mathcal{F}'_\epsilon$ est fini.
		\item Construire une fonction en escalier $g_\epsilon$ telle que $\|g_\epsilon-f\|_\infty<\epsilon$.
		\item Que vaut $\int_0^1g_\epsilon(x)dx$ ?
	\end{enumerate}
	
	\item La fonction $f$ est-elle continue par morceaux ?
\end{enumerate}

\begin{preuve}
	La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{} Soit $x\in\RR$. 
\begin{enumerate}
	\item Donner la nature des suites
	\\
	\begin{enumerate*}[label=(\roman*)]
		\item $(1+\frac{x}{n})^n$,
		\item $n\sin\left(\frac{x}{n}\right)$.
	\end{enumerate*}
	
	\item Étudier la convergence uniforme sur $\RR$ ou sur les compacts de  $\RR$.
\end{enumerate}

\begin{preuve}
	\begin{enumerate}
		\item 
		C'est clair pour $x=0$, et pour $x\neq0$
		\begin{equation}
		\label{eq:exp}
		\underset{n \rightarrow +\infty}{\lim} \bigg(1+\frac{x}{n}\bigg)^n   = \underset{n \rightarrow +\infty}{\lim} \exp\bigg(n\ln \Big(1+\frac{x}{n}\Big)\bigg)=\underset{n \rightarrow +\infty}{\lim} \exp\bigg(n \Big(\frac{x}{n}+o\Big(\frac{x}{n}\Big)\Big)\bigg)= e^{x}.     
		\end{equation} 
		En outre, si $x \neq 0$,
		\begin{equation}
		\label{eq:sin1}
		\underset{n \rightarrow +\infty}{\lim} n\sin(x/n) = \underset{n \rightarrow +\infty}{\lim} x \frac{\sin(x/n)}{x/n} = x     
		\end{equation} 
		et si $x=0$
		\begin{equation}
		\label{eq:sin2}    
		\underset{n \rightarrow +\infty}{\lim} n\sin(x/n)  = 0 = x.     
		\end{equation}
		
		\item La convergence n'est pas uniforme sur $\RR$. 
		En effet, pour la première suite on considère la suite $(x_{n})_{n \in \NN}$ donnée par $x_{n} = n$. 
		Elle satisfait que $(1+x_{n}/n)^n = 2^n$. 
		Comme $|2^{n} - e^{n}|$ converge vers $+ \infty$, on voit bien que la convergence n'est pas uniforme. 
		De même, pour la deuxième suite on observe que $(x_{n})_{n \in \NN}$ donnée par $x_{n} = n \pi$ vérifie que $n\sin(x_{n}/n) = 0$. 
		Comme $|n \sin(\pi) - n| = n$ converge vers $+ \infty$, on voit bien que la convergence n'est pas uniforme. 
		
		Sur les compacts, il suffit de démontrer la convergence uniforme sur $[-R,R]$ pour $R>0$ fixé.
		D'après le développement limité de $\ln(1+y)$ au voisinage de $0$, on sait qu'il existe $\eta>0$ tel que 
		$|\ln(1+y) -y|\leq y^2$ pour tout $y\in [-\eta,\eta]$. Ainsi, pour tout $n\geq N$ où $N$ est fixé tel que $R/N\leq \eta$, on peut appliquer cette égalité à $x/n$, quelque soit $x\in [-R,R]$ :
		\[
		\Big|n\ln\Big(1+\frac{x}{n}\Big)- x\Big| \leq \frac{x^2}{n}\leq \frac{R^2}{n}.
		\]
		Cette inégalité nous permets aussi de dire que 
		
		$n\ln\Big(1+\frac{x}{n}\Big) \in [-R-R^2/n,R+R^2/n] \subset [-R-R^2,R+R^2]$ ce qui est utile pour l'inégalité des accroissements finis appliquées à la fonction exponentielle 
		\begin{gather*}
		\Bigg| \bigg(1+\frac{x}{n}\bigg)^n -e^x \Bigg| =  \Bigg|\exp\bigg(n\ln \Big(1+\frac{x}{n}\Big)\bigg) -\exp(x) \Bigg| \leq \Big(\sup_{y\in  [-R-R^2,R+R^2]} |e^y|\Big) \Big|n\ln\Big(1+\frac{x}{n}\Big)- x\Big| \\
		\sup_{x\in [-R,R]}\Bigg| \bigg(1+\frac{x}{n}\bigg)^n -e^x \Bigg|  \leq \frac{R^2 e^{R+R^2}}{n} \to 0 \text{ quand }n\to \infty.
		\end{gather*}
		
		On raisonne de même pour (ii), mais en utilisant que $| \sin y -y| \leq y^2$ pour tout $y\in [-\eta,\eta]$. Ainsi, pour tout $n\geq N$ où $N$ est fixé tel que $R/N\leq \eta$, on peut appliquer cette égalité à $x/n$, quelque soit $x\in [-R,R]$ :
		\[
		\Big|n\sin \frac{x}{n}- x\Big| \leq \frac{x^2}{n}\leq \frac{R^2}{n} \to 0  \text{ quand }n\to \infty.
		\]
		
		Dans les deux cas, nous avons donc la convergence uniforme sur tout compact.
	\end{enumerate}
\end{preuve}

%%%%%%%%%%%%%%%
\exo{} Donner la nature des séries numériques
\\
\begin{enumerate*}[label=(\alph*)]
	\item $\sum_{n=3}^{\infty} \frac{1}{n^\alpha\ln^\beta (n)}$, avec $\alpha, \beta \in \RR_{\geq 0}$,
	\item $\sum_{n=1}^{\infty} \frac{(-1)^n}{\sqrt{n}}$.
\end{enumerate*}

\begin{preuve}
	\begin{enumerate}
		\item On remarque d'abord que $\ln(n) \geq 1$, pour tout $n \in \NN_{\geq 3}$. 
		Cela nous dit que, si $\alpha > 1$, alors 
		\[     \sum_{n=3}^{\infty} \frac{1}{n^\alpha\ln^\beta (n)} \leq \sum_{n=3}^{\infty} \frac{1}{n^{\alpha}},     \]
		qui converge par le critère de Riemann. 
		
		Pour $\alpha<1$, nous considérons $\gamma = \frac{1+\alpha}2\in ]\alpha,1[$ ainsi
		$\displaystyle
		\frac{1}{n^\alpha\ln^\beta (n)} \geq \frac{1}{n^\gamma}  \frac{n^{\gamma-\alpha}}{\ln^\beta (n)}\geq \frac{C}{n^\gamma}
		$
		et donc la série $  \sum_{n=3}^{\infty}  \frac{1}{n^\alpha\ln^\beta (n)}  $ diverge.
		
		Pour $\alpha=1$, par décroissance de la fonction $f(x)=1/(x\ln^\beta x)$ on a
		\[
		\int_3^{N+1} f(x)\, dx \leq \sum_{n=3}^N f(n) \leq \int_2^N f(x)\, dx.
		\]
		En calculant $ \int_2^N f(x)\, dx = -\frac{1}{\beta-1}\frac1{\ln^{\beta-1} N } +\frac{1}{\beta-1}\frac1{\ln^{\beta-1} 2}$ pour $\beta\neq 1$ et $ \int_2^N f(x)\, dx = \ln\ln N-\ln\ln 2$, nous concluons que la série converge si et seulement si $\beta>1$.
		
		
		\item Comme la suite $(1/\sqrt{n})_{n\in\NN^{*}}$ est décroissante avec limite zéro, le théorème des séries alternées nous dit que 
		$\sum_{n=1}^{\infty} \frac{(-1)^n}{\sqrt{n}}$ est convergente. 
	\end{enumerate}
\end{preuve}

%%%%%%%%%%%%%%%
\exo{} Donner la nature des séries de fonctions suivantes 
\\
\begin{enumerate*}[label=(\alph*)]
	\item $\sum_{n=0}^\infty \frac{x^n}{n!}$,
	\item $\sum_{n=1}^\infty\frac{\sin(nx)}{n^2}$,
	\item $\sum_{n=0}^\infty 2^{-n}\cos(3^n x)$.
\end{enumerate*}
\\
Préciser les intervalles sur lesquels la convergence est uniforme.

\begin{preuve}
	\begin{enumerate}
		\item Le critère de D'Alembert nous dit que le rayon de convergence de la série entière est $+ \infty$. 
		Cela implique que la série converge uniformément sur tout intervalle fini de $\RR$. 
		Par contre, la série ne converge pas uniformément sur $\RR$. 
		
		\item Comme
		\[     \sum_{n=1}^\infty \bigg|\frac{\sin(nx)}{n^2} \bigg| \leq \sum_{n=1}^\infty \frac{1}{n^2},     \]
		pour tout $x \in \RR$, et la dernière série numérique converge, le critère de Weierstrass nous dit que la série 
		$\sum_{n=1}^\infty \sin(nx)/n^2$ converge absolument et uniformément sur $\RR$. 
		
		\item Comme
		\[     \sum_{n=0}^\infty \bigg|\frac{\cos(3^n x)}{2^{n}} \bigg| \leq \sum_{n=0}^\infty \frac{1}{2^n},     \]
		pour tout $x \in \RR$, et la dernière série numérique converge, le critère de Weierstrass nous dit que la série 
		$\sum_{n=0}^\infty \cos(3^n x)/2^n$ converge absolument et uniformément sur $\RR$. 
	\end{enumerate}
\end{preuve}

%%%%%%%%%%%%%%%
\exo{} 
On considère la fonction $f : [1,2] \rightarrow \RR$ donnée par $f(x)=x^{-2}$. 
Soit $n \in \NN^{*}$. 
On découpe l'intervalle $[1,2]$ en $n$ intervalles égaux. 
Les bornes des intervalles sont définies à l'aide des points $x_i=(n+i)/n$, pour $0\leq i\leq n$.
\begin{enumerate}
	\item Que représente la somme 
	\[     S_n= \frac{1}{n}\sum_{i=1}^n f(x_i)     \]
	pour $n \in \NN^{*}$ ?
	\item Montrer que 
	\[     S_n=n \sum_{i=1}^n \frac{1}{(n+i)^2},     \]
	pour tout $n \in \NN^{*}$. 
	
	\item Démontrer que 
	\[     n \sum_{i=1}^n \frac{1}{(n+i)(n+i+1)} < S_n < n \sum_{i=1}^n \frac{1}{(n+i-1)(n+i)},     \]
	pour tout $n \in \NN^{*}$. 
	
	\item En déduire que
	\[     \frac{n^2}{(n+1)(2n+1)} < S_n<\frac{1}{2},     \]
	pour tout $n \in \NN^{*}$. 
	\\
	\textbf{Indication :} remarquer que 
	\[     \frac{1}{k(k+1)}=\frac{1}{k}-\frac{1}{k+1}.     \]
	
	\item En déduire que
	\[     \underset{n \rightarrow \infty}{\lim} S_n=\frac{1}{2}.     \]
	
	\item Calculer $\int_1^2f(x) dx$.
\end{enumerate} 

\begin{preuve}
	La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{Autour des mesures de réunion finie d'intervalles} 
Tous les intervalles considérés dans cet exercice seront inclus dans $[a,b]$.
Soit $\mathcal{U}$ l'ensemble des réunions finies d'intervalles inclus dans $[a,b]$.
\begin{enumerate}
	\item Montrer que si $X\subset[a,b]$ est un ensemble fini, alors $X \in \mathcal{U}$.
	
	\item Soit $X\subseteq [a,b]$. 
	Montrer que la fonction indicatrice $\mathbb{1}_X$ de $X$ est une fonction en escalier sur $[a,b]$ si et seulement si $X\in \mathcal{U}$.
	
	\item Montrer que si $A$ et $B$ appartiennent à $\mathcal{U}$, alors $A\cup B\in \mathcal{U}$ et $A\cap B\in \mathcal{U}$.
	Montrer que si $A\in \mathcal{U}$, $[a,b]\setminus A \in \mathcal{U}$.
	
	\item On définit l'application $m$ de $\mathcal{U}$ dans $\RR$ par $m(X)=\int_a^b \mathbb{1}_{X}(x) dx$.
	\begin{enumerate}[label=(\roman*)]
		\item Montrer que si $X$ est un intervalle, $m(X)$ est égal à la longueur de $X$.
		
		\item Montrer que si $X\in \mathcal{U}$ et $J$ est un ensemble fini, alors $m(X\cup J)=m(X)$.
		
		\item Montrer que si $X\subseteq Y$ et $X,Y\in \mathcal{U}$, $m(X)\leq m(Y)$.
		
		\item Montrer que si $X\subseteq Y$ et $X,Y\in \mathcal{U}$, $m(X\cup Y)=m(X)+m(Y)-m(X\cap Y)$.
	\end{enumerate}
	% \qst Soit $(X_n)_{n\geq 1}$ une suite  d\'ecroissante d'\'el\'ements de $\cal U$ tels que $\displaystyle\cap_{n=1}^{+\infty}X_n=\emptyset$.
	% 
	% Montrer que la suite $(m(X_n))_{n\geq 1}$ est d\'ecroissante. Supposons qu'il existe $\delta >0$ tel que pour tout $n\in\N$, $m(X_n)>\delta$.
	% 
	% Montrer que pour tout $n\geq 1$, il existe $Y_n\in{\cal U}$ ferm\'e tel que $Y_n\subset X_n$ et $m(Y_n)\geq m(X_n)-\frac{\delta}{2^{n+1}}$.
	% 
	% Soit $F_n=\cap_{k=1}^nY_k$. Montrer que $(F_n)_{n\geq 1}$ est une suite d\'ecroissante de ferm\'es et que $m(X_n\setminus F_n)\leq \delta$. En d\'eduire que pour tout $n\geq 1$, $F_n$ n'est pas vide et par un argument topologique que $\displaystyle\cap_{n=1}^{+\infty}F_n\not=\emptyset$.
	% 
	% D\'eduire des r\'esultats pr\'ec\'edents que la suite $(m(X_n))_{n\geq 1}$ tend vers 0.
	% 
	% \qst Soit $(X_n)_{n\geq 1}$ une suite  croissante d'\'el\'ements de $\cal U$ tels que $X=\displaystyle\cup_{n=1}^{+\infty}X_n\in{\cal U}$. Montrer que 
	% la suite $(m(X_n))_{n\geq 1}$ croit vers $m(X)$.
	
	\item Montrer que si $(X_n)_{n\in \NN^{*}}$ est une suite  d'éléments de $\mathcal{U}$ deux à deux disjoints, alors 
	\[     \sum_{n=1}^{+\infty}m(X_n)<+\infty.     \]
\end{enumerate}

\begin{preuve}
	\begin{enumerate}
		\item Comme tout point est un intervalle de longueur zéro, le résultat est immédiat. 
		\item On remarque d'abord que tout élément $X \in \mathcal{U}$ est de la forme $X = \cup_{i=1}^{n} I_{i}$, où $I_{i} \subseteq [a,b]$ est un intervalle avec bornes $a_{i} \leq b_{i}$ pour tout $i \in \{ 1 , \dots , n \}$, et $b_{i} < a_{i+1}$, pour tout $i \in \{ 1 , \dots , n - 1 \}$. 
		Soit $X \in \mathcal{U}$ comme ci-dessus. 
		Alors $\mathbb{1}_X = \sum_{i=1}^{n} \mathbb{1}_{I_{i}]}$ est une fonction en escalier pour la subdivision $a \leq a_{1} \leq b_{1} < \dots < a_{n} \leq b_{n} \leq b$. 
		Réciproquement, soit $X \subseteq [a,b]$ tel que $\mathbb{1}_X$ est un fonction en escalier pour la subdivision $a = c_{0} < \dots < c_{n+1}=b$, alors, pour tout $i \in \{0, \dots, n\}$, $\hskip 0.6mm] \hskip 0.6mm c_{i}, c_{i+1} \hskip 0.6mm [ \hskip 0.6mm \subseteq X$ ou $\hskip 0.6mm] \hskip 0.6mm c_{i}, c_{i+1} \hskip 0.6mm [ \hskip 0.6mm \cap X = \emptyset$. 
		Soient $I = \{ i \in \{0, \dots, n\} : \hskip 0.6mm] \hskip 0.6mmc_{i}, c_{i+1} \hskip 0.6mm [ \hskip 0.6mm \subseteq X \}$ et $J = i \in \{0, \dots, n\} : c_{i} \in X \}$
		Alors $X$ es la réunion des points dans $J$ et des intervalles dans $I$. 
		
		\item Soient $A = \cup_{\ell \in L} I_{\ell}$ et $B = \cup_{\ell' \in L'} J_{\ell'}$, avec $L$ et $L'$ deux ensembles finis d'indices et 
		$I_{\ell}, J_{\ell'} \subseteq [a,b]$ des intervalles pour tout $\ell \in L$ et $\ell' \in L'$. 
		Alors $A\cup B = \cup_{\ell \in L} I_{\ell} \cup \cup_{\ell' \in L'} J_{\ell'} \in \mathcal{U}$. 
		En outre, comme l'intersection de deux intervalles est un intervalle,
		$A\cap B = \cup_{\ell \in L} \cup_{\ell' \in L'} (I_{\ell} \cap J_{\ell'})\in \mathcal{U}$. 
		Finalement, c'est clair que si $I \subseteq [a,b]$ est un intervalle, alors $[a,b]\setminus I$ est la réunion de deux intervalles. 
		Alors, $[a,b]\setminus A = \cup_{\ell \in L} ([a,b] \setminus I_{\ell}) \in \mathcal{U}$.
		
		\item Tous les items sont immédiats. 
		
		\item On pose $Y_{n} = \sqcup_{i=1}^{n} X_{n} \in \mathcal{U}$. 
		La réunion étant disjointe implique que 
		\[     s_{n} = m(Y_{n}) = \sum_{i=1}^{n} m(X_{n}).     \]
		Comme $m(X_{n}) \geq 0$ pour tout $n \in \NN^{*}$, la suite $(s_{n})_{n \in \NN^{*}}$ est croissante. 
		En outre, l'inclusion $Y_{n} \subseteq [a,b]$ nous dit que $s_{n} = m(Y_{n}) \leq b-a$, \textit{i.e.} la suite 
		$(s_{n})_{n \in \NN^{*}}$ est bornée. 
		Le théorème de Bolzano-Weierstrass nous dit alors que $(s_{n})_{n \in \NN^{*}}$ est convergente. 
	\end{enumerate}
\end{preuve}

%%%%%%%%%%%%%%%
%\exo{Intégrales de Wallis} 
%Ces intégrales sont données par
%\[     I_n=\int_0^\pi(\sin (x))^n dx,     \]
%pour $n \in \NN$. 
%\begin{enumerate}
%\item Montrer que $I_0=\pi$ et $I_1=2$, et que la suite $(I_n)$ est décroissante.

%\item Montrer la relation de récurrence $I_{n+1}=\frac{n}{n+1}I_{n-1}$ pour tout $n \in \NN^{*}$.

%\item Montrer que, pour tout $n\in\NN$,
%\[     I_{2n}=\pi\frac{(2n)!}{4^n(n!)^2} \text{ et }
%I_{2n+1}=2\frac{4^n(n!)^2}{(2n+1)!}.     \]

%\item Montrer que $I_{2n}\sim I_{2n+1}\sim I_{2n+2}$, et en déduire que
%\[     I_{2n}^2\sim I_{2n}I_{2n+1}\sim\frac{\pi}{n}.     \]
%\end{enumerate}

\subsection{Quelques applications}

%%%%%%%%%%%%%%%
\exo{Produit de Wallis}
\begin{enumerate}
	\item Montrer que 
	\[     \int \sin^{n}(x) dx = - \frac{1}{n} \sin^{n-1}(x) \cos (x) + \frac{n-1}{n} \int \sin^{n-2}(x) dx,     \]
	pour tout entier $n \geq 2$. 
	
	\item À partir d'une récurrence et de l'item précédent, en déduire que 
	\[     \int_{0}^{\pi/2} \sin^{2n}(x) dx = \frac{2n-1}{2n} \frac{2n-3}{2n-2} \dots \frac{1}{2} \frac{\pi}{2} 
	= \frac{\pi}{2} \prod_{i=1}^{n} \frac{2i-1}{2 i}     \]
	et 
	\[     \int_{0}^{\pi/2} \sin^{2n+1}(x) dx = \frac{2n}{2n+1} \frac{2n-2}{2n-1} \dots \frac{2}{3} 
	= \prod_{i=1}^{n} \frac{2i}{2 i+1},     \]
	pour tout $n \in \NN$.
	
	\item Utiliser l'item précédent et le fait que les puissances de $\sin(x)$ sont décroissantes (pour $x \in [0,\pi/2]$) pour conclure que  
	\[     \frac{1}{1+1/(2n)} \leq \frac{\int_{0}^{\pi/2} \sin^{2n+1} (x) dx}{\int_{0}^{\pi/2} \sin^{2n}(x) dx} \leq 1,     \]
	pour tout $n \in \NN^{*}$. 
	
	\item En déduire le \emph{produit de Wallis} 
	\[     \frac{\pi}{2} = \underset{n \rightarrow + \infty}{\lim}  \prod_{i=1}^{n} \frac{(2i)^{2}}{(2 i-1)(2i+1)}.     \]
	
	\item Montrer que 
	\begin{equation}
	\label{eq:pi}
	\sqrt{\pi} = \underset{n \rightarrow + \infty}{\lim}  \frac{(n!)^{2} 2^{2n}}{(2n)! n^{1/2}}.     
	\end{equation}
	\\
	\textbf{Indication :} réécrire le produit de Wallis sous la forme
	\[     \frac{\pi}{2} = \underset{n \rightarrow + \infty}{\lim}  \prod_{i=1}^{n} \frac{(2i)^{2}}{(2 i-1)^{2}} \frac{1}{2n+1}     \]
	et prendre la racine carrée. 
\end{enumerate}

\begin{preuve}
	La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}

%%%%%%%%%%%%%%%
\exo{Formule de Stirling}
\begin{enumerate}
	\item Soient $\phi, \psi : \hskip 0.6mm ]\hskip 0.6mm -1,1 \hskip 0.6mm [ \hskip 0.6mm \rightarrow \RR$ définies par 
	\[     \phi(x) = \frac{1}{2} \ln\bigg( \frac{1+x}{1-x} \bigg) - x     
	\text{ et  }
	\psi(x) = \phi(x) - \frac{x^{3}}{3(1-x^{2})}.     \]
	Montrer que 
	\[     \phi'(x) = \frac{x^{2}}{1-x^{2}} \text{ et } \psi'(x) = \frac{-2 x^{4}}{3(1-x^{2})^{2}}     \]
	pour tout $x \in \hskip 0.6mm ]\hskip 0.6mm -1,1 \hskip 0.6mm [ \hskip 0.6mm$. 
	En déduire que $\phi(x) > 0$ et $\psi(x) < 0$, pour tout $x \in \hskip 0.6mm ]\hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
	
	\item En déduire que 
	\[     0 \leq \frac{1}{2} \ln\bigg( \frac{1+x}{1-x} \bigg) - x \leq \frac{x^{3}}{3(1-x^{2})},     \]
	pour tout $x \in [0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
	
	\item Soit $x_{n} = 1/(2n+1)$, pour $n \in \NN$. 
	Montrer que 
	\[     \frac{1+x_{n}}{1-x_{n}} = \frac{n+1}{n}     
	\text{ et }      \frac{x_{n}^{3}}{3(1-x_{n}^{2})} = \frac{1}{12 (2n+1)n (n+1)},     \]
	pour tout $n \in \NN^{*}$. 
	
	\item En déduire que 
	\[     0 \leq \frac{1}{2} \ln\bigg( \frac{n+1}{n} \bigg) - \frac{1}{2 n + 1} \leq \frac{1}{12 (2n+1)n (n+1)},     \]
	ou, de façon équivalente,
	\[     0 \leq \bigg(n + \frac{1}{2}\bigg) \ln\bigg( \frac{n+1}{n} \bigg) - 1 \leq \frac{1}{12} \bigg(\frac{1}{n} - \frac{1}{n+1}\bigg),     \]
	pour tout $n \in\NN^{*}$. 
	
	\item Soient $(a_{n})_{n \in \NN^{*}}$ et $(b_{n})_{n \in \NN^{*}}$ deux suites réelles données par 
	\[     a_{n} = \frac{n^{n+1/2} e^{-n}}{n!} \text{ et } b_{n} = a_{n} e^{1/(12 n)}.     \]
	Montrer que $a_{n} \leq b_{n}$, $a_{n} \leq a_{n+1}$ et $b_{n+1} \leq b_{n}$, pour tout $n \in \NN^{*}$. 
	En déduire qu'il existe un unique nombre réel $c > 0$ tel que $a_{n} \leq c \leq b_{n}$, pour tout $n \in \NN^{*}$. 
	
	\item En déduire que, pour tout $n \in \NN^{*}$, il existe $\theta_{n} \in [0,1]$ tel que
	\begin{equation}
	\label{eq:STIR}
	n! = c^{-1} n^{n+1/2} e^{-n} e^{\theta_{n}/(12n)}. 
	\end{equation}
	
	\item Utiliser \eqref{eq:pi} pour montrer que $c = 1/\sqrt{2 \pi}$. 
	\\
	\textbf{Indication :} 
	remplacer $n$ par $2n$ dans \eqref{eq:STIR} et prendre la limite quand $n$ tend vers $+ \infty$ pour obtenir
	\[     c = \underset{n \rightarrow + \infty}{\lim} \frac{(2n)^{2n+1/2} e^{-2 n}}{(2n)!} = \underset{n \rightarrow + \infty}{\lim} \frac{(n!)^{2} 2^{2n} \sqrt{2}}{(2n)! \sqrt{n}} \bigg( \frac{n^{n+1/2} e^{-n}}{n!} \bigg)^{2} = \sqrt{2 \pi} c^{2},     \]
	où l'on a utilisé \eqref{eq:pi} dans la dernier égalité.  
\end{enumerate}

\begin{preuve}
	La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}


\newpage \section{Intégration élémentaire (2ème partie)}


	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f:[a,b]\rightarrow\RR$ une fonction continue par morceaux.
	\begin{enumerate}
		\item Montrer que $\int_a^bf(x)dx=\int_a^bf(a+b-x)dx$.
		
		\item En déduire la valeur des intégrales suivantes :
		\\
		\begin{enumerate*}[label=(\roman*)]
			\item $\int_0^{\pi}\frac{x\sin(x)}{1+\cos^2(x)}dx$, 
			\item $\int_0^{\frac{\pi}{4}}\ln (1+\tan(x))dx$. 
		\end{enumerate*}
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item Nous ne pouvons pas appliquer directement le théorème de changement de variable car la fonction $f$ n'est pas supposé continue.
			
			Si $f$ est une fonction en escalier sur $[a,b]$, on montre que $\tilde f(x)=f(a+b-x)$ est aussi une fonction en escalier sur $[a,b]$ telle que $\int_a^b f = \int_a^b \tilde f$. Par densité des fonctions en escaliers, on montre que la propriété reste vrai sur les fonctions continue par morceaux.
			
			
			\item L'égalité précédente nous dit que 
			\[     \int_0^{\pi}\frac{x\sin(x)}{1+\cos^2(x)}dx = \int_0^{\pi}\frac{(\pi -x)\sin(x)}{1+\cos^2(x)}dx,     \]
			où l'on a utilisé que $\sin(\pi - x) = \sin(x)$ et $\cos(\pi - x) = - \cos(x)$. 
			En conséquence, 
			\[     \int_0^{\pi}\frac{x\sin(x)}{1+\cos^2(x)}dx = \frac{1}{2} \int_0^{\pi} \frac{\pi \sin(x)}{1+\cos^2(x)}dx = -\frac{\pi}{2} \bigg[ \arctan\big(\cos(x)\big) \bigg]_{0}^{\pi} = \frac{\pi^{2}}{4}.     \]
			De même, 
			\[     \int_0^{\frac{\pi}{4}}\ln (1+\tan(x))dx = \int_0^{\frac{\pi}{4}}\ln\bigg(\frac{2}{1+\tan(x)}\bigg)dx,     \]
			où l'on a utilisé que $\sin(\pi/4 - x) = \sqrt{2}(\cos(x) - \sin(x))/2$ et $\cos(\pi/4 - x) = \sqrt{2}(\cos(x) + \sin(x))/2$. 
			En conséquence, 
			\[     \int_0^{\frac{\pi}{4}}\ln (1+\tan(x))dx = \frac{\pi \ln(2)}{8}.     \]
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}\label{exo:2} Soit $f:[a,b]\rightarrow\C$ continue et soit $g:[a,b]\rightarrow\RR_{\geq0}$ continue et décroissante. 
	Pour $x\in[a,b]$, on note $F(x)=\int_a^xf(t)dt$ et $H(x)=\int_a^x|f(t)|dt$.
	Soit $N \in \NN^{*}$ et $x_j=a+j(b-a)/N$, pour tout $j \in \{ 0, \dots, N\}$.
	Soient 
	\[     I_N= \sum_{j=0}^{N-1}\int_{x_j}^{x_{j+1}}f(t)g(x_j)dt \text{ et } I=\int_a^bf(t)g(t)dt.     \]
	
	\begin{enumerate}
		\item Montrer que 
		\[     | I_N - I | \leq \sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\big(H(x_{j+1})-H(x_j)\big).     \]
		En déduire que $I_N$ tend vers $I$ quand $N$ tend vers $+\infty$.
		
		\item\label{item:b} Montrer que 
		\[     I_N=\sum_{j=0}^{N-1}g(x_j)\big(F(x_{j+1})-F(x_j)\big)=g(b)F(b)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)F(x_{j+1}).     \]
		En déduire que 
		\[     \bigg|\int_a^bf(t)g(t)dt\bigg|\leq g(a)\sup_{c\in[a,b]}\bigg|\int_a^cf(t)dt\bigg|.     \]
		
		\item On suppose maintenant que $f$ est une fonction à valeurs réelles. 
		Montrez qu'il existe $c\in[a,b]$ tel que 
		\[     \int_a^bf(t)g(t)dt= g(a)\int_a^cf(t)dt.     \]
		\\
		\textbf{Indication :} se servir de la fonction $G(x) = \operatorname{max}(F(x),0)$ pour majorer l'expression de $I_{N}$ dans l'item précédent. 
		%Quelle ressemblance y a-t-il avec un théorème du cours ?
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit bien que 
			\begin{align*}
			| &I_N - I | \leq \bigg| \sum_{j=0}^{N-1}\int_{x_j}^{x_{j+1}}f(t)\big(g(x_j)-g(t)\big)dt \bigg| 
			\leq \sum_{j=0}^{N-1}\int_{x_j}^{x_{j+1}} \Big| f(t)\big(g(x_j)-g(t)\big) \Big| dt 
			\\
			&\leq \sum_{j=0}^{N-1} \big(g(x_j)-g(x_{j+1})\big) \int_{x_j}^{x_{j+1}} |f(t)| dt 
			= \sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\big(H(x_{j+1})-H(x_j)\big).     
			\end{align*}
			Comme la fonction $H$ est continue sur un intervalle fermé et borné, elle est uniformément continue.
			En particulier, étant donné $\epsilon > 0$, il existe $\delta > 0$ tel que $|x-y| < \delta$ implique $|H(x) - H(y)| < \epsilon/(g(a)-g(b))$. 
			On fixe $\epsilon > 0$ et on prend $N_{0} \in \NN^{*}$ tel que $N_{0} > 1/\delta$. 
			Alors 
			\[     \sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\big(H(x_{j+1})-H(x_j)\big) < \frac{\epsilon}{g(a)-g(b)} \sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big) = \epsilon,     \]
			pour tout $N \geq N_{0}$. 
			Cela implique que $I_{N}$ converge vers $I$ quand $N$ tend vers $+ \infty$. 
			
			\item L'égalité 
			\[     I_N=\sum_{j=0}^{N-1}g(x_j)\big(F(x_{j+1})-F(x_j)\big)     \]
			suit directement de la définition de $F$. 
			En outre, 
			\begin{align*}
			\sum_{j=0}^{N-1}g(x_j)\big(F(x_{j+1})-F(x_j)\big) &= \sum_{j=0}^{N-1}g(x_j) F(x_{j+1}) - \sum_{j=0}^{N-1}g(x_j) F(x_j) 
			\\
			&= \sum_{j=0}^{N-1}g(x_j) F(x_{j+1}) +g(b) F(b) - \sum_{j=1}^{N}g(x_j) F(x_j) 
			\\
			&= \sum_{j=0}^{N-1}g(x_j) F(x_{j+1}) +g(b) F(b) - \sum_{j=0}^{N-1}g(x_{j+1}) F(x_{j+1}) 
			\\
			&= g(b)F(b)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)F(x_{j+1}),
			\end{align*}
			où l'on a utilisé dans la troisième égalité que $F(a) = 0$ et on a changé l'indice $j$ par $j+1$ dans la dernière somme dans la quatrième égalité. 
			
			La deuxième égalité de cet item implique que, pour tout $N \in \NN^{*}$, 
			\begin{align*}     
			|I_{N}| &\leq g(b)\big|F(b)\big|+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\big|F(x_{j+1})\big| 
			\\
			&\leq 
			\Big(g(b)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\Big) \sup_{c\in[a,b]}\bigg|\int_a^cf(t)dt\bigg| 
			= g(a)\sup_{c\in[a,b]}\bigg|\int_a^cf(t)dt\bigg|,     
			\end{align*}
			où l'on a utilisé que $|F(x)| \leq \sup\{ |F(c)| : c \in [a,b] \}$, pour tout $x \in [a,b]$. 
			Si l'on prend la limite quand $N$ tend vers $+ \infty$ on trouve l'inégalité demandée. 
			
			\item D'abord on peut supposer sans perte de généralité que $I \neq 0$ (sinon $c=a$ marche), et même $I > 0$ (sinon, on peut changer $f$ par $-f$). 
			On suppose aussi $g(a) > 0$ (sinon $c=a$ marche). 
			On définit la fonction continue $G(x) = \operatorname{max}(0,F(x))$, et on note que 
			En outre, 
			\begin{align*}
			I_{N} &= g(b)F(b)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)F(x_{j+1}) 
			\\ 
			&\leq g(b)G(b)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)G(x_{j+1})  
			\\
			&\leq g(b)\sup_{c \in [a,b]} G(c)+\sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big)\sup_{c \in [a,b]} G(c) 
			\\
			&= \bigg( g(b) + \sum_{j=0}^{N-1}\big(g(x_j)-g(x_{j+1})\big) \bigg) \sup_{c \in [a,b]} G(c) = g(a) \sup_{c \in [a,b]} G(c),
			\end{align*}
			Le premier item nous dit alors que $I \leq g(a) \sup_{c \in [a,b]} G(c)$, ce qui implique qu'il existe $c \in [a,b]$ tel que $I = g(a) G(c)$, d'après le théorème de la valeur intermédiaire, vu que $G(a) = 0$. 
			Comme $I, g(a) >0$, alors $G(c)$ est aussi positif et donc il coïncide avec $F(c)$.
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	Soit $f:\RR\rightarrow\RR$ une fonction continue et décroissante, tendant vers $0$ en $+\infty$.
	Montrez que la suite de terme général $\int_n^{n^2}f(t)e^{it}dt$ admet une limite quand $n$ tend vers $+\infty$. 
	
	\begin{preuve}
		Il s'agit d'un conséquence immédiate de l'exercice \ref{item:b}. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Irrationalité de $\pi$} 
	Dans cet exercice, on va montrer que $\pi$ est irrationnel (on utilise la définition de $\pi$ donnée dans l'exercice \textbf{3} de la fiche 0). 
	On procède par l'absurde: on suppose alors $\pi = p/q$, avec $p, q \in \NN^{*}$ premiers entre eux. 
	Pour tout $n \in \NN$, on définit le polynôme 
	\[     f_{n}(x) = \frac{x^{n}(p-qx)^{n}}{n!}.     \]
	\begin{enumerate}
		\item Montrer que $f_{n}(0) = 0$ et que $f_{n}(\pi - x) = f_{n}(x)$, pour tout $n \in \NN^*$. 
		En déduire que $f_{n}^{(k)}(\pi - x) = (-1)^{k} f_{n}^{(k)}(x)$, pour tous $n, k \in \NN^*$. 
		
		\item Montrer que $f_{n}^{(k)}(0) \in \Z$, pour tous $n, k \in \NN$. 
		En déduire que $f_{n}^{(k)}(\pi) \in \Z$, pour tous $n, k \in \NN$. 
		
		\item On pose 
		\[     F_{n}(x) = \sum_{k=0}^{n} (-1)^{k} f_{n}^{(2k)}(x),     \]
		pour tout $n \in \NN$. 
		Montrer que $F_{n}(0), F_{n}(\pi) \in \Z$, pour tout $n \in \NN$. 
		
		\item Montrer que $F_{n}(x) + F_{n}''(x) = f_{n}(x)$ et que 
		\[     \frac{d}{dx}\big(F'_{n}(x) \sin(x) - F_{n}(x) \cos(x) \big) = f_{n}(x) \sin(x),     \]
		pour tout $n \in \NN$. 
		En déduire que 
		\[     \int_{0}^{\pi} f_{n}(x) \sin(x) dx = F_{n}(\pi) + F_{n}(0) \in \Z.     \]
		
		\item Montrer que 
		\[     0 < f_{n}(x) \sin(x) < \frac{p^{n} \pi^{n}}{n!},     \]
		pour tout $n \in \NN$ et $x \in \hskip0.6mm ] \hskip0.6mm 0,\pi \hskip0.6mm [ \hskip0.6mm$. 
		En déduire que 
		\[     0 < \int_{0}^{\pi} f_{n}(x) \sin(x) dx < \frac{\pi (p \pi)^{n}}{n!},     \]
		pour tout $n \in \NN$. 
		Conclure. 
	\end{enumerate}
	
	\begin{preuve}
		La solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Inégalité de H\"older} Soient $p, q \in \RR$ tels que $p, q > 1$ et $1/p + 1/q=1$. 
	Soient $f$ et $g$ deux fonctions continues de $[a,b]$ dans $\RR$.
	\begin{enumerate}
		\item Montrer que, pour tout $u,v \in \RR_{\geq 0}$, $u^{1/p} v^{1/q} \leq u/p+v/q$. 
		
		\item En déduire que 
		\[     \bigg|\int_a^bf(t)g(t)dt\bigg|\leq\left( \int_a^b|f(t)|^pdt\right)^{\frac{1}{p}} \left( \int_a^b|g(t)|^qdt\right)^{\frac{1}{q}}.     \]
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item L'identité est immédiate si $u=0$ ou $v=0$. 
			On suppose alors $u, v >0$. 
			Sans perte de généralité on peut supposer que $u \geq v$.  
			On pose $x = u/v$. 
			L'identité $u^{1/p} v^{1/q} \leq u/p+v/q$ devient $x^{1/p} \leq 1/q + x/p$. 
			Elle est vérifiée pour $x =1$. 
			En outre, la dérivée $x^{-1/q}/p$ du membre de gauche est clairement inférieure à la dérivée $1/p$ du membre de droite pour $x >1$. 
			Cela implique que  $x^{1/p} \leq 1/q + x/p$, pour tout $x \geq 1$, ce qui montre l'inégalité demandée. 
			
			\item Pour $t \in [a,b]$, on prend 
			\[     u(t) = \frac{|f(t)|^{p}}{\int_a^b|f(t)|^p dt} \text{ et } v(t) = \frac{|g(t)|^{q}}{\int_a^b|g(t)|^q dt}.     \]
			D'après l'inégalité dans l'item précédent, on voit que 
			\[     u(t)^{1/p} v(t)^{1/q} \leq \frac{1}{p} \frac{|f(t)|^{p}}{\int_a^b|f(t)|^p dt} + \frac{1}{q} \frac{|g(t)|^{q}}{\int_a^b|g(t)|^q dt}.     \]
			Si l'on intègre sur $[a,b]$ les fonctions dans l'inégalité précédente on déduit que 
			\[     \bigg|\int_a^bf(t)g(t)dt\bigg|\leq\left( \int_a^b|f(t)|^pdt\right)^{\frac{1}{p}} \left( \int_a^b|g(t)|^qdt\right)^{\frac{1}{q}}.     \]
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f:\hskip 0.6mm ] \hskip 0.6mm 0,1\hskip 0.6mm[ \hskip 0.6mm \rightarrow \RR$ la fonction définie par 
	$x \mapsto (x-1)/\ln(x)$.
	\begin{enumerate}
		\item Montrer que $f$ se prolonge en une fonction continue sur $[0,1]$.
		
		\item Soit $\varphi : \hskip 0.6mm]\hskip 0.6mm 0,1\hskip 0.6mm [\hskip 0.6mm \rightarrow \RR$ définie par $\varphi(x)=\int_x^{x^2} dt/\ln(t)$. 
		Montrer que $\varphi$ se prolonge en une fonction continue sur $[0,1]$. 
		\\
		\textbf{Indication :} pour la limite en 1, on pourra utiliser une primitive de $1/ (t \ln(t))$.
		
		\item Montrer que $\varphi$ est dérivable sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
		Est-elle dérivable en $0$ et $1$ ?
		
		\item Calculer $\int_0^1f(t) dt$.
	\end{enumerate}
	\begin{preuve}
		\begin{enumerate}
			\item C'est clair que la limite de $f(x)$ quand $x>0$ tend vers $0$ est $0$. 
			En outre, la règle de Bernoulli-L'H\^opital nous dit que
			\[     \underset{x \rightarrow 1-}{\lim} \frac{x-1}{\ln(x)} = \underset{x \rightarrow 1-}{\lim} \frac{1}{1/x} = 1.     \]
			
			\item C'est clair que la limite de $\varphi(x)$ quand $x>0$ tend vers $0$ est $0$. 
			Par ailleurs,  
			\begin{equation}
			\label{eq:int}
			\begin{split}
			\int_{x}^{x^{2}} \frac{dt}{\ln(t)} &= \int_{x}^{x^{2}} \frac{(t-1) dt}{t \ln(t)} + \int_{x}^{x^{2}} \frac{dt}{t \ln(t)} 
			= \int_{x}^{x^{2}} \frac{(t-1) dt}{t \ln(t)} + \bigg[ \ln\big(\big|\ln(t)\big|\big) \bigg]_{x}^{x^{2}}
			\\
			&= \int_{x}^{x^{2}} \frac{(t-1) dt}{t \ln(t)} + \ln\bigg(\bigg|\frac{\ln(x^{2})}{\ln(x)}\bigg|\bigg) = \int_{x}^{x^{2}} \frac{(t-1) dt}{t \ln(t)} + \ln(2), 
			\end{split}
			\end{equation} 
			pour tout $x \in \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
			En plus, d'après l'item précédent, la fonction $t \mapsto (t-1)/(t \ln(t))$ peut se prolonger par continuité en $t = 1$, ce qui nous dit que l'intégrale 
			\[     G(x) = \int_{x}^{1} \frac{(t-1) dt}{t \ln(t)}     \]
			converge et elle est une fonction continue sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 ]$. 
			Cela implique que  
			\[     \underset{x \rightarrow 1-}{\lim} \varphi(t) = \ln(2).     \]
			
			\item Soit $F(t)$ une primitive de $1/\ln(t)$ définie sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
			Alors $\varphi(x) = F(x^{2}) - F(x)$ sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$ et en particulier 
			\[     \varphi'(x) = 2x F'(x^{2}) - F'(x) = \frac{2x}{\ln(x^{2})} - \frac{1}{\ln(x)} = \frac{x}{\ln(x)} - \frac{1}{\ln(x)} = \frac{x-1}{\ln(x)},     \] 
			pour tout $x \in \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
			En outre, la règle de Bernoulli-L'H\^opital nous dit que 
			\[     \underset{x \rightarrow 0+}{\lim} \frac{\varphi(x)}{x} = \underset{x \rightarrow 0+}{\lim} \frac{\varphi'(x)}{1} = \underset{x \rightarrow 0+}{\lim} \frac{x-1}{\ln(x)} = 0.     \]
			Le même argument nous dit que la dérivée de $\varphi(x)$ en $x=1$ est 
			\[     \underset{x \rightarrow 1-}{\lim} \frac{x-1}{\ln(x)} = 1.     \]
			
			\item C'est clair que 
			\[     \int_0^1f(t) dt = \underset{x \rightarrow 1-}{\lim} \varphi(t) = \ln(2).     \]
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On définit $f : \RR_{\geq 0} \rightarrow \RR$ via $f(x)=e^{-x^2}\int_0^xe^{t^2} dt$.
	\begin{enumerate}
		\item Montrer que 
		\[     f(x)\geq \frac{1}{2x}-\frac{e^{-x^2}}{2x},     \]
		pour tout $x\in \RR_{\geq 0}$.
		
		\item En découpant l'intervalle $[0,x]$ en $[0,x-\sqrt{x}]$ et $[x-\sqrt{x},x]$ pour $x > 1$, montrer que $f(x)$ est équivalent à $1/(2x)$ en $+\infty$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On remarque que 
			\[     f(x)\geq \frac{1}{2x}-\frac{e^{-x^2}}{2x}     \]
			est équivalente à 
			\[     2x \int_0^xe^{t^2} dt \geq e^{x^2}-1,     \]
			pour tout $x\in \RR_{\geq 0}$. 
			Pour démontrer cette inégalité, on observe qu'elle est vérifiée si $x = 0$ et pour $x > 0$ la dérivée $2 x e^{x^{2}} + 2 \int_0^x e^{t^2} dt$ du membre de gauche est clairement supérieure ou égale à la dérivée $2 x e^{x^{2}}$ du membre de droite.  
			
			\item On voit bien que 
			\[     0 \leq 2x e^{-x^2}\int_0^{x-\sqrt{x}}e^{t^2} dt \leq 2 x (x - \sqrt{x}) e^{-x^{2}} e^{(x-\sqrt{x})^{2}} = \frac{2x (x- \sqrt{x})}{e^{x(2 \sqrt{x}-1)}}.     \]
			Comme le dernier membre de cette inégalité converge vers zéro quand $x$ tend vers $+ \infty$, l'intégrale $\leq 2x e^{-x^2}\int_0^{x-\sqrt{x}}e^{t^2} dt$. 
			
			En outre, une intégration par parties nous dit que 
			\begin{equation}
			\label{eq:int2}
			\begin{split}
			2x e^{-x^2}\int_0^{x-\sqrt{x}}e^{t^2} dt &= 2x e^{-x^2} \bigg[ \frac{e^{t^{2}}}{2t} \bigg]_{x-\sqrt{x}}^{x} + x e^{-x^2} \int_{x-\sqrt{x}}^{x} \frac{e^{t^2}}{t^{2}} dt 
			\\
			&= 1 - \frac{1}{e^{x(2 \sqrt{x}-1)} (1-1/\sqrt{x})} + x e^{-x^{2}} \int_{x-\sqrt{x}}^{x} \frac{e^{t^2}}{t^{2}} dt.      
			\end{split}
			\end{equation} 
			C'est clair que la limite du deuxième opérande dans le dernier membre de \eqref{eq:int2} est zéro quand $x$ tend vers $+ \infty$. 
			Par ailleurs, 
			\[     0 \leq x e^{-x^{2}} \int_{x-\sqrt{x}}^{x} \frac{e^{t^2}}{t^{2}} dt \leq x e^{-x^{2}} \frac{e^{x^{2}}}{(x-\sqrt{x})^{2}} \sqrt{x} = \frac{1}{\sqrt{x}-2+1/\sqrt{x}}.     \]
			Comme le dernier membre de cette inégalité converge vers zéro quand $x$ tend vers $+ \infty$, 
			la limite du dernier opérande dans le dernier membre de \eqref{eq:int2} est zéro quand $x$ tend vers $+ \infty$. 
			Cela montre que \eqref{eq:int2} converge vers $1$ quand $x$ tend vers $+ \infty$. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} \label{exo:8} Déterminer la limite des sommes suivantes quand $n$ tend vers $+\infty$, en les interprétant comme des sommes de Riemann:
	\\
	\begin{enumerate*}[label=(\alph*)]
		\item $S_n=\sum_{k=1}^n \frac{n+k}{n^2+k^2}$,
		\item $S_n=\sum_{k=1}^n \frac{k^2}{n^2(n^3+k^3)^{1/3}}$,
		\item $S_n=\sum_{k=1}^n \frac{k}{n^2}\sin (k\pi/(n+1))$,
	\end{enumerate*}
	\\
	\begin{enumerate*}[label=(\alph*)]
		\setcounter{enumi}{3}
		\item $S_n=\frac{1}{n}\sum_{k=0}^{n-1}\cos^2(k\pi/n)$,
		\item $S_n=\sum_{k=0}^{n-1}\frac{1}{2n+3k}$,
		\item $S_n=\sum_{k=0}^{n-1}\frac{1}{\sqrt{n^2-k^2}}$, 
	\end{enumerate*}
	\\
	\begin{enumerate*}[label=(\alph*)]
		\setcounter{enumi}{6}
		\item $S_n=\sum_{k=n}^{2n}\frac{\pi}{k}$. 
	\end{enumerate*}
	En déduire la valeur de la limite
	\[     \underset{n \rightarrow +\infty}{\lim} \sum_{k=n}^{2n} \sin (\pi/k).     \]
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit bien que 
			\[      S_n=\sum_{k=1}^n \frac{n+k}{n^2+k^2} = \sum_{k=1}^n \frac{1}{n} \frac{1+k/n}{1+(k/n)^2}     \]
			coïncide avec une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = (1+x)/(1+x^{2})$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} \frac{1+x}{1+x^{2}} dx = \bigg[ \arctan(x) + \frac{\ln(1+x^{2})}{2} \bigg]_{0}^{1} = \frac{\pi + \ln(4)}{4}.     \]
			
			\item C'est clair que 
			\[      S_n=\sum_{k=1}^n \frac{k^2}{n^2(n^3+k^3)^{1/3}} = \sum_{k=1}^n \frac{1}{n} \frac{(k/n)^2}{(1+(k/n)^3)^{1/3}}     \]
			coïncide avec une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = x^{2}/(1+x^{3})^{1/3}$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} \frac{x^{2}}{\sqrt[3]{1+x^{3}}} dx = \bigg[ \frac{1}{2} (1+x^{3}) ^{2/3} \bigg]_{0}^{1} = \frac{1}{\sqrt[3]{2}} - \frac{1}{2}.     \]
			
			\item On voit bien que 
			\[      S_n =\sum_{k=1}^n \frac{k}{n^2}\sin (k\pi/(n+1)) = \bigg(\frac{n+1}{n}\bigg)^{2} \sum_{k=0}^n \frac{1}{n+1} \frac{k}{n+1}\sin (k\pi/(n+1))     \]
			est le produit de $(n+1)^{2}/n^{2}$ et une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = x \sin(\pi x)$ 
			associée à la subdivision $\{ x_{j} = j/(n+1) : j \in \{0, \dots, n+1\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} x \sin(\pi x) dx = \bigg[ \frac{\sin(\pi x)}{\pi^{2}} - \frac{x \cos(\pi x)}{\pi} \bigg]_{0}^{1} = \frac{1}{\pi}.     \]
			
			\item C'est clair que 
			\[      S_n =\frac{1}{n}\sum_{k=0}^{n-1}\cos^2(k\pi/n)     \] 
			coïncide avec une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = \cos^{2}(\pi x)$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} \cos^{2}(\pi x) dx = \bigg[ \frac{x}{2} + \frac{\sin(2 \pi x)}{4 \pi} \bigg]_{0}^{1} = \frac{1}{2}.     \]
			
			\item On voit bien que 
			\[      S_n =\sum_{k=0}^{n-1}\frac{1}{2n+3k} = \sum_{k=0}^{n-1} \frac{1}{n} \frac{1}{2+3k/n}     \]
			est une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = 1/(2+3x)$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} \frac{1}{2+3x} dx = \bigg[ \frac{\ln(3 x + 2)}{3} \bigg]_{0}^{1} = \frac{\ln(5/2)}{3}.     \]
			
			\item C'est clair que 
			\[      S_n =\sum_{k=0}^{n-1}\frac{1}{\sqrt{n^2-k^2}} = \sum_{k=0}^{n-1} \frac{1}{n} \frac{1}{\sqrt{1-(k/n)^2}}     \]
			est une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = 1/\sqrt{1-x^{2}}$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. Hélas, la fonction n'étant pas bornée, elle n'est pas intégrable et le théorème des sommes de Riemann ne s'applique pas. On fixe $A\in [0,1[$ et on coupe la somme en deux
			\[      S_n =\sum_{k=0, k\leq An}\frac{1}{n} \frac{1}{\sqrt{1-(k/n)^2}} +  \sum_{k\in ]An,n-1]} \frac{1}{\sqrt{n^2-k^2}}.\]
			La première somme correspond à la somme de Riemann sur l'intervalle $[1,A]$ et converge donc quand $n\to +\infty$ vers $\int_0^Af=\arcsin A$.
			La seconde somme ce majore par le théorème de comparaison série intégrale
			\[
			\sum_{k\in ]An,n-1]} \frac{1}{\sqrt{n^2-k^2}} \leq \frac1{n}\sum_{p\in [1,n-An]}\frac{1}{\sqrt{p}} \leq 1 +\int_1^{n-An}\frac1{\sqrt t}\,dt \leq 2\sqrt{1-A}.
			\]
			
			En conséquence, pour tout $\epsilon>0$ fixé, on choisit $A\in [0,1[$ telle que $2\sqrt{1-A}\leq \epsilon/3$ et $|\arcsin A-\arcsin 1 |\leq \epsilon/3$, puis on fixe $N$ assez grand telle que la première somme soit proche de $\arcsin A$ à  $\epsilon/3$ près. On a donc pour $n\geq N$, $|S_n-\arcsin 1|\leq \epsilon$, ce qui implique que 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \arcsin1= \frac{\pi}{2}.     \]
			
			\item On voit bien que 
			\[      S_n =\sum_{k=n}^{2n}\frac{\pi}{k} = \sum_{k=0}^{n} \frac{\pi}{k+n} = \sum_{k=0}^{n} \frac{1}{n} \frac{\pi}{1+k/n} = \frac{\pi}{n}+\sum_{k=1}^{n} \frac{1}{n} \frac{\pi}{1+k/n}      \]
			est la somme de $\pi/n$ et une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = \pi/x$ 
			associée à la subdivision $\{ x_{j} = 1+ j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[1,2]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{1}^{2} \frac{\pi}{x} dx = \pi \bigg[ \ln(x) \bigg]_{1}^{2} = \pi \ln(2).     \]
			
			Par ailleurs, comme $|\sin(x)| \leq |x|$, pour tout $x \in \RR$ et 
			\[     \underset{x \rightarrow 0}{\lim} \frac{\sin(x)}{x} = 1     \]
			alors, pour tout $\epsilon > 0$, il existe $\delta > 0$ tel que $-\epsilon x \leq \sin(x) - x \leq 0$, pour tout $x \in [-\delta,\delta]$. 
			Soit $n_{0} \in \NN$ tel que $n_{0} > \pi/\delta$. 
			Alors, si $n \geq n_{0}$, 
			\[      -\epsilon \sum_{k=n}^{2n}\frac{\pi}{k} \leq \sum_{k=n}^{2n} \sin\bigg(\frac{\pi}{k}\bigg) - \sum_{k=n}^{2n}\frac{\pi}{k} \leq 0.     \]
			Si l'on prend la limite inférieure ou supérieure quand $n$ tend vers $+ \infty$, on trouve que 
			\[      -\epsilon \pi \ln(2) \leq \underset{n \rightarrow +\infty}{\operatorname{liminf}} \sum_{k=n}^{2n} \sin\bigg(\frac{\pi}{k}\bigg) - \pi \ln(2) \leq 0,     \]
			et de même pour la limite supérieure. 
			Comme les inégalités précédentes sont valables pour tout $\epsilon > 0$, on conclut que 
			\[     \underset{n \rightarrow +\infty}{\operatorname{liminf}} \sum_{k=n}^{2n} \sin\bigg(\frac{\pi}{k}\bigg) = \pi \ln(2)     \]
			et de même pour la limite supérieure. 
			Cela implique que la limite 
			\[     \underset{n \rightarrow +\infty}{\lim} \sum_{k=n}^{2n} \sin (\pi/k)     \]
			existe et vaut $\pi \ln(2)$.
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit 
	\[     S_n=\frac{1}{n}\sum_{k=0}^{n-1}\cos (ka/n),     \] 
	avec $a \in \RR \setminus \{ 0 \}$.
	\begin{enumerate}
		\item Évaluer directement $\lim_{n \rightarrow + \infty}S_n$. 
		% partie reelle de exp
		\item Évaluer la même limite en utilisant que $S_n$ est une somme de Riemann.
		%ecrire somme jsqu $2n$-$2$ fois la somme des pairs usqua n. 
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit bien que 
			\begin{align*}
			S_n &= \frac{1}{n}\sum_{k=0}^{n-1}\cos (ka/n) 
			= \frac{1}{n} \operatorname{Re}\bigg( \sum_{k=0}^{n-1}e^{i k a/n} \bigg) 
			=  \frac{1}{n} \operatorname{Re}\bigg( \frac{1 - e^{ia}}{1- e^{ia/n}} \bigg)
			\\
			&= \frac{1 - \cos(a) - \cos(a/n) + \cos(a (n-1)/n)}{2 n (1-\cos(a/n))}
			\\
			&= \frac{n}{a^{2}} \frac{1 - \cos(a) - \cos(a/n) + \cos(a (n-1)/n)}{(1-\cos(a/n))/(a/\sqrt{2}n)^{2}}.
			\end{align*}
			Comme la limite de $(1-\cos(x))/(x^{2}/2)$ est $1$ quand $x$ tend vers zéro, on voit que la limite de $S_{n}$ quand $n$ tend vers $+ \infty$ 
			coïncide avec 
			\begin{align*}
			\underset{n \rightarrow + \infty}{\lim}& \frac{n}{a^{2}} \big(1 - \cos(a) - \cos(a/n) + \cos(a (n-1)/n)\big)
			\\
			&= \underset{n \rightarrow + \infty} {\lim} \frac{1-\cos(a)}{\sqrt{2}n} \frac{1 - \cos(a/n)}{(a/\sqrt{2}n)^{2}} + 
			\underset{n \rightarrow + \infty} {\lim} \frac{1}{a} \frac{\sin(a) \sin(a/n)}{a/n}
			= \frac{\sin(a)}{a},
			\end{align*}
			où l'on a utilisé que $\cos(a(n-1)/n) = \cos(a) \cos(a/n) + \sin(a) \sin(a/n)$ et que la limite $\sin(x)/x$ est $1$ quand $x$ tend vers zéro. 
			
			\item C'est clair que $S_{n}$ est une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = \cos(a x)$ 
			associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
			En conséquence, 
			\[     \underset{n \rightarrow +\infty}{\lim} S_{n} = \int_{0}^{1} \cos(a x) dx = \bigg[ \frac{\sin(ax)}{a} \bigg]_{0}^{1} = \frac{\sin(a)}{a}.     \]
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour $n \in \NN$, on pose 
	\[     U_n=\sum_{k=1}^n \frac{1}{k} \text{ et } V_n=\sum_{k=1}^n \frac{(-1)^{k-1}}{k}.     \]
	\begin{enumerate}
		\item Calculer la limite de $U_{2n}-U_n$ quand $n \rightarrow +\infty$ en utilisant une somme de Riemann.
		\item Rappeler pourquoi $\lim_{n \rightarrow +\infty}V_n$ existe et calculer cette limite.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit vien que $U_{2n}-U_n$ coïncide avec $S_{n}/\pi$ dans le dernier item de l'exercice \ref{exo:8}. 
			En conséquence, sa limite existe quand $n$ tend vers $+ \infty$ et elle vaut $\ln(2)$. 
			
			\item Le critère de Leibniz nous dit que la limite de $V_{n}$ existe quand $n$ tend vers $+ \infty$. 
			En outre, on voit bien que $V_{2n} = U_{n} - U_{2 n}$, pour tout $n \in \NN^{*}$. 
			Cela implique que la limite de $V_{n}$ quand $n$ tend vers $+ \infty$ est $\ln(2)$.
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $x\in \RR \setminus \{\pm 1\}$. 
	On pose $f(x) =\int_{0}^{2\pi }\ln ( x^2 - 2x \cos t + 1) dt$.
	\begin{enumerate}
		\item Déterminer le domaine $\operatorname{Dom}(f)$ de $f$. 
		\item Factoriser le polynôme $X^n - 1$ dans $\mathbb{C}[X]$.
		\item Calculer $f (x)$ à l'aide de ses sommes de Riemann. 
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On remarque que 
			\[     f(x) = \int_{0}^{2\pi }\ln ( x^2 - 2x \cos t + 1) dt = \int_{0}^{2\pi }\ln \big(|x- e^{it}|^{2}\big) dt = 2 \int_{0}^{2\pi }\ln (|x- e^{it}|) dt.     \]
			En particulier le domaine $\operatorname{Dom}(f)$ de $f$ est $\RR \setminus \{ \pm 1\}$. 
			\item C'est clair que $X^n - 1 = \prod_{\zeta \in \mathbb{G}_{n}} (X- \zeta)$, où $\mathbb{G}_{n} \subseteq \C$ est l'ensemble formé des racines de l'unité d'ordre $n$. 
			\item Soit 
			\[     S_{n}(x) = \frac{2 \pi}{n} \sum_{k=0}^{n-1} \ln \big(|x- e^{i 2 \pi k/n}|\big) = \frac{2 \pi}{n} \ln \Big( \prod_{k=0}^{n-1} |x- e^{i 2 \pi k/n}| \Big) = 2 \pi \ln\big(|x^{n}-1|^{1/n}\big).     \]
			C'est clair que $S_{n}(x)$ est une somme de Riemman de l'intégral qui donne $f(x)/2$. 
			Comme la limite simple de la suite $|x^{n}-1|^{1/n}$ est $|x|$ si $|x| > 1$ et $1$ si $|x|<1$, on conclut que 
			\[     \underset{n \rightarrow + \infty}{\lim} S_{n}(x) = 2\pi \ln (|x|)     \]
			si $|x| > 1$ et cette limite vaut zéro si $|x|<1$. 
			En conséquence, $f(x) = 0$ si $|x|<1$ et $f(x) = 4 \pi \ln (|x|)$, si $|x| > 1$. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour $n \in \NN^{*}$ on pose 
	\[     u_n =\sum_{k=1}^n\frac{1}{\sqrt{(k + n) (k + 1 + n)}}.     \]
	Déterminer la limite de $u_n$. 
	\begin{preuve}
		C'est clair que 
		\begin{equation}
		\label{eq:int3}
		\begin{split}
		\underset{L_{n}}{\underbrace{\sum_{k=1}^n\frac{1}{n+1} \frac{1}{1 + k/(n+1)}}} &= \sum_{k=1}^n\frac{1}{\sqrt{(k + n+1)^{2}}} 
		\leq \sum_{k=1}^n\frac{1}{\sqrt{(k + n) (k + 1 + n)}} 
		\\ &\leq \sum_{k=1}^n\frac{1}{\sqrt{(k + n)^{2}}} 
		= \underset{R_{n}}{\underbrace{\sum_{k=1}^n\frac{1}{n} \frac{1}{1 + k/n}}}.  
		\end{split}   
		\end{equation}
		On dénote $L_{n}$ et $R_{n}$ les sommes indiquées ci-dessus. 
		C'est clair que $R_{n}$ est une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = 1/(1+x)$ 
		associée à la subdivision $\{ x_{j} = j/n : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$. 
		En conséquence, 
		\[     \underset{n \rightarrow +\infty}{\lim} R_{n} = \int_{0}^{1} \frac{1}{1+x} dx = \bigg[ \ln(1+x) \bigg]_{0}^{1} = \ln(2).     \]
		Par ailleurs, soit 
		\[     \bar{L}_{n} = \sum_{k=0}^n\frac{1}{n+1} \frac{1}{1 + k/(n+1)} = \frac{1}{n+1} + L_{n}.     \]
		On voit bien que $\bar{L}_{n}$ est une somme de Riemann de l'application $f : [0,1] \rightarrow \RR$ donnée par $f(x) = 1/(1+x)$ 
		associée à la subdivision $\{ x_{j} = j/(n+1) : j \in \{0, \dots, n\} \}$ de l'intervalle $[0,1]$ et en particulier
		\[     \underset{n \rightarrow +\infty}{\lim} \bar{L}_{n} = \int_{0}^{1} \frac{1}{1+x} dx = \ln(2).     \]
		Cela implique que 
		\[     \underset{n \rightarrow +\infty}{\lim} L_{n} = \underset{n \rightarrow +\infty}{\lim} \bar{L}_{n} - \underset{n \rightarrow +\infty}{\lim} \frac{1}{n+1} = \underset{n \rightarrow +\infty}{\lim} \bar{L}_{n} = \ln(2).     \]
		L'inégalité \eqref{eq:int3} dit que la limite de $u_n$ quand $n$ tend vers $+ \infty$ est $\ln(2)$.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f:[0,1]\rightarrow \RR$ une fonction de classe $C^1$ telle que $f(0)=0$.
	\begin{enumerate}
		\item Montrer que 
		\[     \int_0^1f^2(x)dx\leq\frac{1}{2}\int_0^1f'^2(x)dx.     \]
		\\
		\textbf{Indication :} écrire $f(x) = \int_{0}^{x} f'(s) ds$, pour $x \in [0,1]$, et utiliser l'inégalité de Cauchy-Schwartz. 
		\item Améliorer l'inégalité précédente quand de plus $f(1)=0$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On écrit
			\[     |f(x)| = \bigg|\int_{0}^{x} f'(s) ds\bigg| \leq || 1 ||_{2} . ||f'||_{2} = \sqrt{x} \sqrt{\int_{0}^{x} |f'(s)|^{2} ds},     \]
			où l'on a utilisé l'inégalité de Cauchy-Schwarz. 
			En conséquence. 
			\[     \int_{0}^{1} |f(x)|^{2} dx \leq \int_{0}^{1} x \int_{0}^{x} |f'(s)|^{2} ds dx
			\leq \int_{0}^{1} x dx \int_{0}^{1} |f'(s)|^{2} ds = \frac{1}{2} \int_{0}^{1} |f'(s)|^{2} ds.     \]
			
			\item La première inégalité de l'item précédent nous dit que 
			\[     \int_{0}^{1/2} |f(x)|^{2} dx \leq \int_{0}^{1/2} x \int_{0}^{x} |f'(s)|^{2} ds dx
			\leq \int_{0}^{1/2} x dx \int_{0}^{1/2} |f'(s)|^{2} ds = \frac{1}{8} \int_{0}^{1/2} |f'(s)|^{2} ds.     \]
			Cette inégalité est aussi valable pour la fonction $g : [0,1] \rightarrow \RR$ définie par $g(x) = f(1-x)$, puisque $g(0) = f(1) = 0$. 
			En conséquence, 
			\[     \int_{0}^{1/2} |g(y)|^{2} dy \leq  \frac{1}{8} \int_{0}^{1/2} |g'(y)|^{2} dy.     \]
			En outre, la substitution $y = 1-x$ nous dit que 
			\[     \int_{0}^{1/2} |g(y)|^{2} dy =  \int_{1/2}^{1} |f(x)|^{2} dx \text{ et }  \int_{0}^{1/2} |g'(y)|^{2} dy = \int_{1/2}^{1} |f'(x)|^{2} dx.     \]
			En conséquence, 
			\[     \int_{0}^{1} |f(x)|^{2} dx \leq \frac{1}{8} \int_{0}^{1} |f'(x)|^{2} dx.     \]
		\end{enumerate}
	\end{preuve} 
	

\newpage \section{Intégrales impropres}

\noindent\textbf{Si $f:I \rightarrow \RR$ est une fonction continue par morceaux sur un intervalle $I$, on dira que $f$ est intégrable sur $I$ si l'intégrale généralisée $\int_I |f(x)|dx $ est convergente, \textit{i.e.} si $\int_I f(x)dx $ est absolument convergente.}


	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f:[a,b \hskip 0.6mm [ \hskip 0.6mm \rightarrow \RR$ une fonction continue par morceaux telle que 
	$\int_a^b |f(x)| dx$ converge. 
	Soit $( x_n )_{n\in \NN}$ une suite croissante d'éléments de $[a,b \hskip 0.6mm [ \hskip 0.6mm$ convergent vers $b$. 
	\begin{enumerate} 
		\item Montrer que la suite $I_n=\int_a^{x_n} f(x) dx$ est de Cauchy. 
		\item Montrer que cette limite ne dépend pas de la suite $( x_n )_{n\in \NN}$ choisie. 
		En déduire que l'intégrale $\int_a^b f(x) dx$ est convergente.
	\end{enumerate}
	
	\begin{preuve}
		Il s'agit des résultats du cours. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}
	\label{exo:2} 
	Déterminer la nature des intégrales généralisées suivantes:
	\\
	\begin{enumerate*}[label=(\alph*)]
		\item $\int_0^1 \ln (x) dx$, 
		\item $\int_0^1\frac{dx}{\sqrt{1-x^2}}$, 
		\item $\int_0^1\frac{dx}{e^x-1}$, 
		\item $\int_1^{+\infty}\frac{e^{1/x}}{x^2} dx$, 
		\item $\int_0^{+\infty} \frac{\sin(x)}{x} dx$, 
	\end{enumerate*}
	\\
	\begin{enumerate*}[label=(\alph*)]
		\setcounter{enumi}{5}
		\item $\int_0^1 \frac{\ln(1+x)}{x} dx$, 
		\item $\int_0^{\pi/2} \frac{\cos (x)-1}{\sin^2 (x)} dx$, 
		\item $\int_0^1\frac{1-x^2}{1-\sqrt{x}} dx$.
	\end{enumerate*}
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit bien que
			\[     \int_0^1 \ln (x) dx = \bigg[ x \ln(x) - x \bigg]_{0}^{1} = -1.     \]
			
			\item C'est clair que
			\[     \int_0^1 \frac{dx}{\sqrt{1-x^2}} dx = \bigg[ \arcsin(x) \bigg]_{0}^{1} = \frac{\pi}{2}.     \]
			
			\item Comme 
			\[     \int_0^1 \frac{dx}{e^x-1} dx = \underset{\epsilon \rightarrow 0}{\lim} \bigg[ \ln(|1 - e^{x}|) - x \bigg]_{\epsilon}^{1}     \]
			et la dernière limite diverge, l'intégrale est divergente. 
			
			\item On voit bien que
			\[     \int_1^{+\infty} \frac{e^{1/x}}{x^2} dx = \underset{M \rightarrow + \infty}{\lim} \bigg[ - e^{1/x} \bigg]_{1}^{M} = e -1.     \]
			
			\item C'est clair que
			\[     \int_{0}^{+\infty} \frac{\sin(x)}{x} dx = \sum_{n=0}^{+\infty} \int_{n \pi}^{(n+1)\pi} \frac{\sin(x)}{x} dx = \sum_{n=0}^{+\infty} (-1)^{n} \int_{n \pi}^{(n+1)\pi} \frac{|\sin(x)|}{x} dx.     \]
			Soit $a_{n} = \int_{n \pi}^{(n+1)\pi} |\sin(x)|/x dx \geq 0$, pour $n \in \NN$. 
			On voit bien que 
			\[     a_{n} = \int_{0}^{\pi} \frac{|\sin(x)|}{n\pi + x} dx,     \]
			pour tout $n \in \NN$.
			Cela implique que 
			\[     a_{n+1} = \int_{0}^{\pi} \frac{|\sin(x)|}{n\pi + x + \pi} dx \leq \int_{0}^{\pi} \frac{|\sin(x)|}{n\pi + x} dx = a_{n}.     \]
			Le critère de Leibniz nous dit alors que $\int_0^{+\infty} \sin(x)/x dx = \sum_{n=0}^{+\infty} (-1)^{n} a_{n}$ converge. 
			
			\item C'est clair que $\ln(1+x)/x$ est une fonction continue sur $\hskip 0.6mm ] \hskip 0.6mm 0,1]$. 
			En outre, la règle de Bernoulli-L'H\^opital dit que 
			\[     \underset{x \rightarrow 0}{\lim} \frac{\ln(1+x)}{x} = \underset{x \rightarrow 0}{\lim} \frac{1/(1+x)}{1} = 1,     \]
			ce qui implique que $\ln(1+x)/x$ se prolonge en une fonction continue sur $[ 0,1]$. 
			En conséquence, l'intégrale $\int_0^1 \ln(1+x)/x dx$ converge. 
			
			\item On voit bien que $(\cos (x)-1)/\sin^2 (x)$ est une fonction continue sur $\hskip 0.6mm ] \hskip 0.6mm 0,\pi/2]$. 
			Par ailleurs, d'après la règle de Bernoulli-L'H\^opital on voit que 
			\[     \underset{x \rightarrow 0}{\lim} \frac{\cos (x)-1}{\sin^2 (x)} = \underset{x \rightarrow 0}{\lim} \frac{-\sin(x)}{2 \sin(x) \cos(x)} = - \frac{1}{2},     \]
			ce qui implique que $(\cos (x)-1)/\sin^2 (x)$ se prolonge en une fonction continue sur $[ 0,\pi/2]$. 
			En conséquence, l'intégrale $\int_0^1 (\cos (x)-1)/\sin^2 (x) dx$ converge. 
			
			\item On voit bien que 
			\[     \int_0^1 \frac{1-x^2}{1-\sqrt{x}} dx = \int_0^1 (1+x)(1+\sqrt{x}) dx = \bigg[ \frac{2 x^{5/2}}{5} + \frac{2 x^{3/2}}{3} + \frac{x^{2}}{2} + x\bigg]_{0}^{1} = \frac{77}{30}.     \]
		\end{enumerate}
	\end{preuve}                       
	
	%%%%%%%%%%%%%%%
	\exo{} \begin{enumerate} 
		\item Déterminer la nature des intégrales généralisées :
		\\
		\begin{enumerate*}[label=(\roman*)]
			\item $\int_{0}^{+\infty} (x+2-\sqrt{x^{2}+4x+1}) dx$,
			\item $\int_{1}^{+\infty} \frac{\ln (x)}{x+e^{-x}} dx$, 
			\item $\int_{1}^{+\infty} \frac{1}{x^{\cos(1/x)}}dx$, 
		\end{enumerate*}
		\\
		\begin{enumerate*}[label=(\roman*)]
			\setcounter{enumi}{3}
			\item $\int_{1}^{+\infty} e^{-\sqrt{x^{2}-x}} dx$,  
			\item $\int_{2}^{+\infty} \frac{1}{(\ln (x))^{\ln (x)}}dx$,  
			\item $\int_{3}^{+\infty} \frac{dx}{(\ln (\ln (x)))^{\ln (x)}}$, 
		\end{enumerate*}
		\\
		\begin{enumerate*}[label=(\roman*)]
			\setcounter{enumi}{6}
			\item $\int_{0}^{+\infty } \frac{dx}{1+| \sin (x)| }$.
		\end{enumerate*}              
		
		\item Calculer, en montrant la convergence : 
		\\
		\begin{enumerate*}[label=(\roman*)]
			\item $\int_{0}^{+\infty} \frac{dx}{(x+1)(x+2)(x+3)}$, 
			\item $\int_{0}^{+\infty} \frac{dx}{x^{3}+1}$, 
			\item $\int_{0}^{+\infty} \frac{dx}{(x+1)^{2}(x+2)^{2}}$, 
		\end{enumerate*}
		\\
		\begin{enumerate*}[label=(\roman*)]
			\setcounter{enumi}{3}
			\item $\int_{0}^{1} \frac{x\ln (x)}{(1-x^{2})^{3/2}}dx$, 
			\item $\int_{0}^{+\infty} \frac{dx}{(1+x^{2})(1+x^{a})}$ ($a \in \RR$), 
			\item $\int_{1}^{+\infty} \frac{\ln (x)}{x^{n}}dx$ ($n \in \NN^{*}$).
		\end{enumerate*}
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item 
			\begin{enumerate}[label=(\roman*)]
				\item On voit bien que 
				\begin{align*}
				\int_{0}^{+\infty} &(x+2-\sqrt{x^{2}+4x+1}) dx = \int_{0}^{+\infty} \frac{3}{x+2+\sqrt{x^{2}+4x+1}} dx  
				\\
				&\geq \frac{3}{2} \int_{0}^{+\infty} \frac{1}{x+2} dx = \frac{3}{2} \underset{\epsilon \rightarrow 0}{\lim} \underset{M \rightarrow + \infty}{\lim} \bigg[ \ln(x+2) \bigg]_{\epsilon}^{M},     
				\end{align*}
				où l'on a utilisé que $\sqrt{x^{2}+4x+1} \leq \sqrt{x^{2}+4x+4} = x+2$, pour tout $x \geq 0$. 
				Comme la dernière limite diverge (vers $+ \infty$), on voit bien que l'intégrale demandée est divergente. 
				
				\item On voit bien que 
				\begin{align*}
				\int_{1}^{+\infty} \frac{\ln (x)}{x+e^{-x}} dx \geq \int_{1}^{+\infty} \frac{\ln (x)}{2 x} dx = \frac{1}{4}  \underset{M \rightarrow + \infty}{\lim} \bigg[ \ln(x)^{2} \bigg]_{1}^{M},     
				\end{align*}
				où l'on a utilisé que $e^{-x} \leq x$, pour tout $x \geq 1$. 
				Comme la dernière limite diverge (vers $+ \infty$), on voit bien que l'intégrale demandée est divergente. 
				
				\item On voit bien que 
				\begin{align*}
				\int_{1}^{+\infty} \frac{1}{x^{\cos(1/x)}} dx \geq \int_{1}^{+\infty} \frac{1}{x} dx = \underset{M \rightarrow + \infty}{\lim} \bigg[ \ln(x) \bigg]_{1}^{M},     
				\end{align*}
				où l'on a utilisé que $\cos(1/x) \leq 1$, pour tout $x \geq 1$, ce qui implique que $x^{\cos(1/x)} \leq x$, pour tout $x \geq 1$. 
				Comme la dernière limite diverge (vers $+ \infty$), on voit bien que l'intégrale demandée est divergente. 
				
				\item On voit bien que 
				\[     \int_{1}^{+\infty} e^{-\sqrt{x^{2}-x}} dx = \int_{1}^{2} e^{-\sqrt{x^{2}-x}} dx + \int_{2}^{+\infty} e^{-\sqrt{x^{2}-x}} dx.     \]
				Comme la première intégrale du membre de droite existe, vu que $e^{-\sqrt{x^{2}-x}}$ est une fonction continue définie sur un intervalle fermé et borné, l'intégrale demandée converge si et seulement si la deuxième intégrale du membre de droite converge. 
				Comme $e^{-\sqrt{x^{2}-x}}$ est une fonction positive, il suffit de démontrer que l'intégrale est bornée. 
				Dans ce cas 
				\begin{align*}
				\int_{2}^{+\infty} e^{-\sqrt{x^{2}-x}} dx \leq \int_{2}^{+\infty} e^{-x/\sqrt{2}} dx = \underset{M \rightarrow + \infty}{\lim} \bigg[ -\sqrt{2}e^{-x/\sqrt{2}} \bigg]_{2}^{M},     
				\end{align*}
				où l'on a utilisé que $x^{2} - x \geq x^{2}/2$, pour tout $x \geq 2$. 
				Comme la dernière limite converge (vers $0$ quand $M$ tend ver $+ \infty$), on voit bien que l'intégrale demandée est convergente. 
				
				\item Comme $1/(\ln (x))^{\ln (x)} \geq 0$, pour tout $x \in \RR_{\geq 2}$, l'intégrale demandée est convergente si et seulement si elle est bornée. 
				On voit bien que 
				\begin{align*}
				\int_{2}^{+\infty} &\frac{1}{(\ln (x))^{\ln (x)}} dx = \int_{\ln(2)}^{+\infty} \frac{e^{y}}{y^{y}} dy = 
				\int_{\ln(2)}^{e} \frac{e^{y}}{y^{y}} dy +  \int_{e}^{+\infty} \frac{e^{y}}{y^{y}} dy     
				\\
				&\leq \int_{\ln(2)}^{e} \frac{e^{y}}{y^{y}} dy + e^{2} \int_{e}^{+\infty} \frac{1}{y^{2}} dy 
				= \int_{\ln(2)}^{e} \frac{e^{y}}{y^{y}} dy - e^{2} \underset{M \rightarrow + \infty}{\lim} \bigg[ \frac{1}{y} \bigg]_{e}^{M},
				\end{align*}
				où l'on a utilisé la substitution $y = \ln(x)$ et que $(y/e)^{y} \geq (y/e)^{2}$, pour tout $y \geq e$. 
				Comme l'avant-dernière intégrale converge puisque $e^{y}/y^{y}$ est une fonction continue définie sur un intervalle fermé et borné, et la dernière limite converge (quand $M$ tend ver $+ \infty$), on voit bien que l'intégrale demandée est convergente. 
				
				\item On affirme que l'intégrale est convergente. 
				Comme $1/(\ln (\ln (x)))^{\ln (x)} \geq 0$, pour tout $x \in \RR_{\geq 3}$, l'intégrale demandée est convergente si et seulement si elle est bornée. 
				Or, on voit bien que 
				\begin{align*}
				\int_{3}^{+\infty} &\frac{dx}{(\ln (\ln (x)))^{\ln (x)}} dx = \int_{\ln(\ln(3))}^{+\infty} \frac{e^{e^{y}}e^{y}}{y^{e^{y}}} dy = 
				\int_{\ln(\ln(3))}^{e^{2}} \frac{e^{e^{y}}e^{y}}{y^{e^{y}}} dy +  \int_{e^{2}}^{+\infty} \frac{e^{e^{y}}e^{y}}{y^{e^{y}}} dy     
				\\
				&\leq \int_{\ln(\ln(3))}^{e^{2}} \frac{e^{e^{y}}e^{y}}{y^{e^{y}}} dy + \int_{e^{2}}^{+\infty} \frac{e^{y}}{(y/e)^{y}} dy 
				\end{align*}
				où l'on a utilisé la substitution $y = \ln(\ln(x))$, $e^{y} \geq y$ et donc $(y/e)^{e^{y}} \geq (y/e)^{y}$, pour tout $y \geq e$. 
				L'avant-dernière intégrale converge puisque $e^{e^{y}}e^{y}/y^{e^{y}}$ est une fonction continue définie sur un intervalle fermé et borné. 
				Cela implique qu'il suffit de démontrer que la dernière intégrale est bornée.
				C'est clair que 
				\[     \int_{e^{2}}^{+\infty} \frac{1}{(y/e^{2})^{y}} dy \leq \int_{e^{2}}^{+\infty} \frac{1}{(y/e^{2})^{2}},     \]
				où l'on a utilisé que $(y/e^{2})^{y} \geq (y/e)^{2}$, pour tout $y \geq e^{2} \geq 2$. 
				Comme la dernière intégrale converge, l'intégrale demandée aussi. 
				
				\item On voit bien que 
				\begin{align*}
				\int_{0}^{+\infty} \frac{dx}{1+| \sin (x)| } dx \geq \int_{1}^{+\infty} \frac{1}{2} dx = \underset{M \rightarrow + \infty}{\lim} \bigg[ \frac{x}{2} \bigg]_{0}^{M},     
				\end{align*}
				où l'on a utilisé que $1 + |\sin(x)| \leq 2$, pour tout $x \in \RR$. 
				Comme la dernière limite diverge (vers $+ \infty$), on voit bien que l'intégrale demandée est divergente. 
				
			\end{enumerate}
			
			\item 
			\begin{enumerate}[label=(\roman*)]
				\item On voit bien que 
				\begin{align*}
				\int_{0}^{+\infty} \frac{dx}{(x+1)(x+2)(x+3)} &= \underset{M \rightarrow + \infty}{\lim} \bigg[ \frac{\ln(x^{2}+4x+3)}{2} - \ln(x+2) \bigg]_{0}^{M} 
				\\
				&= \ln(2) - \frac{\ln(3)}{2}.     
				\end{align*}
				
				\item On voit bien que 
				\begin{align*}
				&\int_{0}^{+\infty} \frac{dx}{x^{3}+1} 
				\\
				&= \underset{M \rightarrow + \infty}{\lim} \bigg[ -\frac{\ln(x^{2}-x+1)}{6} + \frac{\ln(x+1)}{3} + \frac{\arctan\big((2x-1)/\sqrt{3}\big)}{\sqrt{3}} \bigg]_{0}^{M} 
				\\
				&= \frac{2 \pi}{3 \sqrt{3}}.     
				\end{align*}
				
				\item Une intégration par parties nous dit que  
				\begin{align*}
				\int \frac{x\ln (x)}{(1-x^{2})^{3/2}}dx &= \frac{\ln(x)}{(1-x^{2})^{1/2}} - \int \frac{1}{x(1-x^{2})^{1/2}}dx.     
				\end{align*}
				En outre, la substitution $y = \sqrt{1-x^{2}}$ dans la dernière intégrale nous donne
				\begin{align*}
				- \int \frac{1}{x(1-x^{2})^{1/2}}dx &= \int \frac{1}{1-y^{2}}dy = \frac{1}{2} \ln \bigg( \bigg| \frac{1+ y}{1-y} \bigg|\bigg) + C 
				\\
				&= \frac{1}{2} \ln \bigg( \bigg| \frac{1+ \sqrt{1-x^{2}}}{1- \sqrt{1-x^{2}}} \bigg|\bigg) + C.     
				\end{align*}
				En conséquence, 
				\[          \int \frac{x\ln (x)}{(1-x^{2})^{3/2}}dx = \frac{\ln(x)}{(1-x^{2})^{1/2}} +  \ln \bigg( \bigg| \frac{1+ \sqrt{1-x^{2}}}{1- \sqrt{1-x^{2}}} \bigg|\bigg) + C.     \]
				Un calcul long des limites nous dit que 
				\[          \int_{0}^{1} \frac{x\ln (x)}{(1-x^{2})^{3/2}}dx = - \ln(2).     \]
				
				\item C'est clair que 
				\[     I = \int_{0}^{+\infty} \frac{dx}{(1+x^{2})(1+x^{a})} \leq \int_{0}^{+\infty} \frac{dx}{1+x^{2}} = \frac{\pi}{2},     \]
				puisque $1+x^{a} \geq 1$ pour tout $x \geq 0$, ce qui implique que l'intégrale $I$ est convergente. 
				On fait la substitution $x = \tan(y)$, ce qui donne 
				\[     \int_{0}^{+\infty} \frac{dx}{(1+x^{2})(1+x^{a})} = \int_{0}^{\pi/2} \frac{dy}{1+\tan^{a}(y)}.     \]
				En outre, comme $\tan(\pi/2 - y) = 1/\tan(y)$, pour tout $y \in \hskip 0.6mm ] \hskip 0.6mm 0, \pi/2 \hskip 0.6mm [ \hskip 0.6mm$, on voit que 
				\begin{align*}    
				\int_{0}^{\pi/2} &\frac{dy}{1+\tan^{a}(y)} = \int_{0}^{\pi/2} \bigg(1 - \frac{\tan^{a}(y)}{1+\tan^{a}(y)}\bigg) dy 
				\\
				&= \int_{0}^{\pi/2} \bigg(1 - \frac{1}{1+1/\tan^{a}(y)}\bigg) dy 
				= \frac{\pi}{2} - \int_{0}^{\pi/2} \frac{1}{1+\tan^{a}(\pi/2-y)} dy 
				\\
				&= \frac{\pi}{2} - \int_{0}^{\pi/2} \frac{1}{1+\tan^{a}(z)} dz,
				\end{align*}
				où on a fait la substitution $z = \pi/2 - z$ dans la dernière intégrale. 
				Cela implique que $I = \pi/2 - I$ et en conséquence $I = \pi/4$. 
				
				\item C'est facile à vérifier que 
				\[     \int \frac{\ln (x)}{x^{n}}dx = \frac{1+(n-1)\ln (x)}{x^{n-1} (n-1)^{2}} + C     \]
				si $n \in \NN^{*} \setminus \{ 1 \}$ (à partir d'une intégration par parties avec $u= \ln(x)$ et $v' = 1/x^{n}$), et que 
				\[     \int \frac{\ln (x)}{x}dx = \frac{\ln (x)^{2}}{2} + C.     \]
				Cela implique que 
				\[     \int_{1}^{+\infty} \frac{\ln (x)}{x}dx     \]
				diverge et que 
				\[     \int_{1}^{+\infty} \frac{\ln (x)}{x^{n}}dx = \frac{1}{(n-1)^{2}},      \]
				si $n \in \NN^{*} \setminus \{ 1 \}$. 
			\end{enumerate}
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	
	\exo{} Montrer que l'intégrale généralisée 
	\[     \int_0^{+\infty} \frac{\ln(t)dt}{1+t^2}     \]
	converge. 
	Grâce à un changement de variables simple montrer qu'elle est nulle. 
	En déduire que pour tout $a>0$
	\[     \int_0^{+\infty} \frac{\ln(t)dt}{a^2+t^2}=\frac{\pi\ln(a)}{2a}.     \] 
	
	\begin{preuve}
		On voit que 
		\[     \int_0^{1} \frac{|\ln(t)|dt}{1+t^2} \leq \int_0^{1} |\ln(t)|dt = - \int_0^{1} \ln(t) dt = - \underset{\epsilon \rightarrow 0}{\lim} \bigg[t \big(\ln(t) - 1\big)\bigg]^{1}_{\epsilon} = 1.     \]
		En outre, comme la limite de $|\ln(t)|/\sqrt{t}$ quand $t$ tend vers $+\infty$ est zéro, il existe $C>1$ tel que 
		$|\ln(t)| < \sqrt{t}$, pour tout $t > C$. 
		Cela implique que 
		\[     \int_C^{+ \infty} \frac{|\ln(t)|dt}{1+t^2} \leq \int_C^{+ \infty} \frac{\sqrt{t}dt}{t^2} = \int_C^{+ \infty} \frac{dt}{t^{3/2}},     \]
		où l'on a utilisé que $1+t^{2} \geq t^{2}$. 
		Comme la dernière intégrale converge, on voit que $\int_C^{+ \infty} |\ln(t)|dt/(1+t^2)$ converge aussi. 
		Finalement, comme $t \mapsto |\ln(t)|dt/(1+t^2)$ est une fonction continue sur $\RR_{>0}$, l'intégrale $\int_1^{C} |\ln(t)|dt/(1+t^2)$ converge. 
		Cela implique que $\int_0^{+ \infty} |\ln(t)|dt/(1+t^2)$ converge.
		
		Par ailleurs, si l'on fait la substitution $x=1/t$, on trouve que 
		\[     \int_0^{1} \frac{\ln(t)dt}{1+t^2} = \int_{1}^{+\infty} \frac{\ln(1/x) dx}{1+x^2} = - \int_{1}^{+\infty} \frac{\ln(x) dx}{1+x^2}     \]
		ce qui implique que $\int_0^{+ \infty} \ln(t)dt/(1+t^2) = 0$. 
		
		Finalement, 
		\begin{align*}     
		\int_0^{+\infty} &\frac{\ln(t)dt}{a^2+t^2} = \frac{1}{a^{2}} \int_0^{+\infty} \frac{\ln(t)dt}{1+(t/a)^2} 
		= \frac{1}{a} \int_0^{+\infty} \frac{\ln(a x)dx}{1+x^2} 
		\\
		&= \frac{1}{a} \int_0^{+\infty} \frac{(\ln(a) + \ln(x))dx}{1+x^2} = 
		\frac{\ln(a)}{a} \int_0^{+\infty} \frac{dx}{1+x^2} = \frac{\pi \ln(a)}{2a},     
		\end{align*}
		où l'on a fait la substitution $x = t/a$.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour quelles valeurs de $(\alpha , \beta) \in \RR^{2}$, l'intégrale 
	\[     \int_{0}^{+\infty} \frac{dx}{x^{\alpha }+x^{\beta }}     \] 
	est-elle convergente ?
	
	\begin{preuve}
		On écrira $I(\gamma) = \int_{0}^{1} x^{-\gamma} dx$ et $J(\gamma) = \int_{1}^{+ \infty} x^{-\gamma} dx$, pour $\gamma \in \RR$. 
		On remarque que $I(\gamma)$ (resp., $J(\gamma)$) converge si et seulement si $\gamma < 1$ (resp., si $\gamma > 1$). 
		On définit
		\[     \mathcal{I}(\alpha,\beta) = \int_{0}^{+\infty} \frac{dx}{x^{\alpha }+x^{\beta }} = \underset{I(\alpha,\beta)}{\underbrace{\int_{0}^{1} \frac{dx}{x^{\alpha }+x^{\beta }}}} + \underset{J(\alpha,\beta)}{\underbrace{\int_{1}^{+\infty} \frac{dx}{x^{\alpha }+x^{\beta }}}}.     \] 
		On affirme que $\mathcal{I}(\alpha,\beta)$ converge si et seulement si $\alpha > 1$ et $\beta < 1$, ou $\alpha < 1$ et $\beta > 1$.
		
		Désormais, on suppose sans perte de généralité que $\alpha \geq \beta$. 
		C'est clair que $1/(x^{\alpha}+x^{\beta}) \leq 1/x^{\alpha}$ et $1/(x^{\alpha}+x^{\beta}) \leq 1/x^{\beta}$, pour tout $x > 0$, ce qui implique que $I(\alpha,\beta) \leq I(\beta)$ et $J(\alpha,\beta) \leq J(\alpha)$. 
		Alors, si $\alpha > 1$ et $\beta < 1$, comme $\mathcal{I}(\alpha,\beta) \leq I(\beta) + J(\alpha)$, on voit que 
		$\mathcal{I}(\alpha,\beta)$ converge. 
		
		Soit $\beta \geq 1$. 
		Comme $1/(x^{\alpha}+x^{\beta}) \geq 1/(2 x^{\beta})$, pour tout $x \leq 1$, on conclut que 
		$\mathcal{I}(\alpha,\beta) \geq I(\alpha ,  \beta) \geq I(\beta)/2$ diverge, puisque $I(\beta)$ diverge (vers $+ \infty$). 
		De même, si $\alpha \leq 1$, on note que $1/(x^{\alpha}+x^{\beta}) \geq 1/(2 x^{\alpha})$, pour tout $x \geq 1$, 
		ce qui implique que $\mathcal{I}(\alpha,\beta) \geq J(\alpha ,  \beta) \geq J(\alpha)/2$ diverge, puisque $J(\alpha)$ diverge (vers $+ \infty$). 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR_{\geq 0} \rightarrow \RR$ une fonction continue par morceaux et intégrable.
	Montrer que la série de terme général $\int_n^{n+1} f(x) dx$ converge et que sa
	somme est $\int_0^{+\infty} f(x) dx$.
	
	\begin{preuve}
		C'est immédiat de la définition. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : [a,b \hskip 0.6mm [ \hskip 0.6mm \rightarrow \RR$ une fonction continue par morceaux et intégrable.
	Montrer que 
	\[     \underset{x \rightarrow b}{\lim} \int_x^b f(t) dt =0.     \]
	
	\begin{preuve}
		C'est immédiat de la définition. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soient $a$ et $b$ des réels tels que $a<b$. 
	Montrer que la fonction définie sur
	$\hskip 0.6mm ] \hskip 0.6mm a , b \hskip 0.6mm [ \hskip 0.6mm$ par 
	\[     f(x) = \frac{1 }{ \sqrt{(x-a)(b-x)}}     \] 
	est intégrable sur $\hskip 0.6mm ] \hskip 0.6mm a , b \hskip 0.6mm [ \hskip 0.6mm$
	et calculer son intégrale. 
	\\ \textbf{Indication : } on fera le changement de variable $t= (x-a)/(b-x)$.
	
	\begin{preuve}
		La substitution indiquée nous donne 
		\[     \int_{a}^{b} \frac{dx}{ \sqrt{(x-a)(b-x)}} = \int_{0}^{+ \infty} \frac{dt}{\sqrt{t} (1+t)} = \underset{M \rightarrow + \infty}{\lim} \bigg[ 2 \arctan(\sqrt{t}) \bigg]_{0}^{M} = \pi.     \]
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}
	Soit $f : \RR \rightarrow \RR$ une fonction continue par morceaux et intégrable. 
	Montrer que pour tout $a \in \RR \setminus \{ 0 \}$ et tout $b\in \RR$, la fonction $x \mapsto f(ax+b)$
	est intégrable sur $\RR$ et que
	\[     \int_{- \infty}^{+ \infty} f(t)dt = |a| \int_{-\infty}^{+\infty} f(at+b)dt.     \]
	
	\begin{preuve}
		On suppose d'abord que $a > 0$. 
		Soient $A, B > 0$. On voit bien que 
		\[     \int_{-A}^{B} f(at+b)dt = \frac{1}{a} \int_{- a A+b}^{a B + b} f(x) dx.     \]
		où l'on a fait la substitution $x = at+b$. 
		Alors, 
		\begin{align*}
		a \int_{-\infty}^{+\infty} f(at+b)dt &= \underset{A,B \rightarrow + \infty}{\lim} a \int_{-A}^{B} f(at+b)dt = \underset{A,B \rightarrow + \infty}{\lim} \int_{- a A+b}^{a B + b} f(x) dx 
		\\
		&= \int_{-\infty}^{+\infty} f(x)dx,    
		\end{align*} 
		comme on voulait démontrer. 
		
		Si $a < 0$, on regarde $x \mapsto ax+b$ comme la composition des applications $x \mapsto -ax-b$ et $y \mapsto -y$. 
		Il suffit donc de démontrer que la fonction $x \mapsto f(-x)$ est intégrable sur $\RR$ et que
		\[     \int_{- \infty}^{+ \infty} f(t)dt = \int_{-\infty}^{+\infty} f(-t)dt.     \]
		Cela suit du fait que 
		\[     \int_{- \infty}^{+ \infty} f(t)dt = \underset{A,B \rightarrow + \infty}{\lim} \int_{-A}^{B} f(t) dt = 
		\underset{A,B \rightarrow + \infty}{\lim} \int_{-B}^{A} f(-x) dx = \int_{-\infty}^{+\infty} f(x)dx,     \]
		où l'on a fait la substitution $x = -t$. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Théorème de convergence monotone} 
	Soit $( f_n )_{n\in \NN}$ une suites de fonctions de $I$ dans $\RR$ telle que
	\begin{enumerate}[label=(\roman*)]
		\item $f_n$ est intégrable sur $I$,
		\item $(f_n)_{n\in \NN}$ est croissante (\textit{i.e.}, pour tout $x\in I$, $(f_n(x))_{n\in \NN}$ est croissante),
		\item $(f_n)_{n\in \NN}$ converge simplement vers $f$.
	\end{enumerate}
	Montrer que $f$ est intégrable sur $I$ si et seulement si $(\int_I f_n(x) dx )_{n\in \NN}$ converge. 
	De plus, montrer que dans ce cas on a les égalités suivantes
	\[     \int_I f(x) dx = \sup_{n\in \NN} \int_I f_n(x) dx = \underset{n \rightarrow +\infty}{\lim} \int_I f_n(x) dx.     \]
	
	\begin{preuve}
		Il s'agit d'un résultat du cours.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Théorème de convergence dominée} 
	Soit $(f_n)_{n\in \NN}$ une suite de fonctions continues par morceaux de $I$ dans $\RR$ telle que
	\begin{enumerate}[label=(\roman*)]
		\item $f_n$ est intégrable sur $I$,
		\item il existe $\varphi : I \rightarrow \RR_{\geq 0}$ intégrable sur $I$ telle que $|f_n(x)|\leq \varphi(x)$ pour tout $n\in \NN$ et $x \in I$, 
		\item $(f_n)_{n \in \NN}$ converge simplement vers $f$.
	\end{enumerate}
	Montrer que $f$ est intégrable sur $I$ et que
	\[     \int_I f(x) dx = \underset{n \rightarrow +\infty}{\lim} \int_I f_n(x) dx.     \]
	
	\begin{preuve}
		Il s'agit d'un résultat du cours.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Montrer que 
	\[     \int_0^{+\infty} \frac{t}{e^t-1} dt     \] 
	converge et vaut $\sum_{n=1}^{+\infty} 1/n^2$.
	
	\begin{preuve}
		Comme la limite de $t/(e^t-1)$ en $0$ est $1$, la fonction $f(t) = t/(e^t-1)$ définie sur $\RR_{>0}$ se prolonge en une fonction continue $\bar{f} : \RR_{\geq 0} \rightarrow \RR$. 
		En outre, comme 
		\[     \underset{t \rightarrow +\infty}{\lim} \frac{f(t)}{t^{-2}} = 0     \]
		et l'intégrale $\int_{1}^{+\infty} t^{-2} dt$ converge, on conclut que $t/(e^t-1)$ est intégrable sur $\RR_{\geq 0}$. 
		
		En outre, on réécrit
		\[     \frac{t}{e^t-1} = \frac{e^{-t} t}{1-e^{-t}} = \sum_{\ell \in \NN_{0}} t e^{-(\ell+1) t},     \]
		pour tout $t > 0$. 
		En fait, comme les séries partielles du dernier membre sont majorées par la fonction intégrable et positive $\bar{f}(t)$ (sur $\mathbb{R}_{\geq 0}$), le théorème de convergence majorée dit que l'intégrale de $t/(e^t-1)$ sur $\RR_{>0}$ coïncide avec la série convergente
		\[     \sum_{\ell \in \NN_{0}} \int_{0}^{+\infty} t e^{-(\ell+1) t} dt.     \]
		En plus, une intégration par parties nous dit que la primitive de $t e^{-c t}$ (avec $c>0$) est donnée par
		\[     - \frac{(1+ct) e^{-c t}}{c^{2}}.     \]
		En conséquence, un calcul immédiat nous dit que le $\ell$-ième opérande de la série précédente est $1/(\ell+1)^{2}$.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	\begin{enumerate} 
		\item Montrer que l'intégrale 
		\begin{equation}
		\label{eq:int}
		\int_{0}^1 \frac{ \ln(1+ x t^2)}{t^2} dt
		\end{equation}          
		converge pour tout $x \in \RR_{>-1}$. 
		
		\item On définit la fonction $F : \RR_{>-1} \rightarrow \RR$ par 
		\eqref{eq:int}. 
		La fonction $F$ est-elle continue en $0$? 
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate} 
			\item Si $x \geq 0$, $0 \leq \ln(1+ x t^2) \leq x t^{2}$, pour tout $t \in [0,1]$. 
			Cela implique que 
			\begin{equation}
			\label{eq:int1}
			\begin{split}
			\int_{0}^1 \frac{ \ln(1+ x t^2)}{t^2} dt \leq \int_{0}^1 x dt = x.     
			\end{split}
			\end{equation}
			Si $-1< x < 0$, on utilise que $|\ln(1+xt^{2})| \leq |x|t^{2}/(1+x t^{2})$, pour tout $t \in [0,1]$, et, en conséquence, 
			\begin{equation}
			\label{eq:int2}
			\begin{split}
			\int_{0}^1 &\frac{ \ln(1+ x t^2)}{t^2} dt \leq \int_{0}^1 \frac{|x|}{1-|x| t^{2}} dt = \frac{\sqrt{|x|}}{2} \bigg[ \ln\bigg(\bigg| \frac{1+\sqrt{|x|} t}{1-\sqrt{|x|} t}\bigg| \bigg) \bigg]_{0}^{1} 
			\\ 
			&=  \frac{\sqrt{|x|}}{2} \ln\bigg(\bigg| \frac{1+\sqrt{|x|}}{1-\sqrt{|x|}}\bigg| \bigg).     
			\end{split}
			\end{equation}
			Cela implique que \eqref{eq:int} converge absolument pour tout $x \in \RR_{>-1}$. 
			
			\item On note d'abord que $F(0) = 0$. 
			En outre, \eqref{eq:int1} nous dit que, si $x>0$, $0 \leq F(x) \leq x$, ce qui implique que 
			\[      \underset{x \rightarrow 0+}{\lim} F(x) = 0 = F(0).     \]
			Par ailleurs, d'après \eqref{eq:int2}, on voit que 
			\[     |F(x)| \leq \frac{\sqrt{|x|}}{2} \ln\bigg(\bigg| \frac{1+\sqrt{|x|}}{1-\sqrt{|x|}}\bigg| \bigg),     \]
			si $-1 < x < 0$. 
			En conséquence, 
			\[      \underset{x \rightarrow 0-}{\lim} F(x) = 0 = F(0).     \]
			On conclut que $F$ est continue en $0$. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	\label{exo:14}
	Trouver une suite de fonctions $(f_n)_{n \in \NN} \in C^{0}([0,1],\RR_{\geq 0})^{\NN}$
	qui converge simplement vers la fonction nulle sans que la suite $(\int_0^1 f_n(x) dx)_{n \in \NN}$ soit
	majorée.
	
	\begin{preuve}
		Pour $n \in \NN$, on définit $f_{n} : [0,1] \rightarrow \RR$ par 
		\[     f_{n}(x) = \begin{cases} 
		2^{3(n+1)}x, &\text{si $x \in [0,1/2^{n+1}]$},
		\\
		-2^{3(n+1)}(x-1/2^{n}), &\text{si $x \in [1/2^{n+1},1/2^{n}]$},
		\\
		0, &\text{si $x \in [1/2^{n},1]$}.
		\end{cases}
		\]
		C'est clair que $f_{n}$ est continue et $\int_0^1 f_n(x) dx = 2^{n+1}$, pour tout $n \in \NN$, et $(f_n)_{n \in \NN}$
		converge simplement vers la fonction nulle. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Trouver une série de fonctions continues, intégrables et
	d'intégrales nulles sur $\RR_{\geq 0}$, convergeant simplement vers la fonction sinus sur
	$\RR_{\geq 0}$.
	
	\begin{preuve}
		Pour $n \in \NN$, on définit $f_{n} : \RR_{\geq 0} \rightarrow \RR$ par 
		\[     f_{n}(x) = \begin{cases} 
		\sin(x), &\text{si $x \in [0,2n \pi]$},
		\\
		0, &\text{si $x \in \RR_{\geq 2 n \pi}$}.
		\end{cases}
		\]
		C'est clair que $f_{n}$ est continue, intégrable et $\int_0^{+ \infty} f_n(x) dx = 0$, pour tout $n \in \NN$, et $(f_n)_{n \in \NN}$
		converge simplement vers la fonction sinus sur $\RR_{\geq 0}$. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Montrer que la fonction $f : \RR_{\geq 0} \rightarrow \RR$ définie par 
	\[     f(x) = \sum_{n=1}^{+\infty} e^{-nx}\frac{\sin (nx) }{ x^2+n^2}     \] 
	est continue et intégrable (sur $\RR_{\geq 0}$).
	
	\begin{preuve}
		Pour $m \in \NN$, soit $f_{m} : \RR_{\geq 0} \rightarrow \RR$ définie par 
		\[     f_{m}(x) = \sum_{n=1}^{m} e^{-nx}\frac{\sin (nx) }{ x^2+n^2}.     \]
		Si $m = 0$, $f_{m}$ est la fonction nulle. 
		C'est clair que $f_{m}$ est continue et intégrable (sur $\RR_{\geq 0}$), pour tout $m \in \NN^{*}$, 
		puisqu'il s'agit d'un somme finie de fonctions continues et intégrables. 
		En outre, pour $m, m' \in \NN$ tels que $m > m'$ on voit bien que 
		\[     |f_{m}(x) - f_{m'}(x)| \leq \sum_{n=m'+1}^{m} e^{-nx}\frac{|\sin (nx)|}{ x^2+n^2} \leq \sum_{n=m'+1}^{m} e^{-x}\frac{1}{n^2} \leq e^{-x} \sum_{n=m'+1}^{+\infty} \frac{1}{n^2} \leq \sum_{n=m'+1}^{+\infty} \frac{1}{n^2},     \]
		où l'on a utilisé que $n^{2} \leq n^{2} + x^{2}$ et $e^{-nx} \leq e^{-x} \leq 1$, pour tout $n \in \NN^{*}$ et $x \in \RR_{\geq 0}$. 
		Cela nous dit que la suite $\{ f_{m} \}_{m \in \NN}$ est de Cauchy dans l'espace vectoriel $\mathcal{B}(\RR_{\geq 0},\RR)$ de fonctions bornées muni de la norme infini, ce qui implique que $\{ f_{m} \}_{m \in \NN}$ converge uniformément vers $f$ sur $\RR$. 
		En conséquence, $f$ est continue. 
		
		Par ailleurs, comme $\sum_{n=1}^{+\infty} 1/n^2$ converge vers une valeur $C >0$ (en fait, $C = \pi^{2}/6$), $|f_{m}(x)| \leq C e^{-x}$, 
		pour tout $m \in \NN^{*}$ et $x \in \RR_{\geq 0}$. 
		On remarque que $e^{-x}$ est intégrable sur $\RR_{\geq 0}$. 
		Le théorème de convergence dominée nous dit alors que $f$ est intégrable sur $\RR_{\geq 0}$. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Montrer que la fonction $f : \RR_{\geq 0} \rightarrow \RR$ définie par 
	\[      f(x) = \sum_{n=1}^{+\infty} \frac{(-1)^{n-1} }{ x^2+n^2}     \] 
	est continue et intégrable (sur $\RR_{\geq 0}$) et que 
	\[     \int_0^{+\infty} f(x) dx = \sum_{n=1}^{+\infty} (-1)^{n-1} \int_0^{+\infty} \frac{dx }{ x^2+n^2}.     \]
	
	\begin{preuve}
		Pour $m \in \NN^{*}$, soit $f_{m} : \RR_{\geq 0} \rightarrow \RR$ définie par 
		\[     f_{m}(x) = \sum_{n=1}^{m} \frac{(-1)^{n-1} }{x^2+n^2}.     \]
		C'est clair que $f_{m}$ est continue et intégrable (sur $\RR_{\geq 0}$), pour tout $m \in \NN^{*}$, 
		puisqu'il s'agit d'un somme finie de fonctions continues et intégrables. 
		En outre, le critère de Leibniz nous dit que la suite $(f_{m})_{m \in \NN}$ converge simplement vers $f$. 
		L'inégalité $x^{2}+n^{2} \leq x^{2}+(n+1)^{2}$, pour tout $n \in \NN^{*}$ et $x \in \RR_{\geq 0}$, nous dit que 
		$f_{2m}(x) \geq 0$ pour tout $m \in \NN^{*}$ et $x \in \RR_{\geq 0}$. 
		Cela implique aussi que $f_{2m+1}(x) = f_{2m}(x) + 1/(x^{2} + (2m+1)^{2}) \geq 0$. 
		En plus, on voit bien que 
		\[     f_{2m}(x) \leq f_{2m+1}(x) = \frac{1}{x^2+1} + \sum_{n=1}^{m} \bigg(\frac{1}{x^2+(2n+1)^2} - \frac{1}{x^2+(2n)^2}\bigg) \leq \frac{1}{x^2+1}.     \]
		En conséquence, $0 \leq f_{m}(x) \leq f_{1}(x) = 1/(1+x^{2})$. 
		Le théorème de convergence dominée nous dit alors que $f$ est intégrable sur $\RR_{\geq 0}$. 
		En outre, le théorème de Dini nous dit que $(f_{m})_{m \in \NN}$ converge uniformément vers $f$ sur tout intervalle fini, ce qui implique 
		que $f$ est continue. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR_{\geq 0} \rightarrow \RR$ une fonction continue et intégrable. 
	\begin{enumerate}
		\item Montrer que 
		\[     \underset{n \rightarrow \infty}{\lim} \int_0^{+\infty} e^{-nt} f(t) dt = 0.     \]
		\item Montrer que 
		\[     \underset{n \rightarrow \infty}{\lim} n \int_0^{+\infty}e^{-nt} f(t) dt = f(0).     \]
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item
			Pour $n \in \NN^{*}$, soit $f_{n} : \RR_{\geq 0} \rightarrow \RR$ définie par $f_{n}(t) = e^{-nt} f(t)$. 
			Alors, c'est clair que $(f_{n}(t))_{n \in \NN^{*}}$ converge vers $0$ pour tout $t > 0$. 
			En outre, $|f_{n}(t)| = |f(t)| e^{-nt} \leq |f(t)|$, pour tout $t > 0$.
			Le théorème de convergence dominée nous dit alors que 
			\[     \underset{n \rightarrow \infty}{\lim} \int_0^{+\infty} e^{-nt} f(t) dt = \int_0^{+\infty} 0 dt = 0.     \]
			
			\item 
			On note d'abord que l'égalité demandée est équivalente à 
			\[     \underset{n \rightarrow \infty}{\lim} \int_0^{+\infty} n e^{-nt} \big(f(t) - f(0)\big) dt = 0.     \]
			Soit $\epsilon > 0$. 
			Comme $f$ est continue en $0$, il existe $\delta > 0$ tel que $|f(t) - f(0)| < \epsilon/2$ si $x \in [0,\delta]$. 
			On fixe désormais un possible $\delta$ comme ci-dessus.
			En particulier, 
			\begin{align*}
			\bigg| \int_0^{\delta} n e^{-nt} \big(f(t) - f(0)\big) dt \bigg| &\leq \int_0^{\delta} n e^{-nt} \big|f(t) - f(0)\big| dt \leq \frac{\epsilon}{2} \int_0^{\delta} n e^{-nt} dt \\
			&= \frac{\epsilon}{2} (1 - e^{-n \delta}) \leq  \frac{\epsilon}{2}.  
			\end{align*} 
			Comme $e^{x} \geq 1+x$, pour tout $x \geq 0$, on en déduit que $e^{n t /2} \geq nt/2$, \textit{i.e.} $ne^{-n t /2} \leq 2/t$, pour tout $t >0$ et $n \in \NN^{*}$. 
			Cela implique que $n e^{-nt} \leq 2 e^{- n t /2}/t$, pour tout $t >0$ et $n \in \NN^{*}$. 
			En conséquence, 
			\[     \int_{\delta}^{+\infty} n e^{-nt} |f(t)| dt \leq \frac{2}{{\delta}} \int_{\delta}^{+\infty} e^{-nt/2} |f(t)| dt,     \]
			qui converge vers zéro d'après le premier item, \textit{i.e.} il existe $n_{0}=n_{0}(\epsilon,\delta) \in \NN^{*}$ tel que la dernière intégrale est majorée par $\epsilon \delta/8$, si $n \geq n_{0}$. 
			En outre, 
			\[     \int_{\delta}^{+\infty} n e^{-nt} |f(0)| dt = |f(0)| e^{-n\delta},     \]
			qui tend vers $0$ quand $n$ tend vers $+ \infty$, \textit{i.e.} il existe $n_{1}=n_{1}(\epsilon,\delta) \in \NN^{*}$ tel que $|f(0)| e^{-n\delta}$ est majorée par $\epsilon/4$, si $n \geq n_{1}$. 
			Finalement, si $n_{2} = \operatorname{max}(n_{0},n_{1})$,  
			\begin{align*}
			\bigg| \int_0^{+ \infty} &n e^{-nt} \big(f(t) - f(0)\big) dt \bigg| 
			\\
			&= \bigg| \int_0^{\delta} n e^{-nt} \big(f(t) - f(0)\big) dt \bigg| + 
			\bigg| \int_{\delta}^{+ \infty} n e^{-nt} \big(f(t) - f(0)\big) dt \bigg| 
			\\
			&\leq \bigg| \int_0^{\delta} n e^{-nt} \big(f(t) - f(0)\big) dt \bigg|  + \int_{\delta}^{+ \infty} n e^{-nt} \big(|f(t)| + |f(0)|\big) dt
			\\
			&\leq \frac{\epsilon}{2} + \frac{\epsilon}{4} + \frac{\epsilon}{4} = \epsilon,
			\end{align*} 
			si $n \geq n_{2}$. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Posons 
	\[     f_n(x)= \left( 1+\frac{x^2 }{ n} \right)^{-n},     \]
	avec $n \in \NN^{*}$.  
	\begin{enumerate}
		\item Montrer que
		\[     \int_{-\infty}^{+\infty} e^{-x^2} dx = \underset{n \rightarrow \infty}{\lim} \int_{-\infty}^{+\infty} \left( 1+\frac{x^2 }{ n} \right)^{-n} dx     \]
		\item Montrer que 
		\[     \int_{-\infty}^{+\infty}\left( 1+\frac{x^2 }{ n} \right)^{-n} dx = 2 \sqrt{n} \int_0^\frac{\pi }{ 2} \cos^{2n-2} (y) dy     \] 
		et en déduire que
		\[     \int_{-\infty}^{+\infty} e^{-x^2} dt= \sqrt{\pi}.     \]
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item 
			Comme 
			\[     \underset{n \rightarrow \infty}{\lim} \left( 1+\frac{y}{ n} \right)^{n} = e^{y},     \]
			pour tout $y \in \RR$, on conclut que $f_{n}$ converge simplement vers $e^{-x^{2}}$. 
			Par ailleurs, c'est clair que 
			\[     |f_n(x)| = \frac{1}{\big(1+n^{-1} x^2\big)^{n}} = \frac{1}{1+ x^2 + \sum_{k=2}^{n} \begin{pmatrix}n \\ k \end{pmatrix} x^{2k} n^{-k}} \leq  \frac{1}{1+ x^2},     \] 
			pour tout $x \in \RR$ et $n \in \NN^{*}$. 
			Comme $1/(1+ x^2)$ est intégrable sur $\RR$, le théorème de convergence dominée nous dit alors que $e^{-x^{2}}$ est intégrable sur $\RR$ et 
			\[     \int_{-\infty}^{+\infty} e^{-x^2} dx = \underset{n \rightarrow \infty}{\lim} \int_{-\infty}^{+\infty} \left( 1+\frac{x^2 }{ n} \right)^{-n} dx.     \]
			
			\item La première égalité de cet item se trouve directement de faire la substitution $x = \sqrt{n} \tan(y)$ et utiliser que la fonction $\cos^{2n-2}(y)$ est paire. 
			
			Une intégration par parties nous dit que 
			\[     \int \cos^{m}(x) dx = - \frac{1}{m} \cos^{m-1}(x) \sin (x) + \frac{m-1}{m} \int \cos^{m-2}(x) dx,     \]
			pour tout entier $m \geq 2$. 
			En conséquence, 
			\[     \int_{0}^{\pi/2} \cos^{2n-2}(x) dx = \frac{2n-3}{2n-2} \int_{0}^{\pi/2} \cos^{2n-4}(x) dx,     \]
			pour tout entier $n \geq 2$, \textit{i.e.} 
			\[     \int_{0}^{\pi/2} \cos^{2n-2}(x) dx = \frac{2n-3}{2n-2} \dots \frac{1}{2} \frac{\pi}{2} 
			= \frac{\pi}{2} \prod_{i=1}^{n-1} \frac{2i-1}{2 i}.     \]
			Le produit de Wallis (voir le dernier item de l'exercice \textbf{10} de la fiche 1) nous dit que 
			\[     \underset{n \rightarrow \infty}{\lim} 2 \sqrt{n} \frac{\pi}{2} \prod_{i=1}^{n-1} \frac{2i-1}{2 i} = \sqrt{\pi}.     \]
		\end{enumerate}
	\end{preuve}
	
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR \rightarrow \RR$ une fonction continue telle que $x \mapsto xf(x)$ soit intégrable sur $\RR$. 
	Montrer que la fonction $ F : \RR \rightarrow \RR$ donnée par 
	$F(x) = \int_{- \infty}^{+\infty} e^{-ixt}f(t) dt$ est définie sur $\RR$ et elle est de classe $C^1$. 
	Calculer sa fonction dérivée.
	
	\begin{preuve}
		On voit que la fonction $g : \RR^{2} \rightarrow \C$ donnée par $(x,t) \mapsto e^{-ixt}f(t)$ est dérivable par rapport à $x$. 
		Sa dérivée partielle selon $x$ est $(x,t) \mapsto -it f(t) e^{-ixt}$ est une fonction continue, qui satisfait que
		\[     \bigg| \frac{\partial g}{\partial x} (x,t) \bigg| = | -it f(t) e^{-ixt} | = |t f(t)|.     \]
		En outre, $|g(x,t)| = |f(t)|$. 
		Le théorème de différentiation sous le signe de l'intégral implique que 
		$F$ est de classe $C^{1}$ et sa dérivée est 
		\[     F'(x) = \int_{- \infty}^{+\infty} \frac{\partial g}{\partial x} (x,t) dt =\int_{- \infty}^{+\infty} -it f(t) e^{-ixt} dt.     \]
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Étudier l'intégrabilité des fonctions suivantes:
	\begin{enumerate}
		\item $x \mapsto 1/\sqrt[3]{x-1}$ sur $[0,1 \hskip 0.6mm [ \hskip 0.6mm$, 
		\item $x \mapsto \cos (x)/\sqrt{1-\sin (x)}$ sur $\hskip 0.6mm ]0,\pi/ 2 \hskip 0.6mm[\hskip 0.6mm$, 
		\item $x \mapsto 1/(x^2+x+1)$ sur $\RR_{\geq 0}$,
		\item $x \mapsto (x^2+x) e^x  $ sur $\RR_{\leq 0}$,
		\item $x \mapsto 1/\ln (x)$ sur $\hskip 0.6mm]\hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$,
		\item $x \mapsto  1/\sqrt{1-x^2}$ sur $\hskip 0.6mm]\hskip 0.6mm -1,1\hskip 0.6mm [ \hskip 0.6mm$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item C'est clair que 
			\[     \int_{0}^{1} \frac{1}{\sqrt[3]{x-1}} dx = \frac{3}{2} \bigg[ \sqrt[3]{(x-1)^{2}} \bigg]_{0}^{1} = - \frac{3}{2}.     \]
			
			\item On voit bien que 
			\[     \int_{0}^{\pi/2} \frac{\cos (x)}{\sqrt{1-\sin(x)}} dx = -2 \bigg[ \sqrt{1-\sin(x)} \bigg]_{0}^{\pi/2} = 2.     \]
			
			\item C'est clair que 
			\[     \int_{0}^{+ \infty} \frac{1}{x^2+x+1} dx = \frac{2}{\sqrt{3}} \bigg[ \arctan\bigg( \frac{2x+1}{\sqrt{3}} \bigg) \bigg]_{0}^{+ \infty} = 
			\frac{\pi}{\sqrt{3}} - \frac{2}{\sqrt{3}} \arctan\bigg( \frac{1}{\sqrt{3}} \bigg).     \]
			
			\item On voit bien que 
			\[     \int^{0}_{- \infty} (x^2+x) e^x dx = \bigg[ (x^2-x+1) e^x \bigg) \bigg]^{0}_{- \infty} = 
			1.     \]
			
			\item C'est clair que 
			\[     \int_{0}^{1} \frac{1}{|\ln (x)|} dx = \int^{+\infty}_{0} \frac{e^{-y}}{y} dy,     \]
			où l'on a fait la substitution $y = - \ln(x)$. 
			Comme 
			\[     \int_{0}^{1} \frac{e^{-y}}{y} dy \geq \int_{0}^{1} \frac{e^{-1}}{y} dy = e^{-1}\underset{\epsilon \rightarrow 0}{\lim} [ \ln(y) ]_{\epsilon}^{1}      \]
			diverge (vers $+ \infty$), l'intégrale demandée est aussi divergente. 
			
			\item On voit bien que 
			\[     \int_{-1}^{1} \frac{1}{\sqrt{1-x^2}} dx = \bigg[ \arcsin(x) \bigg) \bigg]^{1}_{- 1} = \pi.     \]
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour $\alpha \in \RR$, on définit la fonction $f_{\alpha} : \RR_{> 0} \rightarrow \RR$ via 
	\[     f_{\alpha}(x) = \frac{\ln (1+x)}{x^{\alpha}}.     \]
	Déterminer les valeurs de $\alpha$ telles que $f_{\alpha}$ soit intégrable (sur $\RR_{> 0}$). 
	
	\begin{preuve}
		On remarque d'abord que, comme $f_{\alpha}(x) \geq 0$, pour tout $ x \in \RR_{> 0}$, $f_{\alpha}$ est intégrable (sur $\RR_{> 0}$) si et seulement si l'intégrale (sur $\RR_{> 0}$) est bornée. 
		%D'abord, c'est clair que si $\alpha \leq 0$, alors $f_{\alpha}$ n'est pas intégrable (sur $\RR_{> 0}$). 
		Si $\beta >0$, $\ln (1+x)/x^{\beta}$ tend vers zéro quand $x$ tend vers $+ \infty$. 
		Cela implique qu'il existe $C > 0$ tel que $\ln (1+x) \leq x^{\beta}$ pour tout $x \geq C$. 
		Sans perte de généralité on peut prendre $C > 1$. 
		En conséquence, si $\alpha > 1$, on trouve que 
		\[     \frac{\ln (1+x)}{x^{\alpha}} \leq \frac{x^{(1 - \alpha)/2}}{x^{\alpha}} = \frac{1}{x^{(3 \alpha -1) / 2}},     \]
		pour tout $x \geq C$. 
		Comme $(3 \alpha -1) / 2  > 1$, on conclut que la dernière intégrale dans
		\[     \int_{C}^{+\infty} \frac{\ln (1+x)}{x^{\alpha}} dx \leq \int_{C}^{+\infty} \frac{1}{x^{(3 \alpha -1) / 2}} dx    \]
		est bornée, ce qui implique que la première l'est aussi. 
		Cela nous dit que, pour $\alpha > 1$, $f_{\alpha}$ est intégrable (sur $\RR_{> 0}$) si et seulement si l'intégrale 
		$\int_{0}^{C} \ln(1+x)/x^{\alpha} dx$ converge. 
		En plus, comme $\ln(1+x)/x$ tend vers $1$ quand $x$ tend vers zéro, on voit qu'il existe $\delta > 0$ tel que 
		$1/2 < \ln(1+x)/x < 3/2$ si $x \in \hskip 0.6mm ] \hskip 0.6mm 0 , \delta]$. 
		Sans perte de généralité on peut prendre $\delta < 1 < C$. 
		Comme $\ln(1+x)/x$ est continue sur $\RR_{> 0}$, l'intégrale $\int_{\delta}^{C} \ln (1+x)/x^{\alpha} dx$ converge, ce qui nous dit que, 
		pour $\alpha > 1$,  $f_{\alpha}$ est intégrable (sur $\RR_{> 0}$) si et seulement si l'intégrale 
		$\int_{0}^{\delta} \ln (1+x)/x^{\alpha} dx$ converge. 
		Alors,  
		\[     \int_{0}^{\delta} \frac{1}{2 x^{\alpha -1}} dx \leq \int_{0}^{\delta} \frac{\ln (1+x)}{x^{\alpha}} dx \leq \int_{0}^{\delta} \frac{3}{2 x^{\alpha -1}} dx.     \]
		Comme les première et dernière intégrales convergent si et seulement si $\alpha < 2$, on conclut que 
		$f_{\alpha}$ est intégrable (sur $\RR_{> 0}$) si $\alpha \in \hskip 0.6mm ] \hskip 0.6mm 1 ,  2 \hskip 0.6mm [ \hskip 0.6mm$, 
		et elle diverge si $\alpha \geq 2$. 
		
		En outre, on voit bien que 
		\[     \frac{\ln (1+x)}{x^{\alpha}} \geq \frac{\ln(2)}{x^{\alpha}},     \]
		si $x \geq 1$. 
		Cela implique que 
		\[     \int_{0}^{+\infty}  \frac{\ln (1+x)}{x^{\alpha}} dx \geq \int_{1}^{+\infty}  \frac{\ln (1+x)}{x^{\alpha}} dx \geq \int_{1}^{+\infty} \frac{\ln(2)}{x^{\alpha}} dx,     \]
		et comme la dernière intégrale diverge (vers $+ \infty$) si $\alpha \leq 1$, alors, la première intégrale aussi, ce qui nous dit que 
		$f_{\alpha}$ n'est pas intégrable (sur $\RR_{> 0}$) si $\alpha \leq 1$. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour $\alpha \in \RR$, on définit la fonction $f_{\alpha} : \RR \rightarrow \RR$ via 
	\[     f_{\alpha}(x) = \frac{1}{(x^2+1)^{\alpha}}.     \]
	\begin{enumerate}
		\item Déterminer les valeurs de $\alpha$ telles que $f_{\alpha}$ soit intégrable (sur $\RR$). 
		
		\item On note $I_\alpha$ l'intégrale de $f_{\alpha}$ sur $\RR$ pour les valeurs de $\alpha$ déterminées dans l'item précédent. 
		Établir une relation de récurrence entre les différentes valeurs $I_{\alpha}$ et en déduire l'expression de $I_n$ pour $n \in \NN^{*}$. 
		
		\item Exprimer l'intégrale:
		\[     J_n=\int_{-\infty}^{+\infty}\frac{dx}{(x^2+2px+q)^n}     \] 
		en fonction de $I_n$, lorsque $p$ et $q$ sont deux réels vérifiant l'inégalité $q-p ^2>0$. 
		En déduire l'expression de $J_n$ pour $n\in\NN^{*}$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item 
			On remarque d'abord que, comme $f_{\alpha}(x) \geq 0$, pour tout $ x \in \RR$, $f_{\alpha}$ est intégrable (sur $\RR$) si et seulement si l'intégrale (sur $\RR$) est bornée. 
			En outre, comme $f_{\alpha}(x) $ est une fonction paire, $f_{\alpha}$ est intégrable sur $\RR$ si et seulement si elle intégrable sur 
			$\RR_{\geq 0}$. 
			Comme $f_{\alpha}(x) $ est continue sur $\RR$, l'intégrale $\int_{0}^{1} f_{\alpha}(x) dx$ converge, ce qui nous dit que, 
			$f_{\alpha}$ est intégrable (sur $\RR$) si et seulement si l'intégrale $\int_{1}^{+\infty} f_{\alpha}(x) dx$ converge. 
			
			Or, 
			\[      \frac{1}{2^{\alpha} x^{2 \alpha}} \leq \frac{1}{(x^2+1)^{\alpha}} \leq \frac{1}{x^{2 \alpha}},     \]
			si $x \geq 1$. 
			Si $\alpha > 1/2$, alors l'inégalité 
			\[      \int_{1}^{+\infty} \frac{1}{(x^2+1)^{\alpha}} dx  \leq \int_{1}^{+\infty} \frac{1}{x^{2 \alpha}} dx,     \]
			nous dit que la première intégrale est convergente, \textit{i.e.} $f_{\alpha}$ est intégrable sur $\RR$ pour $\alpha > 1/2$, tandis que, 
			si $\alpha \leq 1/2$, alors l'inégalité 
			\[      \int_{1}^{+\infty} \frac{1}{(x^2+1)^{\alpha}} dx  \geq \int_{1}^{+\infty} \frac{1}{2^{\alpha} x^{2 \alpha}} dx,     \]
			nous dit que la première intégrale diverge, \textit{i.e.} $f_{\alpha}$ est n'est pas intégrable sur $\RR$ pour $\alpha \leq 1/2$.
			
			\item On suppose désormais $\alpha > 3/2$. 
			On voit bien que 
			\begin{align*}
			I_{\alpha} &= \int_{-\infty}^{+\infty} \frac{1}{(x^2+1)^{\alpha}} dx 
			= \int_{-\infty}^{+\infty} \frac{1+x^{2}}{(x^2+1)^{\alpha}} dx - \int_{-\infty}^{+\infty} \frac{x^{2}}{(x^2+1)^{\alpha}} dx  
			\\ 
			&=   I_{\alpha-1} + \bigg[ \frac{x}{2 (\alpha - 1) (x^2+1)^{\alpha-1}} \bigg]_{-\infty}^{+\infty} + \int_{-\infty}^{+\infty} \frac{1}{2 (1-\alpha) (x^2+1)^{\alpha-1}} dx,
			\end{align*}  
			où l'on a fait une intégration par parties avec $u = x$ et $v' = x/(1+x^{2})^{\alpha}$ (\textit{i.e.} $v = (1+x^{2})^{1-\alpha}/(2(1-\alpha))$) dans la dernière égalité. 
			Comme $\alpha > 3/2$ le deuxième opérande du dernier membre de l'équation précédente est nul. 
			On conclut que 
			\begin{align*}
			I_{\alpha} = I_{\alpha -1} + \frac{I_{\alpha-1}}{2 (1-\alpha)} = \frac{2 \alpha - 3}{2 \alpha - 2} I_{\alpha-1},
			\end{align*}
			pour tout $\alpha > 3/2$, ou, de façon équivalente, $I_{\alpha+1} = (2 \alpha - 1) I_{\alpha}/(2 \alpha)$, pour tout $\alpha > 1/2$. 
			En conséquence, 
			\[     I_{n} = I_{1} \prod_{m=1}^{n-1} \frac{2 m - 1}{2 m} = \pi \prod_{m=1}^{n-1} \frac{2 m - 1}{2 m},     \]
			puisque 
			\[     I_{1} = \int_{-\infty}^{+\infty} \frac{1}{x^2+1} dx = \bigg[ \arctan(x) \bigg]_{-\infty}^{+\infty} = \pi.     \]
			
			\item C'est clair que 
			\begin{align*}     J_n &= \int_{-\infty}^{+\infty}\frac{dx}{(x^2+2px+q)^n} = \int_{-\infty}^{+\infty}\frac{dx}{\big((x+p)^2+(q-p^{2})\big)^n} 
			\\
			&= \frac{1}{(q-p^{2})^{n-1/2}} \int_{-\infty}^{+\infty}\frac{dy}{(y^2+1)^n} = \frac{1}{(q-p^{2})^{n-1/2}} I_{n},       
			\end{align*}
			où l'on a fait la substitution $x + p = \sqrt{q-p^{2}} y$. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR_{\geq 0} \to \RR$ une fonction uniformément continue et intégrable (sur $\RR_{\geq 0}$). 
	Montrer que
	\[     \underset{x \rightarrow +\infty}{\lim} f(x)=0.     \]
	
	\begin{preuve}
		On va procéder par l'absurde. 
		Si la dernière limite n'est pas vérifiée, alors il existe $\epsilon > 0$ et une suite croissante $(x_{n})_{n \in \NN} \in \RR^{\NN}_{\geq 0}$ qui converge vers $+ \infty$ telle que $f(x_{n}) > \epsilon$, pour tout $n \in \NN$. 
		On suppose sans perte de généralité que $x_{n+1} > x_{n} + 2$, pour tout $n \in \NN$, et $x_{0}>1$. 
		Comme $f$ est uniformément continue, il existe $\delta > 0$ tel que $|f(x) - f(x_{n})| < \epsilon/2$, pour tout $x \in [x_{n}-\delta,x_{n}+\delta]$ et tout $n \in \NN$. 
		On suppose sans perte de généralité que $\delta < 1$. 
		Cela implique que $[x_{n}-\delta,x_{n}+\delta] \cap [x_{m}-\delta,x_{m}+\delta] = \emptyset$, si $n \neq m$. 
		L'inégalité triangulaire nous dit que $|f(x)| > |f(x_{n})| - |f(x) - f(x_{n})| > \epsilon - \epsilon/2 = \epsilon/2$, pour tout $x \in [x_{n}-\delta,x_{n}+\delta]$ et tout $n \in \NN$. 
		En conséquence, si $I = \sqcup_{n \in \NN} [x_{n}-\delta,x_{n}+\delta] \subseteq \RR_{\geq 0}$, on voit que 
		\[     \int_{0}^{+ \infty} |f(x)| dx \geq  \int_{I} |f(x)| dx = \sum_{n \in \NN} \int_{x_{n}-\delta}^{x_{n}+\delta} |f(x)| dx \geq \sum_{n \in \NN} \int_{x_{n}-\delta}^{x_{n}+\delta} \frac{\epsilon}{2} dx = \sum_{n \in \NN} \delta \epsilon,     \]
		ce qui implique que $f$ n'est pas absolument convergente. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR_{\geq 0} \to \RR_{\geq 0}$ une fonction continue par morceaux, décroissante et intégrable 
	(sur $\RR_{\geq 0}$). 
	Montrer que la fonction $g : \RR_{\geq 0} \to \RR$ donnée par l'expression $g(x) = x(f(x) - f(x+1))$ est intégrable (sur $\RR_{\geq 0}$) et calculer la valeur de son intégrale sur $\RR_{\geq 0}$. 
	
	\begin{preuve}
		Pour $M>0$, on note $I_{M} = \int_{0}^{M} f(x) dx$, avec limite $I = \int_{0}^{+ \infty} f(x) dx$, quand $M$ tend vers $+ \infty$.  
		On voit bien que 
		\begin{align*}     
		\int_{0}^{M} &x\big(f(x) - f(x+1)\big) dx = \int_{0}^{M} x f(x) dx - \int_{0}^{M} x f(x+1) dx 
		\\
		&= \int_{0}^{M} x f(x) dx - \int_{0}^{M} (x+1) f(x+1) dx + \int_{0}^{M} f(x+1) dx 
		\\
		&= \int_{0}^{M} x f(x) dx - \int_{1}^{M+1} y f(y) dy + \int_{1}^{M+1} f(y) dy 
		\\
		&= \int_{0}^{1} x f(x) dx - \int_{M}^{M+1} y f(y) dy + \int_{1}^{M+1} f(y) dy
		\end{align*}
		où l'on a fait la substitution $y = x+1$. 
		C'est clair que la première intégrale du dernier membre converge, puisque $x f(x)$ est une fonction continue par morceaux sur $[0,1]$. 
		En outre, la dernière intégrale du dernier membre converge vers $I - I_{1}$ quand $M$ tend vers $+ \infty$. 
		Il reste à démontrer que le deuxième opérande converge quand $M$ tend vers $+ \infty$. 
		Or, 
		\[     \int_{M}^{M+1} y f(y) dy \leq f(M) \int_{M}^{M+1} y dy = f(M) \bigg( M + \frac{1}{2} \bigg).     \]
		L'inégalité 
		\[     \frac{M}{2} f(M) \leq \int_{M/2}^{M} f(y) dy     \]
		nous dit que $M f(M)$ converge vers zéro quand $M$ tend vers $+ \infty$ et en conséquence, l'intégrale $\int_{M}^{M+1} y f(y) dy$ 
		converge vers zéro quand $M$ tend vers $+ \infty$. 
		Cela implique que la limite de $\int_{0}^{M} x (f(x) - f(x+1)) dx$ quand $M$ tend vers $+ \infty$ existe et elle vaut 
		$\int_{0}^{1} x f(x) dx + I - I_{1}$.
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	\begin{enumerate} 
		\item Montrer que, pour $x \in \RR \setminus \Z \pi$ et $n \in \NN$ :
		\[     \frac{1 }{ 2} +\sum_{k=1}^n \cos (2kx) = \frac{\sin \big((2n+1)x\big) }{ 2 \sin (x)}.     \]
		En déduire 
		\[     \int_0^{\pi/2} \frac{\sin \big((2n+1)x\big) }{\sin x}dx =\frac{\pi }{2}.     \]
		\item Vérifier que l'application $f :[0,\pi/2] \to \RR$ définie par 
		\[     f(x) = \begin{cases} \frac{1 }{x}- \frac{1 }{ \sin (x)}, &\text{si $x \neq 0$}, 
		\\
		0, &\text{si $x=0$}, 
		\end{cases} 
		\] 
		est de classe $C^{1}$. 
		En déduire que 
		\[     \underset{n \rightarrow +\infty}{\lim} \int_0^{\pi/2} \frac{\sin \big((2n+1)x\big)}{  x} = \frac{\pi }{ 2}.     \]
		\item En conclure que 
		\[     \int_0^{+\infty} \frac{\sin (x)}{x}dx = \frac{\pi }{ 2}.     \]
		
		\item Montrer que 
		\[     \bigg|\int_{(n-1)\pi}^{n\pi } \frac{\sin (x)}{x} dx\bigg| \geq \frac{2}{n\pi}     \] 
		pour tout $n \in \NN^{*}$. 
		En déduire que $\int_{0}^{+\infty} (\sin (x)/x) dx$ n'est pas absolument convergente. 
		On dit dans ce cas qu'elle est \emph{semi-convergente}.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate} 
			\item On voit bien que 
			\begin{align*}
			\frac{1}{2} &+ \sum_{k=1}^{n}\cos (2kx) 
			= - \frac{1}{2} + \operatorname{Re}\bigg( \sum_{k=0}^{n}  e^{i 2 k x} \bigg) 
			=  - \frac{1}{2} + \operatorname{Re}\bigg( \frac{e^{i (2 n+1) x}-1}{e^{i 2 x}-1} \bigg)
			\\
			&= - \frac{1}{2} + \operatorname{Re}\bigg( \frac{e^{i (2 n+1) x} - e^{-i  x }}{e^{i  x }- e^{-i x}} \bigg) 
			= - \frac{1}{2} - \frac{1}{2 \sin(x)} \operatorname{Re}\bigg( i(e^{i (2 n+1) x} - e^{-i  x }) \bigg)     
			\\
			&=- \frac{1}{2} - \frac{1}{2 \sin(x)} \Big( -\sin\big((2n+1)x\big) - \sin(x)) \Big) = \frac{\sin \big((2n+1)x\big) }{ 2 \sin (x)},
			\end{align*}
			pour $x \in \RR \setminus \Z \pi$. 
			En conséquence, 
			\begin{align*} 
			\int_0^{\pi/2} \frac{\sin \big((2n+1)x\big) }{\sin x}dx &= \int_0^{\pi/2} dx + 2 \sum_{k=1}^{n} \int_0^{\pi/2}\cos (2kx)  dx 
			\\
			&= \frac{\pi }{2} + \sum_{k=1}^{n} \bigg[ \frac{\sin(2kx)}{k} \bigg]_0^{\pi/2} = \frac{\pi }{2}.     
			\end{align*}
			
			\item C'est clair que $f$ est $C^{1}$ en tout point $x \in [0,\pi/2]$ différent de zéro. 
			En plus, 
			\[     f'(x) = -\frac{1 }{x^{2}} + \frac{\cos(x)}{ \sin (x)^{2}},     \]
			si $x \in \hskip 0.6mm ] \hskip 0.6mm 0, \pi/2]$.
			En outre, la règle de Bernoulli-L'H\^opital nous dit que
			\begin{align*}     
			f'(0) = \underset{x \rightarrow 0}{\lim} \frac{f(x) - f(0)}{x} 
			&= \underset{x \rightarrow 0}{\lim} \frac{\sin(x)-x}{x^{2} \sin(x)} 
			= \underset{x \rightarrow 0}{\lim} \frac{\cos(x)-1}{2x \sin(x) + x^{2} \cos(x)} 
			\\
			&= \underset{x \rightarrow 0}{\lim} \frac{- \sin(x)}{(2-x^{2}) \sin(x) + 4 x \cos(x)}
			\\
			&= \underset{x \rightarrow 0}{\lim} \frac{- \cos(x)}{-6x \sin(x) + (6- x^{2}) \cos(x)} = -\frac{1}{6}. 
			\end{align*}
			La même règle nous dit que 
			\begin{align*}     
			\underset{x \rightarrow 0}{\lim} f'(x) &= \underset{x \rightarrow 0}{\lim} \frac{x^{2}\cos(x)-\sin^{2}(x)}{x^{2} \sin^{2}(x)} 
			= \underset{x \rightarrow 0}{\lim} \frac{2 x \cos(x)-x^{2} \sin(x) - \sin(2x)}{2 x \sin^{2}(x) + x^{2} \sin(2x)}
			\\
			&= \underset{x \rightarrow 0}{\lim} \frac{(2 - x^{2}) \cos(x) - 4 x \sin(x) - 2 \cos(2x)}{2 \sin^{2}(x) + 4 x \sin(x) + 2 x^{2} \cos(2x)}
			\\
			&= \underset{x \rightarrow 0}{\lim} \frac{- 6 x \cos(x) + (x^{2} - 6) \sin(x) + 4 \sin(2x)}{(6 - 4 x^{2}) \sin(2 x) + 12 x \cos(2x)}
			\\
			&= \underset{x \rightarrow 0}{\lim} \frac{(x^{2} - 12) \cos(x) + 8 x \sin(x) + 8 \cos(2x)}{-32 x \sin(2 x) + (24 - 8 x^{2}) \cos(2x)} = -\frac{1}{6} = f'(0).
			\end{align*}
			Cela implique que $f$ est de classe $C^{1}$. 
			Le lemme de Riemann-Lebesgue nous dit alors que 
			\[     0 = \underset{n \rightarrow +\infty}{\lim} \int_0^{\pi/2} \sin\big((2n+1)x\big) f(x) dx 
			= \underset{n \rightarrow +\infty}{\lim} \int_0^{\pi/2} \frac{\sin \big((2n+1)x\big)}{x} - \frac{\pi }{ 2}.     \]
			
			\item 
			On voit bien que 
			\[     \int_0^{\pi (n + 1/2)} \frac{\sin (y)}{y}dy = \frac{\pi/2 }{ 2} \frac{\sin \big((2n+1)x\big)}{x},     \]
			où l'on a fait la substitution $y = (2n+1)x$. 
			Or, on remarque que l'on a déjà démontré que la limite 
			\[     \underset{M \rightarrow +\infty}{\lim}\int_0^{M} \frac{\sin (y)}{y}dy     \]
			existe (voire l'exercice \ref{exo:2}, (e)). 
			Cela implique que 
			\[     \underset{n \rightarrow +\infty}{\lim} \int_0^{\pi (n + 1/2)} \frac{\sin (y)}{y}dy = \underset{n \rightarrow +\infty}{\lim} \frac{\pi/2 }{ 2} \frac{\sin \big((2n+1)x\big)}{x} = \frac{\pi }{ 2}.     \]
			
			\item On voit bien que  
			\[     \bigg|\int_{(n-1)\pi}^{n\pi } \frac{\sin (x)}{x} dx\bigg| = \int_{(n-1)\pi}^{n\pi } (-1)^{n+1} \frac{\sin (x)}{x} dx \geq \frac{1}{n\pi} \int_{(n-1)\pi}^{n\pi } (-1)^{n+1} \sin (x) dx = \frac{2}{n\pi},      \] 
			pour tout $n \in \NN^{*}$. 
			Comme 
			\[     \int_{0}^{N\pi } \frac{|\sin (x)|}{x} dx = \sum_{n=1}^{N} \bigg|\int_{(n-1)\pi}^{n\pi } \frac{\sin (x)}{x} dx\bigg| \geq \sum_{n=1}^{N} \frac{2}{n\pi},     \]
			pour tout $N \in \NN^{*}$, on conclut que $\int_0^{+ \infty} (\sin (x)/x) dx$ n'est pas absolument convergente. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Déterminer si les intégrales suivantes sont convergentes ou absolument convergentes:
	\\
	\begin{enumerate*}[label=(\alph*)]
		\item  $\int_1^{+\infty} \frac{\sin (x)}{ \sqrt{ x} +\cos (x)} dx$, 
		\item  $\int_1^{+\infty} \frac{\sin (x) }{\sqrt{ x } +\sin (x)} dx$.
	\end{enumerate*}
	
	\begin{preuve}
		On remarque d'abord que $\sqrt{x} + \cos (x) \neq 0$ et $\sqrt{x} + \sin (x) \neq 0$, pour tout $x \geq 1$. 
		C'est facile à voir que 
		\[     \underset{x \rightarrow +\infty}{\lim} \frac{\sin (x)/(\sqrt{ x} +\cos (x))}{\sin(x)/\sqrt{x}} = 1 \text{ et } \underset{x \rightarrow +\infty}{\lim} \frac{\sin (x)/(\sqrt{x} +\sin (x))}{\sin(x)/\sqrt{x}} = 1.     \]
		L'exercice \textbf{19} de la fiche $6$ dit que l'intégrale $\int_{1}^{+\infty} \sin(x)/\sqrt{x} dx$ converge, mais la convergence n'est pas absolue. 
		L'exercice \ref{exo:28} implique alors que intégrales données sont convergentes mais elles ne sont pas absolument convergentes. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}
	\label{exo:28}
	Soient $f$ et $g$ des fonctions réelles continues par morceaux sur $[a,b \hskip 0.6mm [ \hskip 0.6mm$
	et supposons $g(x) > 0$, pour tout $x \in [a,b \hskip 0.6mm [ \hskip 0.6mm$. 
	\begin{enumerate}
		\item Montrer que si $g$ est intégrable sur $[a,b \hskip 0.6mm [ \hskip 0.6mm$ alors
		\[     f= o_b (g) \Longrightarrow \int_x^b f =o_b\bigg(\int_x^b g\bigg)     \]
		et 
		\[     f \sim_b g \Longrightarrow \int_x^b f \sim_b \int_x^b g.     \]
		\item Montrer que si $g$ est n'est pas intégrable sur $[a,b \hskip 0.6mm [ \hskip 0.6mm$ alors
		\[     f= o_b (g) \Longrightarrow \int^x_a f =o_b\bigg(\int^x_a g\bigg)     \]
		et 
		\[     f \sim_b g \Longrightarrow \int^x_a f \sim_b \int^x_a g.     \]
	\end{enumerate}
	
	\begin{preuve}
		Il s'agit d'un résultat du cours. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On considère la fonction $f : \RR \rightarrow \RR$ définie par
	\[     f(x) = \int_x^{+\infty} \frac{\operatorname{arctan} (t) }{ 1+t^2} dt.     \]
	Trouver un équivalent de $f(x)$ de la forme $C/x^{\alpha}$ (avec $C \in \RR$ et $\alpha > 0$) lorsque $x$ tend vers $+\infty$. 
	
	\begin{preuve}
		On voit bien que 
		\[     f(x) = \int_x^{+\infty} \frac{\operatorname{arctan} (t) }{ 1+t^2} dt = \underset{M \rightarrow +\infty}{\lim} \bigg[ \frac{\operatorname{arctan}^{2} (t) }{2} \bigg]_{x}^{M} 
		= \frac{\pi^{2}}{8} - \frac{\operatorname{arctan}^{2} (x) }{2}.     \]
		En outre, 
		\[     \underset{x \rightarrow +\infty}{\lim} \frac{f(x)}{\pi/(2x)} 
		= \underset{x \rightarrow +\infty}{\lim} \frac{\pi^{2} - 4 \operatorname{arctan}^{2} (x) }{4\pi/x}
		= \underset{x \rightarrow +\infty}{\lim} \frac{2\operatorname{arctan} (x)/(1+x^2)}{\pi/x^{2}} = 1,     \]
		où l'on a utilisé la règle de Bernoulli-L'H\^opital. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	\begin{enumerate} 
		\item Montrer la fonction $\Gamma (p) = \int_0^{+\infty} e^{-t} t^{p-1} dt$
		est définie et de classe $C^{\infty}$ sur $\RR_{>0}$. 
		\item Montrer que, pour tout $p >0$, $\Gamma (p+1) = p \Gamma (p)$. 
		En déduire la valeur $\Gamma(n)$ pour $n \in \NN^{*}$.
		\item Montrer que 
		\[     \Gamma\bigg(\frac{1 }{ 2}\bigg) = 2\int_0^{+\infty} e^{-x^2} dx = \sqrt{\pi}     \] 
		et en déduire les valeurs $\Gamma (n+\frac{1 }{ 2})$ pour $n \in \NN^{*}$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate} 
			\item On va montrer la convergence de l'intégrale
			\[     \int_0^{+\infty} e^{-t} t^{p-1} |\ln^{n}(t)| dt = \int_0^{1} e^{-t} t^{p-1} |\ln^{n}(t)| dt + \int_{1}^{+\infty} e^{-t} t^{p-1} |\ln^{n}(t)| dt,     \]
			pour tout $n \in \NN$. 
			Comme la fonction $e^{-t} t^{p-1} |\ln^{n}(t)| > 0$ pour tout $t \in \RR_{>0}$, l'intégrale est convergente si et seulement si elle est bornée. 
			Si $t \in \hskip0.6mm ] \hskip0.6mm 0,1]$, alors $e^{-t} t^{p-1} |\ln^{n}(t)| \leq t^{p-1} |\ln^{n}(t)|$, ce qui dit que 
			\[     \int_0^{1} e^{-t} t^{p-1} |\ln^{n}(t)| dt \leq \int_0^{1} t^{p-1} |\ln^{n}(t)| dt.     \]
			On remarque que $t^{\alpha} |\ln^{n}(t)|$ converge vers zéro quand $t$ tend vers $0$, pour tout $\alpha > 0$ et $n \in \NN$. 
			Alors il existe $\delta > 0$ tel que $t^{\alpha} |\ln^{n}(t)| < 1$, pour tout $t \in \hskip 0.6 mm ] \hskip 0.6 mm 0,\delta]$. 
			Soit $0 < \alpha < p$. 
			Alors $t^{p-1} |\ln^{n}(t)| \leq t^{p-1-\alpha}$, pour tout $t \in \hskip 0.6 mm ] \hskip 0.6 mm 0,\delta]$. 
			Cela implique que les intégrales
			\[     \int_0^{\delta} t^{p-1} |\ln^{n}(t)| dt \leq \int_0^{\delta} t^{p-1-\alpha} dt     \] 
			convergent, puisque $p-1-\alpha > -1$. 
			En outre, l'intégrale 
			\[     \int_{\delta}^{1} t^{p-1} |\ln^{n}(t)| dt     \]
			converge, puisque $t^{p-1} |\ln^{n}(t)|$ est continue sur $[\delta,1]$.
			On conclut que 
			\[     \int_0^{1} t^{p-1} |\ln^{n}(t)| dt     \]
			est convergente si $p > 0$, ce qui implique que $\int_0^{1} e^{-t} t^{p-1} |\ln^{n}(t)| dt$ l'est aussi. 
			
			Par ailleurs, si $t \geq 1$, on voit que $e^{-t} t^{p-1} |\ln^{n}(t)|  \leq e^{-t/2} |\ln^{n}(t)| $. 
			Cela implique que 
			\[     \int_{1}^{+ \infty} e^{-t} t^{p-1} |\ln^{n}(t)|  dt \leq \int_{1}^{+ \infty} e^{-t/2} |\ln^{n}(t)|  dt.     \]
			On remarque que $e^{-t/4} |\ln^{n}(t)|$ converge vers zéro quand $t$ tend vers $+ \infty$, pour tout $n \in \NN$. 
			Alors il existe $C > 1$ tel que $e^{-t/4} |\ln^{n}(t)| < 1$, pour tout $t \in \RR_{\geq C}$.
			Cela implique que les intégrales
			\[     \int_{C}^{+ \infty} e^{-t/2} |\ln^{n}(t)|  dt \leq \int_{C}^{+ \infty} e^{-t/4}  dt     \]
			convergent. 
			En outre, l'intégrale 
			\[     \int_{1}^{C} e^{-t/2} |\ln^{n}(t)| dt     \]
			converge, puisque $e^{-t/2} |\ln^{n}(t)|$ est continue sur $[1,C]$.
			On conclut que $\int_{1}^{+ \infty} e^{-t/2} |\ln^{n}(t)|  dt$ est convergente, ce qui implique que 
			$\int_{1}^{+ \infty} e^{-t} t^{p-1} |\ln^{n}(t)|  dt$ l'est aussi. 
			En conséquence, $\int_0^{+\infty} e^{-t} t^{p-1} dt$ converge. 
			
			Soit $f : \RR_{>0}^{2} \rightarrow \RR$ donnée par $f(t,p) = e^{-t} t^{p-1}$. 
			On voit que 
			\[     \frac{\partial^{n} f}{\partial p^{n}}(t,p) = e^{-t} t^{p-1} \ln^{n}(t).     \]
			D'après les majorations dans les paragraphes précédents, on voit que, pour tout $p > 0$, il existe un voisinage $V$ de $p$ 
			tel que $|e^{-t} t^{p-1} \ln^{n}(t)|$ est majorée par une fonction intégrable. 
			Le théorème de différentiation sous le signe de l'intégral nous dit alors que $\Gamma$ est de classe $C^{\infty}$ et que 
			\[     \Gamma^{(n)}(p) = \int_0^{+\infty} e^{-t} t^{p-1} \ln^{n}(t) dt.     \]
			
			\item On voit bien que 
			\[     \Gamma (p+1) = \int_0^{+\infty} e^{-t} t^{p} dt = \underset{M \rightarrow +\infty}{\lim} \bigg[ -e^{-t} t^{p} \bigg]_0^{M} + p \int_0^{+\infty} e^{-t} t^{p-1} dt = p \Gamma (p),     \]
			où l'on a fait une intégration par parties avec $u = t^{p}$ et $v' = e^{-t}$. 
			Comme $\Gamma(1) = \int_0^{+\infty} e^{-t} dt = 1$, on conclut que $\Gamma(n) = (n-1)!$, pour tout $n \in \NN^{*}$.
			
			\item On voit bien que 
			\[      \Gamma\bigg(\frac{1}{ 2}\bigg) = \int_0^{+\infty} e^{-t} \sqrt{t} dt = 2\int_0^{+\infty} e^{-x^2} dx = \sqrt{\pi},     \]
			où l'on a fait la substitution $t = x^{2}$. 
			D'après l'item précédent, on voit que 
			\[     \Gamma(n+1/2)= \frac{(2n-1)!! \sqrt{\pi}}{2^{n}},     \]
			où l'on rappelle que $(2n+1)!! = (2n-1)!! (2n+1)$ et $1!! = 1$, pour tout $n \in \NN^{*}$.
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On définit la fonction $F : \RR_{>0} \rightarrow \RR$ via 
	\[     F(x) = \int_{0}^{+\infty} \frac{\sin (t)}{ x^2+t^2} dt.     \]
	\begin{enumerate} 
		\item Montrer que la fonction $F$ est de classe $C^{\infty}$. 
		\item Trouver les limites en $0$ et en $+\infty$ de $F$. 
		\item Étudier le signe et les variations de $F$ sur $\RR_{>0}$.
		\\
		\textbf{Indication: } pour $x>0$ fixé, étudier la suite 
		\[     u_n= \int_{n\pi}^{(n+1)\pi} \frac{|\sin (t)|}{ x^2+t^2} dt.     \]
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate} 
			\item Soit $f : \RR_{>0}^{2} \rightarrow \RR$ donnée par $f(t,x) = \sin(t)/(x^2+t^2)$. 
			Un argument par récurrence simple nous dit que, pour tout $n \in \NN$, il existe un polynôme $P_{n}$ à coefficients réels de degré inférieur ou égal à $n$ tel que  
			\[     \frac{\partial^{n} f}{\partial x^{n}}(t,x) = \frac{P_{n}(x) \sin(t)}{(x^2+t^2)^{n+1}}.     \]
			En conséquence, 
			\[     \bigg| \frac{\partial^{n} f}{\partial x^{n}}(t,x) \bigg| \leq \frac{|P_{n}(x)|}{(x^2+t^2)^{n+1}},     \]
			pour tout $(t,x) \in \RR^{2}_{>0}$. 
			Le théorème de différentiation sous le signe de l'intégral nous dit alors que $F$ est de classe $C^{\infty}$. 
			
			\item Pour $x >0$, on voit bien que 
			\begin{align*}
			F(x) &= \frac{1}{x^{2}} \int_{0}^{+\infty} \frac{\sin (t)}{ 1+(t/x)^2} dt = \frac{1}{x} \int_{0}^{+\infty} \frac{\sin (x y)}{1+y^2} dy,
			\end{align*} 
			où l'on a fait la substitution $y = t/x$. 
			En conséquence, 
			\begin{align*}
			|F(x)| \leq \frac{1}{x} \int_{0}^{+\infty} \frac{1}{1+y^2} dy \leq \frac{\pi}{2 x},
			\end{align*} 
			ce qui dit que la limite de $F(x)$ quand $x$ tend vers $+ \infty$ est zéro. 
			
			Comme $\sin(t)/t$ converge vers $1$ quand $t$ tend vers $0$, il existe $\delta > 0$ tel que $1/2 < \sin(t)/t$, pour tout $t \in \RR_{>0}$ tel que $t \leq \delta$. 
			%Sans perte de généralité on peut prendre $\delta < \pi$. 
			Cela implique que 
			\[     \int_{0}^{\delta} \frac{\sin (t)}{ x^2+t^2} dt \geq \int_{0}^{\delta} \frac{t}{2(x^2+t^2)} dt = \bigg[\frac{\ln(x^{2}+t^{2})}{4} \bigg]_{0}^{\delta} = \frac{\ln\big(1+(\delta/x)^{2}\big)}{4}.     \]
			On voit bien que le dernier membre diverge (vers $+\infty$) quand $x$ tend vers zéro 
			En outre, on considère la fonction $g : \RR_{\geq \delta} \times \RR_{\geq 0} \rightarrow \RR$ donnée par $g(t,x) = \sin (t)/(x^{2}+t^2)$. 
			On voit bien que $|g(t,x)| \leq 1/t^{2}$, pour tout $x \in \RR_{\geq 0}$, et $1/t^{2}$ est absolument intégrable sur $\RR_{\geq \delta}$. 
			Le théorème de convergence dominée nous dit que 
			\[     \underset{x \rightarrow 0}{\lim} \int_{\delta}^{+\infty} \frac{\sin (t)}{ x^2+t^2} dt = \int_{\delta}^{+\infty} \frac{\sin (t)}{t^2} dt,     \]
			\textit{i.e.} la limite de $\int_{\delta}^{+\infty} \sin (t)/(x^2+t^2) dt$ converge quand $x$ tend vers zéro. 
			En conséquence, la limite de $F(x) = \int_{0}^{+\infty} \sin (t)/(x^2+t^2) dt$ diverge quand $x$ tend vers zéro
			
			\item Pour tout $x \in \RR_{\geq 0}$ et $n \in \NN$, on considère 
			\[     u_n(x) = \int_{n\pi}^{(n+1)\pi} \frac{|\sin (t)|}{ x^2+t^2} dt \geq 0.     \]
			%C'est facile à voir que $u_{n}(x) \leq u_{n}(y)$, si $x \geq y$. 
			On remarque que $u_{n}(x) \geq u_{n+1}(x)$, pour tout $x \in \RR_{\geq 0}$ et $n \in \NN$, puisque 
			\begin{align*}
			u_{n+1}(x) &= \int_{(n+1)\pi}^{(n+2)\pi} \frac{|\sin (t)|}{ x^2+t^2} dt = \int_{n\pi}^{(n+1)\pi} \frac{|\sin (y)|}{ x^2+y^2 + \pi (\pi + 2t)} dy 
			\\
			&\leq \int_{n\pi}^{(n+1)\pi} \frac{|\sin (y)|}{ x^2+y^2} dy = u_{n}(x),     
			\end{align*} 
			où l'on a fait la substitution $t = y + \pi$. 
			En conséquence, 
			\[     F(x) = \sum_{n=0}^{+\infty} (-1)^{n} u_{n}(x) = \sum_{m=0}^{+\infty} \big(\underset{\geq 0}{\underbrace{u_{2m}(x) - u_{2m+1}(x)}}\big) \geq 0.     \]
			
			Par ailleurs, d'après le premier item on sait que 
			\begin{align*}
			F'(x) &= - \int_{0}^{+\infty} \frac{2 x \sin (t)}{ (x^2+t^2)^{2}} dt.          
			\end{align*} 
			Soit 
			\[     v_n(x) = \int_{n\pi}^{(n+1)\pi} \frac{2 x |\sin (t)|}{(x^2+t^2)^{2}} dt \geq 0     \]
			pour tout $x \in \RR_{\geq 0}$ et $n \in \NN$. 
			Le même argument que pour $u_{n}(x)$ nous dit que $v_{n}(x) \geq v_{n+1}(x)$, pour tout $x \in \RR_{\geq 0}$ et $n \in \NN$. 
			Cela implique que 
			\[     F'(x) = - \sum_{n=0}^{+\infty} (-1)^{n} v_{n}(x) = -\sum_{m=0}^{+\infty} \big(\underset{\geq 0}{\underbrace{v_{2m}(x) - v_{2m+1}(x)}}\big) \leq 0.     \]
			En conséquence, $F$ est décroissante. 
			
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} 
	On pose
	\[     B(u,v)=\int_0^1t^{u-1}(1-t)^{v-1}dt.     \]
	\begin{enumerate} 
		\item Pour quelles valeurs de $(u,v) \in \RR^2$ cette intégrale est-elle définie ? 
		On note $\mathcal{D}$ l'ensemble de ces valeurs.
		
		\item Montrer que 
		\[     B(u,v)=\int_0^{+\infty} \frac{s^{u-1}}{(1+s)^{u+v}}ds,     \]
		pour tout $(u,v)\in \mathcal{D}$.
		
		\item Calculer $B(1/2,1/2)$.
		
		\item Prouver que $(u,v)\in \mathcal{D}$ si et seulement si $(v,u)\in \mathcal{D}$, et $B(v,u)=B(u,v)$.
		
		\item Montrer que, si $(u,v)\in \mathcal{D}$, alors $(u+1,v), (u,v+1) \in \mathcal{D}$ et 
		\[     B(u,v)=B(u+1,v)+B(u,v+1).     \]
		
		\item Au moyen d'une intégration par parties, prouver que 
		\[     B(u+1,v)=\frac {u}{u+v}B(u,v),     \] 
		pour tout $(u,v)\in \mathcal{D}$.
		
		\item En déduire l'expression de $B(u+n,v)$ en fonction de $B(u,v)$ pour tout
		entier naturel non nul $n$.
		
		\item Calculer $B(n,p)$, avec $(n,p)\in \NN^{*}\times \NN^{*}$.
		
		\item Calculer $B(1/3,2/3)$. 
		%\\
		%\textbf{Indication :} chercher une paramétrisation de la courbe $y^3=t^2(1-t)$ pour $t \in [0,1]$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item Comme $t^{u-1}(1-t)^{v-1} \geq 0$ pour tout $t \in \hskip 0.6mm]\hskip 0.6mm 0,1\hskip 0.6mm[\hskip 0.6mm$, 
			l'intégrale qui définit $B$ est convergente si et seulement si elle est majorée. 
			C'est facile à vérifier que $\mathcal{D} = \RR^{2}_{>0}$. 
			En effet, si $u \leq 0$, alors 
			\[     \int_0^1 t^{u-1}(1-t)^{v-1}dt \geq \int_0^1t^{u-1}dt.     \]
			Comme la dernière intégrale diverge (vers $+\infty$), la première intégrale aussi. 
			Le cas de $v \leq 0$ est analogue.  
			
			\item L'expression donnée suit de fait la substitution $t = s/(s+1)$. 
			On remarque que l'application de $\RR_{\geq 0}$ dans $[0,1 \hskip 0.6mm [ \hskip 0.6mm $ donnée par $s \mapsto s/(s+1)$ 
			est strictement croissante (et donc bijective), différentiable, avec application réciproque $t \mapsto t/(1-t)$ différentiable. 
			
			\item La première partie de la question est triviale. 
			En plus, c'est clair que 
			\begin{align*}     
			B(1/2,1/2) &= \int_0^1 \frac{1}{\sqrt{t(1-t)}} dt = \int_0^1 \frac{1}{\sqrt{(1/4) - (t-1/2)^{2}}} dt =  \int_{-1}^1 \frac{1}{\sqrt{(1 - x^{2}}} dx 
			\\
			&= \bigg[ \arcsin(x)\bigg]_{-1}^{1} = \pi,     
			\end{align*}
			où l'on a fait la substitution $t-1/2 = x/2$. 
			
			\item La première partie de la question est triviale. 
			Par ailleurs, on voit bien que 
			\[     B(u,v)=\int_0^1t^{u-1}(1-t)^{v-1}dt = \int_0^1 (1-x)^{u-1}x^{v-1}dx = B(v,u),     \]
			où l'on a fait la substitution $x = 1- t$.
			
			\item La première partie de la question est triviale. 
			D'ailleurs, c'est clair que 
			\begin{align*}   
			B(u,v) &= \int_0^1t^{u-1}(1-t)^{v-1}dt = \int_0^1t^{u-1}(1-t)^{v-1} \big(t + (1-t)\big)dt 
			\\
			&= \int_0^1 t^{u}(1-t)^{v-1}dt + \int_0^1t^{u-1}(1-t)^{v}dt = B(u+1,v)+B(u,v+1).  
			\end{align*}
			
			\item On voit bien que 
			\begin{align*}     
			B(u+1,v)&=\int_0^1t^{u}(1-t)^{v-1}dt = - \frac{1}{v} \bigg[ t^{u}(1-t)^{v} \bigg]_0^1 + \frac{u}{v} \int_0^1t^{u-1}(1-t)^{v}dt 
			\\ 
			&= \frac{u}{v} B(u,v+1) = \frac{u}{v} \big(B(u,v) - B(u+1,v)\big),     
			\end{align*}
			où l'on a fait une intégration par parties avec $f(t) = t^{u}$ et $g'(t) = (1-t)^{v-1}$ (\textit{i.e.} $g(t) = -(1-t)^{v}/v$) et l'item précédent. 
			En conséquence, 
			\[     B(u+1,v)=\frac {u}{u+v}B(u,v).     \]   
			
			\item C'est facile à vérifier par récurrence que 
			\[     B(u+n,v) = \prod_{i=0}^{n-1} \frac{u+i}{u+i+v} B(u,v),      \]
			pour tout $n \in \NN^{*}$. 
			
			\item C'est facile à vérifier par récurrence que 
			\[     B(n,p) = \frac{(n-1)! (p-1)!}{(n+p-1)!},     \]
			pour tout $(n,p)\in \NN^{*}\times \NN^{*}$.  
			
			\item D'après l'item (b), on voit bien que
			\begin{align*}
			B(&1/3,2/3) = \int_0^{+\infty} \frac{1}{\sqrt[3]{s^{2}}(1+s)}ds = 3 \int_0^{+\infty} \frac{1}{1+x^{3}}dx 
			\\
			&= \int_0^{+\infty} \bigg( \frac{1}{1+x} - \frac{2x-1}{2(x^{2}-x+1)} + \frac{3/2}{x^{2}-x+1}  \bigg) dx 
			\\
			&= \int_0^{+\infty} \bigg( \frac{1}{1+x} - \frac{2x-1}{2(x^{2}-x+1)} + \frac{3/2}{(x-1/2)^{2}+3/4}  \bigg) dx 
			\\
			&= \underset{M \rightarrow + \infty}{\lim}\bigg[ \ln(1+x) - \frac{\ln(x^{2}-x+1)}{2} + \sqrt{3} \arctan\Big(\frac{2x-1}{\sqrt{3}}\Big)  \bigg]_{0}^{M}
			\\
			&= \underset{M \rightarrow + \infty}{\lim}\bigg( \ln\Big(\frac{M+1}{\sqrt{M^{2}-M+1}}\Big) + \sqrt{3} \arctan\Big(\frac{2M-1}{\sqrt{3}}\Big)  \bigg) + \frac{\pi}{2 \sqrt{3}}
			\\
			&= \frac{2 \pi}{\sqrt{3}},
			\end{align*}
			où l'on a fait la substitution $s = x^{3}$ et on a utilisé $\arctan(1/\sqrt{3}) = \pi/6$.  
		\end{enumerate}
	\end{preuve}
	
	
	%%%%%%%%%%%%%%%
	\exo{}
	\begin{enumerate} 
		\item Montrer que, pour tout x réel, les intégrales généralisées
		\[
		\int_0^{+\infty }\frac{e^{-t}\cos (tx)}{\sqrt{t}}dt, 
		\int_0^{+\infty }\frac{e^{-t}\sin (tx)}{\sqrt{t}}dt 
		\text{ et } 
		\int_0^{+\infty }\frac{e^{(-1+ix)t}}{\sqrt{t}}
		dt 
		\]
		convergent.
		
		\item On définit alors $2$ applications $u, v : \RR \rightarrow \RR$ 
		et une application $z : \RR \rightarrow \C$ par 
		\[
		u(x) = \int_0^{+\infty }\frac{e^{-t}\cos (tx)}{\sqrt{t}}dt, 
		v(x) = \int_0^{+\infty }\frac{e^{-t}\sin (tx)}{\sqrt{t}}dt 
		\]
		et 
		\[
		z(x) = \int_0^{+\infty }\frac{e^{(-1+ix)t}}{\sqrt{t}}dt. 
		\]
		Étudier la parité des applications $u$ et $v$ et montrer que $z(x)=u(x)+iv(x)$, pour tout $x \in \RR$.
		
		\item On pose $\lambda = u(0)$. 
		Montrer que $\lambda > 1- e^{-1}$.
		
		\item Montrer que les applications $u$, $v$ et $z$ sont continues.
		
		\item Montrer que, pour tout réel $x$, l'intégrale généralisée 
		\[     \int_0^{+\infty} \sqrt{t}e^{(-1+ix)t}dt     \] 
		converge.
		
		\item Montrer que les applications $u$, $v$ et $z$ sont dérivables.
		
		\item Prouver que
		\[     z'(x)= \frac{-1}{2(x+i)}z(x),     \]
		pour tout $x \in \RR$. 
		\\
		\textbf{Indication :} on pourra utiliser une intégration par parties. 
		
		\item En déduire que les applications $u$, $v$ et $z$ sont indéfiniment dérivables.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item Par définition, l'intégrale sur $\RR_{>0}$ d'une fonction $f : \RR_{>0} \rightarrow \C$ de la forme $f(x) = a(x) + i b(x)$, avec $a,b : \RR_{>0} \rightarrow \RR$, existe si et seulement si les intégrales de $a$ et de $b$ sur $\RR_{>0}$ existent. 
			En outre, par définition, une fonction complexe $f : \RR_{>0} \rightarrow \C$ est intégrable si sa valeur absolue 
			$|f| : \RR_{>0} \rightarrow \RR_{\geq 0}$ est intégrable. 
			Il suffit alors d'étudier la convergence de l'intégrale 
			\begin{equation}
			\label{eq:intn}
			\int_0^{+\infty} \frac{|e^{(-1+ix)t}|}{\sqrt{t}} dt = \int_0^{+\infty} \frac{e^{-t}}{\sqrt{t}} dt = \int_0^{1} \frac{e^{-t}}{\sqrt{t}} dt + \int_1^{+\infty} \frac{e^{-t}}{\sqrt{t}} dt.
			\end{equation}
			Comme $e^{-t}/\sqrt{t} \geq 0$ pour tout $t >0$, il suffit de montrer que ces intégrales sont majorées. 
			L'inégalité $1/\sqrt{t} \leq 1$ pour tout $t \geq 1$ nous dit que 
			\[     \int_1^{+\infty} \frac{e^{-t}}{\sqrt{t}} dt \leq \int_1^{+\infty} e^{-t} dt = e^{-1}.     \]
			En conséquence, la dernière intégrale dans \eqref{eq:intn} est convergente. 
			Par ailleurs, si $0< t \leq 1$, on voit que $e^{-t}/\sqrt{t} \leq 1/\sqrt{t}$, ce qui implique que 
			\[     \int_0^{1} \frac{e^{-t}}{\sqrt{t}} dt \leq \int_0^{1} \frac{1}{\sqrt{t}} dt = 2.     \]
			En conséquence, l'avant-dernière intégrale dans \eqref{eq:intn} est convergente. 
			Par conséquent, les intégrales données sont absolument convergentes.  
			
			\item C'est clair que 
			\[     u(-x) = \int_0^{+\infty }\frac{e^{-t}\cos (-tx)}{\sqrt{t}}dt = \int_0^{+\infty }\frac{e^{-t}\cos (tx)}{\sqrt{t}}dt = u(x)     \]
			et 
			\[     v(-x) = \int_0^{+\infty }\frac{e^{-t}\sin(-tx)}{\sqrt{t}}dt = -\int_0^{+\infty }\frac{e^{-t}\sin (tx)}{\sqrt{t}}dt = - v(x).     \]
			L'égalité $z(x)=u(x)+iv(x)$, pour tout $x \in \RR$, est une conséquence de la définition d'intégrale. 
			
			\item On voit que
			\[     \lambda = u(0) = \int_0^{+\infty} \frac{e^{-t}}{\sqrt{t}}dt = \int_0^{1} \frac{e^{-t}}{\sqrt{t}}dt + \int_{1}^{+\infty} \frac{e^{-t}}{\sqrt{t}}dt
			> \int_0^{1} e^{-t} dt = 1 - e^{-1},     \]
			où l'on a utilisé que $e^{-t}/\sqrt{t} \geq e^{-t}$ si $0 < t \leq 1$ et $e^{-t}/\sqrt{t} > 0$ si $t \geq 1$.
			
			\item La majoration $|e^{-t} e^{itx}|/\sqrt{t} \leq e^{-t} /\sqrt{t}$ et le théorème de convergence dominée nous disent que $z$ 
			est une application continue, ce qui implique que $u$ et $v$ sont continues aussi. 
			
			\item Il suffit alors d'étudier la convergence de l'intégrale 
			\begin{equation}
			\label{eq:intn2}
			\int_0^{+\infty} |e^{(-1+ix)t}| \sqrt{t} dt = \int_0^{+\infty} \sqrt{t} e^{-t} dt = \int_0^{1} \sqrt{t} e^{-t} dt + \int_1^{+\infty} \sqrt{t} e^{-t} dt.
			\end{equation}
			Comme $\sqrt{t} e^{-t} \geq 0$ pour tout $t >0$, il suffit de montrer que ces intégrales sont majorées. 
			L'inégalité $\sqrt{t} e^{-t} \leq e^{-t/2}$ pour tout $t \geq 1$ nous dit que 
			\[     \int_1^{+\infty} \sqrt{t} e^{-t} dt \leq \int_1^{+\infty} e^{-t/2} dt = 2 e^{-1/2}     \]
			En conséquence, la dernière intégrale dans \eqref{eq:intn2} est convergente. 
			Par ailleurs, si $0 < t \leq 1$, on voit que $e^{-t} \sqrt{t} \leq \sqrt{t}$, ce qui implique que 
			\[     \int_0^{1} \sqrt{t} e^{-t} dt \leq \int_0^{1} \sqrt{t} dt = \frac{2}{3}.     \]
			En conséquence, l'avant-dernière intégrale dans \eqref{eq:intn2} est convergente. 
			Cela nous dit que la première 'intégrale dans \eqref{eq:intn2} converge. 
			
			\item Soit $f : \RR_{>0} \times \RR \rightarrow \C$ donnée par $f(t,x) = e^{(-1+ix)t}/\sqrt{t}$. 
			On remarque que 
			\[     \frac{\partial f}{\partial x}(t,x) = i e^{(-1+ix)t} \sqrt{t}.     \]
			L'inégalité $|i e^{(-1+ix)t} \sqrt{t}| \leq \sqrt{t} e^{-t}$ et le théorème de différentiation sous le signe de l'intégrale nous disent que $z$ 
			est une application différentiable, ce qui implique que $u$ et $v$ sont différentiables aussi. 
			En plus, 
			\[     z'(x) = i \int_0^{+\infty} e^{(-1+ix)t} \sqrt{t} dt.     \]
			
			\item Pour tout $x \in \RR$, on voit bien que 
			\begin{align*}
			z(x) &= \int_0^{+\infty} \frac{e^{(-1+ix)t}}{\sqrt{t}}dt = \underset{M \rightarrow + \infty}{\lim} \bigg[ 2 e^{(-1+ix)t} \sqrt{t} \bigg]_{0}^{M} - 2(-1+ix) \int_0^{+\infty} e^{(-1+ix)t} \sqrt{t} dt 
			\\
			&= - 2i(i+x) \int_0^{+\infty} e^{(-1+ix)t} \sqrt{t} dt = - 2 (i+x) z'(x),     
			\end{align*}
			où l'on a fait une intégration par parties avec $f(t) = e^{(-1+ix)t}$ et $g'(t) = 1/\sqrt{t}$.
			
			\item Il s'agit d'une conséquence directe de l'item précédent. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On considère l'expression   
	\[     f(x) = \int_{0}^{+\infty} \frac{\sin (xt)}{1+t}dt.      \]
	Calculer le domaine de définition de $f$ (dans $\RR$) et l'ensemble des points de continuité. 
	En particulier, étudier 
	\[     \underset{x \rightarrow 0+}{\lim} f(x)     \] 
	et un équivalent de $f$ en $+\infty$.
	
	\begin{preuve}
		D'abord, on voit bien que $f(0) = 0$. 
		En outre, comme $\sin$ est une fonction impaire, on voit que si $x$ est dans le domaine de définition de $f$, alors $-x$ aussi 
		et $f(-x) = - f(x)$. 
		Pour $x \neq 0$, on voit bien que 
		\begin{equation}
		\label{eq:intw}
		f(x) = \int_{0}^{+\infty} \frac{\sin (xt)}{1+t}dt = \frac{1}{x} \bigg(1 - \int_{0}^{+\infty} \frac{\cos (x t)}{(1+t)^{2}} dt \bigg),
		\end{equation}
		où l'on a fait une intégration par parties avec $u(t) = 1/(1+t)$ et $v'(t) = \sin(xt)$ (\textit{i.e.} $v(t) = -\cos(xt)/x$). 
		Comme 
		\[     \int_{0}^{+\infty} \frac{|\cos (x t)|}{(1+t)^{2}} dt \leq \int_{0}^{+\infty} \frac{1}{(1+t)^{2}} dt = 1,     \]
		la dernière intégrale dans \eqref{eq:intw} converge (absolument), ce qui implique que l'intégrale qui définit $f$ converge. 
		En conséquence, le domaine de définition de $f$ est $\RR$. 
		On définit l'application $g : \RR \rightarrow \RR$ via 
		\[     g(x) = \int_{0}^{+\infty} \frac{\cos (x t)}{(1+t)^{2}} dt.     \]
		La majoration $|\cos (x t)|/(1+t)^{2} \leq 1/(1+t)^{2}$ et le théorème de convergence dominée nous disent que $g$ est continue sur $\RR$.  
		Cela implique que $f$ est continue en tout $x \neq 0$. 
		En plus, le théorème de Riemann-Lebesgue nous dit que 
		\[     \underset{x \rightarrow + \infty} {\lim} \frac{f(x)}{1/x} = \underset{x \rightarrow + \infty} {\lim} 1 - \int_{0}^{+\infty} \frac{\cos (x t)}{(1+t)^{2}} dt = 1,     \]
		\textit{i.e.} $f \sim 1/x$ quand $x \rightarrow + \infty$. 
		
		Pour étudier le comportement de $f$ autour de zéro, comme $f$ est impaire, il suffit de considérer le cas $x > 0$. 
		On fait d'abord la substitution $y = t x$ et une intégration par parties avec $u(y) = 1/(x+y)$ et $v'(y) = \sin(y)$ (on a choisi $v(y) = 1 - \cos(y)$), ce qui donne 
		\[         f(x) = \int_{0}^{+\infty} \frac{\sin (xt)}{1+t}dt = \int_{0}^{+\infty} \frac{\sin (y)}{x+y}dy = \int_{0}^{+\infty} \frac{1 - \cos (y)}{(x+y)^{2}}dy.     \]
		Noter que l'intégrande de la dernière intégrale est une fonction non négative. 
		En particulier $f(x) > 0$, pour tout $x > 0$. 
		On affirme maintenant que la fonction non négative $(1 - \cos (y))/y^{2}$ est intégrable sur $\RR_{\geq 0}$. 
		En effet, comme la limite de $(1 - \cos (y))/y^{2}$ est $2$ quand $y$ tend vers zéro, l'intégrale $\int_{0}^{+\infty} (1 - \cos (y))/y^{2} dy$ 
		converge si et seulement si $\int_{1}^{+\infty} (1 - \cos (y))/y^{2} dy$ converge. 
		Or, l'inégalité $(1 - \cos (y))/y^{2} \leq 2/y^{2}$ pour tout $y > 0$ et le fait que $y \mapsto 2/y^{2}$ est intégrable sur $\RR_{\geq 1}$ 
		nous disent que $\int_{1}^{+\infty} (1 - \cos (y))/y^{2} dy$ converge. 
		Or, on a l'inégalité $(1 - \cos (y))/(x+y)^{2} \leq (1 - \cos (y))/y^{2}$, pour tout $y > 0$. 
		Le théorème de convergence dominée implique que la limite suivante existe et
		\[     \underset{x \rightarrow 0} {\lim} \int_{\delta}^{+\infty} \frac{1 - \cos (y)}{(x+y)^{2}}dy = \int_{\delta}^{+\infty} \frac{1 - \cos (y)}{y^{2}}dy.     \]
		Comme $(1 - \cos (y))/y^{2}$ est une fonction continue, non négative et non nulle, son intégrale est strictement positive (en fait, on peut démontrer que cette intégrale vaut $\pi/2$). 
		En conséquence, la limite $\ell$ de $f$ en $0+$ est strictement positive. 
		Comme $f(0) = 0$, on conclut que $f$ n'est pas continue en zéro. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On considère l'expression   
	\[     f(x) = \int_{1}^{+\infty} \frac{1}{t^{x+1}+t+1}dt.     \]
	Calculer le domaine de définition de $f$ (dans $\RR$) et l'ensemble des points de continuité. 
	
	\begin{preuve}
		Comme $1/(t^{x+1}+t+1) > 0$ pour tout $t \geq 1$, l'intégral définissant $f$ converge si et seulement si elle est majorée. 
		Si $x > 0$, alors
		\[     \int_{1}^{+\infty} \frac{1}{t^{x+1}+t+1}dt \leq \int_{1}^{+\infty} \frac{1}{t^{x+1}}dt = \underset{M \rightarrow + \infty} {\lim} \bigg[ - \frac{1}{t^{x}x} \bigg]_{1}^{M} = \frac{1}{x},     \]
		où l'on a utilisé que $t^{x+1} + t + 1 \geq t^{x+1}$. 
		Si $x \leq 0$, alors $t^{x+1} + t + 1 \leq 3 t$ nous dit que 
		\[     \int_{1}^{+\infty} \frac{1}{t^{x+1}+t+1}dt \geq \int_{1}^{+\infty} \frac{1}{3 t}dt.     \] 
		Comme la dernière intégrale diverge (vers $+ \infty$), alors $\int_{1}^{+\infty} 1/(t^{x+1}+t+1)dt$ diverge aussi. 
		En conséquence, le domaine de définition de $f$ est $\RR_{> 0}$. 
		
		Pour $x_{0} > 0$, soient $0 < a < b$ tels que $a < x_{0} < b$. 
		On voit bien que $1/(t^{x+1}+t+1) \leq 1/(t^{a+1}+t+1)$, qui est intégrable sur $\RR_{\geq 1}$. 
		Le théorème de convergence dominée nous dit que $f$ est continue en $x_{0}$. 
		En conséquence, $f$ est continue sur $\RR_{> 0}$.  
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f : \RR_{\geq 0} \rightarrow \RR$ la fonction définie par 
	\[     f(x) = \int_{0}^{+\infty }\frac{e^{-xt^{2}}}{1+t^{2}}dt.     \]
	\begin{enumerate} 
		\item Montrer que $f$ est continue et différentiable sur $\RR_{>0}$. 
		\item Calculer la limite de $f$ en $+\infty $.
		\item Trouver une équation différentielle (non triviale) vérifiée par $f$.
		\item Montrer que $f$ n'est pas différentiable en $0$.
	\end{enumerate}
	
	\begin{preuve}
		\begin{enumerate}
			\item On voit bien que $e^{-xt^{2}}/(1+t^{2}) \leq 1/(1+t^{2})$, pour tous $x \geq 0$ et $t \geq 0$. 
			Comme $1/(1+t^{2})$ est intégrable sur $\RR_{\geq 0}$, le théorème de convergence dominée nous dit que $f$ est continue sur $\RR_{0}$. 
			En outre, comme la dérivée partielle suivante existe, est continue et 
			\[     \bigg| \frac{\partial}{\partial x}\bigg( \frac{e^{-xt^{2}}}{1+t^{2}} \bigg)  \bigg| = \bigg| - \frac{t^{2} e^{-xt^{2}}}{1+t^{2}}   \bigg| \leq e^{-x t^{2}},     \]   
			le théorème de différentiation sous le signe de l'intégrale nous dit que $f$ est dérivable sur $\RR_{>0}$ et 
			\[     f'(x) = -\int_{0}^{+\infty }\frac{t^{2}e^{-xt^{2}}}{1+t^{2}}dt,     \]
			pour tout $x>0$.
			
			\item On voit bien que 
			\[     0 \leq f(x) = \int_{0}^{+\infty }\frac{e^{-xt^{2}}}{1+t^{2}}dt \leq \int_{0}^{+\infty } e^{-xt^{2}} dt = \frac{1}{\sqrt{x}}\int_{0}^{+\infty } e^{-s^{2}} ds = \frac{\pi}{2\sqrt{x}},     \]
			pour tout $x>0$, où l'on a fait la substitution $s = \sqrt{x} t$. 
			Cela implique que 
			\[     \underset{x \rightarrow + \infty} {\lim} f(x) = 0.     \]
			
			\item On voit bien que 
			\[     f'(x) = -\int_{0}^{+\infty }\frac{t^{2}e^{-xt^{2}}}{1+t^{2}}dt = -\int_{0}^{+\infty } e^{-xt^{2}} dt +\int_{0}^{+\infty }\frac{e^{-xt^{2}}}{1+t^{2}}dt = - \frac{\pi}{2\sqrt{x}} + f(x),     \]
			pour tout $x>0$,
			où l'on a utilisé le calcul dans l'item précédent pour la première intégrale dans l'avant-dernier membre. 
			
			\item %On note que $f(0) = \pi/2$. 
			C'est clair que 
			\[     \frac{f(x) - f(0)}{x} = \frac{1}{x} \int_{0}^{+\infty }\frac{e^{-xt^{2}}-1}{1+t^{2}}dt = \frac{1}{\sqrt{x}} \int_{0}^{+\infty }\frac{e^{-s^{2}}-1}{x+s^{2}}ds,     \]
			pour tout $x>0$, où l'on a fait la substitution $s = \sqrt{x} t$. 
			Comme $e^{-s^{2}}-1 \leq 0$, pour tout $s > 0$, on voit que 
			\[     \frac{|f(x) - f(0)|}{x} = \frac{1}{\sqrt{x}} \int_{0}^{+\infty }\frac{1-e^{-s^{2}}}{x+s^{2}}ds \geq \frac{1}{\sqrt{x}} \int_{0}^{+\infty }\frac{1-e^{-s^{2}}}{1+s^{2}}ds = \frac{1}{\sqrt{x}}\big( f(0) - f(1) \big),     \]
			si $x < 1$. 
			Cela implique que 
			\[     \underset{x \rightarrow 0+} {\lim} \frac{|f(x) - f(0)|}{x} = + \infty,     \]
			\textit{i.e.} la dérivée (latérale à droite) de $f$ en zéro n'existe pas. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $I$ un intervalle de $\RR$. 
	On note $E$ l'ensemble des fonctions continues sur l'intérieur de $I$ à valeurs réelles. 
	On note $\mathscr{L}^1(I)$ l'ensemble des fonctions de $E$ telles que $\int_I |f(x)| dx$ soit convergente. 
	On note $\mathscr{L}^2(I)$ l'ensemble des fonctions de $E$ telles que $\int_I |f(x)|^2 dx$ soit convergente.
	
	\begin{enumerate}
		\item Montrer que $\mathscr{L}^1(I)$ est un sous-espace vectoriel de $E$. 
		Pour $f\in \mathscr{L}^1(I)$, on pose $\|f\|_1=\int_I|f(x)|dx$. 
		Montrer que $\| \hskip 1.2mm \|_1$ est une norme sur $\mathscr{L}^1(I)$.
		
		\item On suppose que $\|f_n-f\|_1\rightarrow 0$. 
		Est-ce que $f_n$ converge simplement vers $f$ ?
		
		\item On suppose que $(f_n)_{n \in \NN}$ est une suite de $E$ qui converge simplement vers $f$. 
		Est-ce que $(f_n)_{n \in \NN}$ tend vers $f$ pour la norme $\| \hskip 1.2mm \|_1$ ?
		
		\item Montrer que si $f,g \in \mathscr{L}^2(I)$ et $\lambda \in \RR$, alors $f+\lambda g \in \mathscr{L}^2(I)$ et $fg\in \mathscr{L}^1(I)$. 
		En déduire que l'application donnée par $(f,g) \mapsto \int_I f(x)g(x)dx$ est un produit scalaire sur $\mathscr{L}^2(I)$. 
		
		\item $(\mathscr{L}^2(I),\| \hskip 1.2mm \|_2)$ est-il un espace de Hilbert ?
		
		\item Calculer 
		\[     \inf_{(a,b)\in\RR^2}\int_0^{+\infty}(x^2-ax-b)^2e^{-2x}dx.     \]
		
		\item Les espaces $\mathscr{L}^1(I)$ et $\mathscr{L}^2(I)$ sont-ils comparables (au sens de l'inclusion) ?
		
		\item Sur $\mathscr{L}^1(I)\cap \mathscr{L}^2(I)$, les normes $\| \hskip 1.2mm \|_1$ et $\| \hskip 1.2mm \|_2$ sont-elles équivalentes ?
	\end{enumerate}
	\begin{preuve}
		\begin{enumerate}
			\item Il s'agit d'un résultat du cours. 
			
			\item Non. 
			Considérer par exemple $I = [0,1]$ et la suite de fonctions suivante. 
			Pour $n \in \NN$, on définit $f_{n} : [0,1] \rightarrow \RR$ par 
			\[     f_{n}(x) = \begin{cases} 
			2^{n+1}\big(x-(2^{n}-1)/2^{n+1}\big), &\text{si $x \in [(2^{n}-1)/2^{n+1}, 1/2]$},
			\\
			-2^{n+1}\big(x-(2^{n}+1)/2^{n+1}\big), &\text{si $x \in [1/2, (2^{n}+1)/2^{n+1}]$},
			\\
			0, &\text{si $x \in [0,(2^{n}-1)/2^{n+1}] \cup [(2^{n}+1)/2^{n+1}, 1]$}.
			\end{cases}
			\]
			C'est clair que $f_{n}$ est continue et $\int_0^1 f_n(x) dx = 1/2^{n+1}$, pour tout $n \in \NN$, et $(\| f_n \|_{1})_{n \in \NN}$
			converge vers zéro, mais $(f_n)_{n \in \NN}$ ne converge pas vers la fonction nulle, puisque $f_{n}(1/2) = 1$, pour tout $n \in \NN$. 
			
			\item Non. 
			La suite de fonctions dans l'exercice \ref{exo:14} est un contre-exemple. 
			
			\item C'est clair que si $f \in \mathscr{L}^2(I)$ et $\lambda \in \RR$, alors $\lambda f \in \mathscr{L}^2(I)$. 
			Par ailleurs, l'inégalité $((a+b)/2)^{2} \leq (a^{2} + b^{2})/2$, pour tous $a,b \geq 0$ nous dit que $f+g \in \mathscr{L}^2(I)$, si 
			$f, g \in \mathscr{L}^2(I)$. 
			Par ailleurs, $ab \leq (a^{2} + b^{2})/2$, pour tous $a,b \geq 0$, nous dit que $fg \in \mathscr{L}^{1}(I)$, si 
			$f, g \in \mathscr{L}^2(I)$. 
			Le fait que $(f,g) \mapsto \int_I f(x)g(x)dx$ est une forme bilinéaire symétrique sur $\mathscr{L}^2(I)$ est trivial. 
			En plus, si $\| f \|_{2} = 0$, alors la restriction $f|_{I^{\circ}}$ est la fonction nulle. 
			
			\item L'espace $(\mathscr{L}^2(I),\| \hskip 1.2mm \|_2)$ n'est pas complet. 
			En effet, la suite $(S_{n,g})_{n \in \NN^{*}}$ définie par
			\[     S_{n,g} = \sum_{k=1}^{n} \frac{\sin(kx)}{k}     \]
			dans l'exercice \textbf{7} de la fiche $4$ donne une suite de Cauchy dans l'espace $\mathscr{L}^2([-\pi,\pi])$. 
			Par contre, la limite de  $(S_{n,g})$ dans $(\mathscr{L}^2([-\pi,\pi]),\| \hskip 1.2mm \|_2)$ est la fonction non continue 
			$g$ de l'exercice mentionné. 
			
			\item On voit bien que 
			\begin{equation}
			\label{eq:inf}
			\begin{split}
			&\inf_{(a,b)\in\RR^2}\int_0^{+\infty}(x^2-ax-b)^2e^{-2x}dx = \bigg[ \frac{e^{-2x}}{4} \bigg( (-a^{2}+2b) (2 x^{2}+2x+1) 
			\\
			&+ 2 b^{2} + a (4 x^{3} + 6 x^{2} + 6 x + 3) -2 a b (2 x + 1) -2x^{4} - 4 x^{3} - 6 x^{2} - 6 x - 3 \bigg) \bigg]_{0}^{+\infty} 
			\\
			&= \frac{a^{2} + 2 b^{2} + 2 ab - 3 a - 2 b + 3}{4} 
			\end{split}
			\end{equation}        
			Le seul point critique de cette fonction est $(a,b) = (2,-1/2)$, qui est un minimum local strict (d'après le critère de la matrice hessienne). 
			Comme la fonction \eqref{eq:inf} est coercive, ce qui suit directement de la dernière expression, le minimum local est aussi un minimum global. 
			
			\item Si $I$ est une intervalle fini, $\mathscr{L}^2(I) \subseteq \mathscr{L}^{1}(I)$. 
			En effet, comme la fonction constante $\mathbb{1}_{I} \in \mathscr{L}^2(I)$, si $f \in \mathscr{L}^2(I)$, l'item (d) implique que 
			$f = f.\mathbb{1}_{I} \in \mathscr{L}^{1}(I)$. 
			En plus, cela nous dit aussi que $\| f \|_{1} \leq \| \mathbb{1}_{I} \|_{2} \| f \|_{2}$, pour tout $f \in \mathscr{L}^2(I)$. 
			Par contre, l'inclusion $\mathscr{L}^{1}(I) \subseteq \mathscr{L}^{2}(I)$ n'est pas vérifiée en général. 
			Par exemple, si $I= \hskip 0.6mm ] \hskip 0.6mm 0 ,1]$, pour la fonction $f : I \rightarrow \RR$ donnée par $t \mapsto 1/\sqrt{t}$ 
			on voit que $f \notin \mathscr{L}^2(I)$ mais $f \in \mathscr{L}^{1}(I)$. 
			En outre, si $I$ est un intervalle infini, l'inclusion $\mathscr{L}^2(I) \subseteq \mathscr{L}^{1}(I)$ n'est pas vraie en général. 
			Par exemple, si $I = \RR_{> 1}$, on voit que la fonction $f : I \rightarrow \RR$ donnée par $t \mapsto 1/t^{3/4}$ 
			satisfait que $f \in \mathscr{L}^2(I)$ mais $f \notin \mathscr{L}^{1}(I)$. 
			
			\item Non. 
			Considérer par exemple $I = [0,1]$ et la suite de fonctions suivante. 
			Pour $n \in \NN$, on pose $f_{n} : I \rightarrow \RR$ via $f_{n}(x) = (n+1) x^{n}$. 
			C'est clair que $f_{n} \in \mathscr{L}^1(I)\cap \mathscr{L}^2(I)$, pour tout $n \in \NN$. 
			Alors, $\| f_{n} \|_{1} = 1$, mais $\| f_{n} \|_{2} = (n+1)/\sqrt{2n+1}$, pour tout $n \in \NN$. 
		\end{enumerate}
	\end{preuve}


\newpage \section{Séries de Fourier}


	
	%%%%%%%%%%%%%%%
	\exo{La série de Fourier} Soit $\mathbb{E}$ l'espace vectoriel des fonctions $f : \RR \rightarrow \C$ continues par morceaux, périodiques de période $2 \pi$. 
	On définit l'application $\langle \hskip 0.6mm ,  \rangle : \mathbb{E} \times \mathbb{E} \rightarrow \C$ par 
	\[     \langle f , g \rangle = \int_{-\pi}^{\pi}f (x) \overline{g(x)} dx.    \]
	\begin{enumerate}
		\item Montrer que $\langle \hskip 0.6mm , \rangle$ est une forme hermitienne positive sur $\mathbb{E}$. 
		Est-elle définie positive? 
		
		\item Soit $\{ \chi_{n} \}_{n \in \Z} \in \mathbb{E}^{\Z}$ la collection de fonctions donnée par $\chi_{n}(x) = e^{i nx}$. 
		Montrer que 
		\[     \langle \chi_{n} , \chi_{m} \rangle = 2 \pi \delta_{n,m},     \]   
		pour tous $n, m \in \Z$, où $\delta_{n,m} = 1$ si $n=m$ et $0$ si $n \neq m$. 
		Pour $n \in \Z$, on définit les \emph{coefficients de Fourier exponentiels}
		\begin{equation}
		\tag{FE}
		\label{eq:f1}
		c_{n}(f) = \frac{\langle f , \chi_{n} \rangle}{\langle \chi_{n} , \chi_{n} \rangle} = \frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) e^{-inx} dx.
		\end{equation}
		On écrira $c_{n}$ si la fonction $f$ est sous-entendue. 
		
		\item  Soit $\{ \varphi_{n} \}_{n \in \Z} \in \mathbb{E}^{\Z}$ la collection de fonctions donnée par 
		$\varphi_{n}(x) = \cos(nx)$ si $n \in \NN^{*}$, $\varphi_{0}(x) = 1$ et 
		$\varphi_{-n}(x) = \psi_{n}(x) = \sin(nx)$ si $n \in \NN^{*}$. 
		Montrer que 
		\[     \langle \varphi_{0} , \varphi_{0} \rangle = 2 \langle \varphi_{n} ,\varphi_{n} \rangle = 2 \pi \text{ et }  \langle \varphi_{m} , \varphi_{m'} \rangle = 0,     \]   
		pour tout $n \in \Z^{*}$ et tous $m, m' \in \Z$ tels que $m \neq m'$. 
		On définit les \emph{coefficients de Fourier trigonométriques}
		\begin{equation}
		\tag{FT}
		\label{eq:f2}
		\begin{split}
		a_{n}(f) &= \frac{\langle f , \varphi_{n} \rangle}{\langle \varphi_{n} , \varphi_{n} \rangle} 
		= \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos(nx) dx, 
		\\
		a_{0}(f) &= \frac{\langle f , \varphi_{0} \rangle}{\langle \varphi_{0} , \varphi_{0} \rangle} 
		= \frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) dx, 
		\\ b_{n}(f) &= \frac{\langle f , \varphi_{-n} \rangle}{\langle \varphi_{-n} , \varphi_{-n} \rangle} 
		= \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin(nx) dx,  
		\end{split}
		\end{equation}
		pour tout $n \in \NN^{*}$.
		On écrira plus simplement $a_{n}$ ou $b_{n}$ si la fonction $f$ est sous-entendue. 
		
		\item Montrer que $\varphi_{0} = \chi_{0}$, 
		$\chi_{n} + \chi_{-n} = 2 \varphi_{n}$, $\chi_{n} - \chi_{-n} = 2 i \varphi_{-n}$ et $\varphi_{n} + i \varphi_{-n} = \chi_{n}$, 
		pour tout $n \in \NN^{*}$. 
		En déduire une relation entre \eqref{eq:f1} et \eqref{eq:f2}.          
		En particulier, montrer que 
		\begin{equation}
		\label{eq:sfourier}
		\tag{F}
		S_{f,n}(x) = a_{0}(f) + \sum_{k=1}^{n} \big(a_{k}(f) \cos(k x) + b_{k}(f) \sin(k x) \big) = \sum_{k=-n}^{n} c_{k}(f) e^{i k x},   
		\end{equation}
		pour tout $n \in \NN$ et $x \in \RR$. 
		On appelle \eqref{eq:sfourier} la \emph{série de Fourier partielle d'ordre $n$}. 
		
		\item Montrer que si $f|_{]-\pi,\pi [}$ est une fonction paire (resp., impaire), alors $b_{n}(f) = 0$, pour tout $n \in \NN^{*}$ (resp., 
		$a_{n}(f) = 0$, pour tout $n \in \NN$).
	\end{enumerate}                          
	
	\begin{preuve}
		Il s'agit des résultats du cours, même si la solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Les bases de Fourier sont totales}
	\label{exercice:2}
	\begin{enumerate}
		\item Pour $f, g \in \mathbb{E}$, on définit $f * g : \RR \rightarrow \C$ par 
		\[     (f * g)(x) = \int_{-\pi}^{\pi} f(t) g(x-t) dt.     \]
		Montrer que  $f * g \in \mathbb{E}$. 
		
		\item Pour tout $n \in \NN$, on considère la fonction continue et périodique de période $2 \pi$ donnée par 
		\[     D_{n} = \frac{1}{2 \pi} \sum_{k=-m}^{m} \chi_{k}.     \]
		\begin{enumerate}[label=(\roman*)]
			\item Montrer que $D_{n} * f = S_{f,n}$, pour tout $n \in \NN$. 
			\item Montrer que 
			\[     D_{n}(x) = \frac{1}{2 \pi} \frac{\sin\big((2n+1)x/2\big)}{\sin(x/2)},     \]
			pour tout $x \in \RR \setminus 2\pi \Z$ et $n \in \NN$. 
			\item Montrer que 
			\[     \int_{-\pi}^{\pi} D_{n}(x) dx = 1,     \]
			pour tout $n \in \NN$. 
		\end{enumerate}
		
		\item Pour tout $n \in \NN$, on considère la fonction continue et périodique de période $2 \pi$ donnée par 
		\[     K_{n} = \frac{1}{2 \pi n} \sum_{m=0}^{n-1} \sum_{k=-m}^{m} \chi_{k}.     \]
		\begin{enumerate}[label=(\roman*)]
			\item Montrer que 
			\[     K_{n} * f = \frac{\sum_{m=0}^{n-1} S_{f,m}}{n}.     \]
			\item Montrer que 
			\[     K_{n}(x) = \frac{1}{2 \pi n} \frac{\sin^{2}(nx/2)}{\sin^{2}(x/2)},     \]
			pour tout $x \in \RR \setminus 2\pi \Z$ et $n \in \NN$. 
			En déduire que $K_{n}(x) \geq 0$, pour tout $x \in \RR$. 
			\item Montrer que 
			\[     \int_{-\pi}^{\pi} K_{n}(x) dx = 1,     \]
			pour tout $n \in \NN$. 
			\item Montrer que, étant donnés $\epsilon >0$ et $0< \delta < \pi$, il existe $N \in \NN$ tel que 
			\[     \int_{-\pi}^{-\delta} K_{n}(x) dx + \int^{\pi}_{\delta} K_{n}(x) dx < \epsilon,     \]
			pour tout $n \geq N$. 
		\end{enumerate}
		
		\item Soit $f \in \mathbb{E}$ et soit $X \subseteq \RR$ une partie compacte telle que $f|_{X}$ soit continue. 
		À partir des items précédents, conclure que $K_{n} * f$ converge uniformément vers $f$ sur $X$. 
		En déduire que l'espace vectoriel engendré par $\{ \chi_{n} \}_{n \in \Z} \in \mathbb{E}^{\Z}$ (ou par $\{ \varphi_{n} \}_{n \in \Z} \in \mathbb{E}^{\Z}$) est dense dans $\mathbb{E}$ muni de la semi-norme $|| \hskip 1.2mm ||_{2}$ associée 
		à $\langle \hskip 0.6mm , \rangle$. 
		
		\item Utiliser l'item précédent et le théorème de Bessel pour montrer que, si $f \in \mathbb{E}$ satisfait que $c_{n}(f) = 0$ pour tout $n \in \Z$, alors il existe $F \subseteq \hskip 0.8mm]\hskip 0.8mm-\pi,\pi\hskip 0.8mm[\hskip 0.8mm$ fini tel que $f|_{\hskip 0.8mm]\hskip 0.8mm-\pi,\pi\hskip 0.8mm [\hskip 0.8mm\setminus F}$ est la fonction nulle. 
	\end{enumerate} 
	
	\begin{preuve}
		Il s'agit des résultats du cours, même si la solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Pour chaque item ci-dessous, on considérera la fonction $f \in \mathbb{E}$ dont la restriction de $f$ à $[-\pi,\pi \hskip 0.6mm [ \hskip 0.6mm$ 
	coïncide avec l'expression indiquée: 
	\\
	\begin{enumerate*}[label=(\alph*)]
		\item $x$, 
		\item $x^{2}$,
		\item $|x|$, 
		\item $\sin^{2}(x)$, 
		\item $|\sin(x)|$.
		%\item $|\cos(x)|$. 
	\end{enumerate*}
	\\ 
	Calculer les coefficients de Fourier trigonométriques respectifs.
	
	\begin{preuve}
		\begin{enumerate} 
			\item Comme $x$ est une fonction impaire, alors $a_{n} = 0$, pour tout $n \in \NN$. 
			En outre, c'est facile à calculer
			\[     b_{n} = \frac{1}{\pi} \int_{-\pi}^{\pi} x \sin(n x) dx = \bigg[ \frac{\sin(n x) - n x \cos(n x)}{\pi n^{2}} \bigg]_{-\pi}^{\pi} = \frac{2 (-1)^{n+1}}{n},     \]
			pour tout $n \in \NN^{*}$.
			
			\item Comme $x^{2}$ est une fonction paire, alors $b_{n} = 0$, pour tout $n \in \NN^{*}$. 
			En outre, c'est facile à calculer
			\[     a_{0} = \frac{1}{2\pi} \int_{-\pi}^{\pi} x^{2} dx = \bigg[ \frac{x^{3}}{6 \pi} \bigg]_{-\pi}^{\pi} = \frac{\pi^{2}}{3}     \]
			et
			\[     a_{n} = \frac{1}{\pi} \int_{-\pi}^{\pi} x \cos(n x) dx = \bigg[ \frac{(n^{2}x^{2}-2) \sin(n x) + 2 n x \cos(n x)}{\pi n^{3}} \bigg]_{-\pi}^{\pi} = \frac{4 (-1)^{n}}{n^{2}},     \]
			pour tout $n \in \NN^{*}$.
			
			\item Comme $|x|$ est une fonction paire, alors $b_{n} = 0$, pour tout $n \in \NN^{*}$. 
			En outre, c'est facile à calculer
			\[     a_{0} = \frac{1}{2\pi} \int_{-\pi}^{\pi} |x| dx = \frac{1}{\pi} \int_{0}^{\pi} x dx = \bigg[ \frac{x^{2}}{2 \pi} \bigg]_{0}^{\pi} = \frac{\pi}{2}     \]
			et
			\begin{align*}
			a_{n} &= \frac{1}{\pi} \int_{-\pi}^{\pi} |x| \cos(n x) dx = \frac{2}{\pi} \int_{0}^{\pi} x \cos(nx) dx = 2 \bigg[ \frac{n x \sin(n x) + \cos(n x)}{\pi n^{2}} \bigg]_{0}^{\pi} 
			\\
			&= \frac{2 \big((-1)^{n}-1\big)}{\pi n^{2}},
			\end{align*}
			pour tout $n \in \NN^{*}$.
			
			\item Comme $\sin^{2}(x)$ est une fonction paire, alors $b_{n} = 0$, pour tout $n \in \NN^{*}$. 
			En outre, c'est facile à calculer
			\[     a_{0} = \frac{1}{2\pi} \int_{-\pi}^{\pi} \sin^{2}(x) dx = \bigg[ \frac{x}{4 \pi} - \frac{\sin(2x)}{8 \pi} \bigg]_{-\pi}^{\pi} = \frac{1}{2}.     \]
			En plus, 
			\[     a_{2} = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin^{2}(x) \cos(2 x) dx = \bigg[ -\frac{x}{4 \pi} + \frac{4 \sin(2 x) - \sin(4 x)}{16 \pi} \bigg]_{-\pi}^{\pi} = - \frac{1}{2},     \]
			tandis que 
			\[     a_{n} = \frac{1}{\pi} \int_{-\pi}^{\pi} \sin^{2}(x) \cos(n x) dx = \bigg[\frac{\sin(n x)}{2 n \pi} - \frac{\sin\big((n-2) x\big)}{4 (n-2) \pi} - \frac{\sin\big((n+2) x\big)}{4 (n+2) \pi} \bigg]_{-\pi}^{\pi} = 0,     \]
			pour tout $n \in \NN^{*} \setminus \{ 2 \}$.
			
			\item Comme $| \sin(x)|$ est une fonction paire, alors $b_{n} = 0$, pour tout $n \in \NN^{*}$. 
			En outre, c'est facile à calculer
			\[     a_{0} = \frac{1}{2\pi} \int_{-\pi}^{\pi} |\sin(x)| dx = \frac{1}{\pi} \int_{0}^{\pi} \sin(x) dx = \bigg[ - \cos(x) \bigg]_{0}^{\pi} = \frac{2}{\pi}.     \]
			En plus, 
			\[     a_{1} = \frac{1}{\pi} \int_{-\pi}^{\pi} |\sin(x)| \cos(x) dx = \frac{2}{\pi} \int_{0}^{\pi} \sin(x) \cos(x) dx 
			= \bigg[\frac{\sin(2 x)}{\pi} \bigg]_{0}^{\pi} = 0,     \]
			tandis que
			\begin{align*}
			a_{n} &= \frac{1}{\pi} \int_{-\pi}^{\pi} |\sin(x)| \cos(n x) dx = \frac{2}{\pi} \int_{0}^{\pi} \sin(x) \cos(n x) dx 
			\\
			&= 2 \bigg[\frac{n \sin(x) \sin(n x) + \cos(x) \cos(n x)}{\pi(n^{2}-1)} \bigg]_{0}^{\pi} 
			= 2 \frac{1+(-1)^{n}}{\pi(1-n^{2})},     
			\end{align*}
			pour tout $n \in \NN^{*} \setminus \{ 1 \}$.
			
			%\item Comme $| \cos(x)|$ est une fonction paire, alors $b_{n} = 0$, pour tout $n \in \NN^{*}$. 
			%En outre, c'est facile à calculer
			%\[     a_{0} = \frac{1}{2\pi} \int_{-\pi}^{\pi} |\cos(x)| dx = \frac{1}{\pi} \int_{0}^{\pi} \cos(x) dx = \bigg[ \sin(x) \bigg]_{0}^{\pi} = 0.     \]
			%En plus, 
			%\[     a_{1} = \frac{1}{\pi} \int_{-\pi}^{\pi} |\cos(x)| \cos(x) dx = \frac{2}{\pi} \int_{0}^{\pi} \cos^{2}(x) dx 
			%= \bigg[\frac{x}{\pi} + \frac{\sin(2 x)}{2\pi} \bigg]_{0}^{\pi} = 1,     \]
			%tandis que
			%\begin{align*}
			%     a_{n} &= \frac{1}{\pi} \int_{-\pi}^{\pi} |\cos(x)| \cos(n x) dx = \frac{2}{\pi} \int_{0}^{\pi} \cos(x) \cos(n x) dx 
			%\\
			%&= \bigg[\frac{n \cos(x) \sin(n x) - \sin(x) \cos(n x)}{\pi(n^{2}-1)} \bigg]_{0}^{\pi} 
			%= 0,     
			%\end{align*}
			%pour tout $n \in \NN^{*} \setminus \{ 1 \}$.
		\end{enumerate} 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}
	\begin{enumerate} 
		\item Soit $f \in \mathbb{E}$ de classe $C^{1}$. 
		Montrer qu'il existe une constante $C > 0$ telle que $|c_{n}(f)| \leq C/|n|$, pour tout $n \in \Z^{*}$. 
		\item Soit $f \in \mathbb{E}$ de classe $C^{2}$. 
		Montrer qu'il existe une constante $C > 0$ telle que $|c_{n}(f)| \leq C/n^{2}$, pour tout $n \in \Z^{*}$. 
		\item Soit $f \in \mathbb{E}$ telle que la série de Fourier $S_{f,n}$ converge uniformément. 
		Utiliser l'exercice \ref{exercice:2} pour montrer que la limite uniforme de $S_{f,n}$ est $f$. 
		\item Soit $f \in \mathbb{E}$ qui satisfait qu'il existe $C > 0$ telle que $|c_{n}(f)| \leq C/n^{2}$, pour tout $n \in \Z^{*}$. 
		Montrer que la série de Fourier de $f$ converge uniformément vers $f$. 
		En déduire que la série de Fourier de $f$ converge uniformément vers $f$ si $f$ est de classe $C^{2}$. 
	\end{enumerate} 
	
	\begin{preuve}
		\begin{enumerate} 
			\item Soit $n \in \Z^{*}$. 
			On voit que 
			\begin{align*} 
			c_{n}(f) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) e^{-inx} dx = \bigg[ \frac{f(x)e^{-inx}}{-in} \bigg]_{-\pi}^{\pi} + \frac{1}{2 n i \pi} \int_{-\pi}^{\pi} f'(x) e^{-inx} dx 
			\\
			&= \frac{1}{2 n i \pi} \int_{-\pi}^{\pi} f'(x) e^{-inx} dx,     
			\end{align*}
			où l'on a fait une intégration par parties et on a utilisé que $f$ est continue. 
			Soit $C > 0$ telle que $|f'(x)| < C$, pour tout $x \in [-\pi , \pi]$. 
			Alors, $|c_{n}(f)| \leq C/|n|$, comme on voulait démontrer. 
			\item Il s'agit du même type d'argument que celui dans l'item précédent, mais il faut appliquer deux fois la méthode d'intégration par parties.  
			\item Soit $f \in \mathbb{E}$ telle que la série de Fourier $S_{f,n}$ converge uniformément et soit $S_{f}$ la limite uniforme. 
			En particulier, $S_{f}$ est une fonction continue. 
			Pour tout $m \in \Z$ on voit bien que 
			\begin{align*}   
			c_{m}(S_{f}) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} S_{f}(x) e^{-imx} dx = \underset{n \rightarrow + \infty} {\lim} \frac{1}{2\pi} \int_{-\pi}^{\pi} S_{f,m}(x) e^{-imx} dx 
			\\  
			&= \underset{n \rightarrow + \infty} {\lim} \sum_{p=-n}^{n} \frac{1}{2\pi} \int_{-\pi}^{\pi} c_{p}(f) e^{i(p-m)x} dx 
			= \underset{n \rightarrow + \infty} {\lim} c_{m}(f) = c_{m}(f).
			\end{align*}
			D'après le dernier item de l'exercice \ref{exercice:2}, on voit que, comme $f$ et $S_{f}$ ont la même série de Fourier, alors 
			$f = S_{f}$.
			
			\item On suppose qu'il existe $C > 0$ telle que $|c_{n}(f)| \leq C/n^{2}$, pour tout $n \in \Z^{*}$. 
			Alors 
			\[     \bigg| \sum_{m=-n}^{n} c_{m}(f) e^{imx} \bigg| \leq \sum_{m=-n}^{n} |c_{m}(f)| |e^{imx}| \leq |c_{0}(f)| + 2 C \sum_{m=1}^{n} \frac{1}{n^{2}}.     \]
			D'après le critère de Weierstrass la série de Fourier converge absolument et uniformément. 
			L'item précédent nous dit que la limite uniforme de la série est $f$. 
			D'après le troisième item de cet exercice, on conclut que la série de Fourier de $f$ converge uniformément vers $f$ si $f$ est de classe $C^{2}$. 
		\end{enumerate} 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Lemme de Riemann-Lebesgue}
	Soit $f:[a,b]\rightarrow\RR$ une fonction intégrable et soit $\lambda\in\RR$. 
	On pose 
	\[     I(\lambda)=\int_a^bf(x)e^{i\lambda x}dx.     \]
	\begin{enumerate}
		\item On suppose maintenant que $f$ est de classe $C^{1}$. 
		Montrer que 
		\begin{equation}
		\label{eq:rlp}
		\int_{a}^{b} f(x) e^{i \lambda x} dx = \frac{f(b) e^{i \lambda b}}{i \lambda} - \frac{f(a) e^{i \lambda a}}{i \lambda} - \frac{1}{i\lambda} \int_{a}^{b} f'(x) e^{i \lambda x} dx.     
		\end{equation}
		En déduire que $\lambda \mapsto \lambda I(\lambda)$ est bornée et, en particulier, $I(\lambda)$ tend vers $0$ quand $\lambda$ tend vers $+\infty$. 
		En déduire les mêmes résultats pour $f$ de classe $C^{1}$ par morceaux. 
		
		\item En déduire le \emph{Lemme de Riemann-Lebesgue}: pour toute fonction $f$ intégrable, 
		\begin{equation}
		\tag{RL}
		\label{eq:rl}
		\underset{\lambda \rightarrow +\infty}{\lim} I(\lambda) = \underset{\lambda \rightarrow +\infty}{\lim} \int_a^bf(x)e^{i\lambda x}dx = 0. 
		\end{equation}
		
		\item On suppose $f$ décroissante et positive, montrer que $\lambda \mapsto \lambda I(\lambda)$ est bornée.
		
		\item Soit $F : \RR \rightarrow \C$ un fonction périodique de période $\ell >0$ telle que $F|_{[0,\ell]}$ soit intégrable et 
		soit $f:[a,b]\rightarrow\C$ une fonction de classe $C^{1}$. 
		Montrer que pour tout $\epsilon > 0$ il existe $C > 0$ tel que
		\begin{equation}
		\tag{RL-U}
		\label{eq:rlu}
		\bigg|\int_a^b F(x+t) f(t) e^{i\lambda t}dt \bigg| < \epsilon,
		\end{equation}         
		pour tout $\lambda > C$ et tout $x \in \RR$. 
		\\
		\textbf{Indication :} réduire d'abord au cas où $F$ satisfait que $F|_{[0,\ell]}$ est en escalier à partir de prendre une 
		approximation uniforme de $F$, puis démontrer le cas des fonctions en escalier à partir de \eqref{eq:rlp}.
	\end{enumerate}
	
	\begin{preuve}
		Il s'agit des résultats du cours, même si la solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Convergence uniforme locale} Soit $f \in \mathbb{E}$. 
	\begin{enumerate}
		\item Montrer que les limites
		\[     \underset{h \rightarrow 0+}{\lim} f(x+h) \text{ et } \underset{h \rightarrow 0-}{\lim} f(x+h)     \]
		existent. 
		On les note $f(x+)$ et $f(x-)$, respectivement. 
		On pose $\operatorname{Av}_{f} : \RR \rightarrow \C$ via $\operatorname{Av}_{f}(x) = (f(x+)+ f(x-))/2$. 
		Montrer que $\operatorname{Av}_{f} \in \mathbb{E}$. 
		
		\item Soit $X \subseteq \RR$. 
		On dit que $f$ satisfait la \emph{condition de Lipschitz à gauche (resp., à droite)} sur $X$ s'il existe $C > 0$ 
		et $\delta > 0$ tels que $|f(x-h) - f(x-)| \leq C h$ (resp., $|f(x+h) - f(x+)| \leq C h$), pour tous $x \in X$ et $h \in \hskip 0.6mm ] \hskip 0.6mm 0, \delta]$. 
		Montrer que si $X$ est un point $x$ et $f \in \mathbb{E}$ est différentiable à gauche (resp., à droite) en $x$, alors elle satisfait la condition de Lipschitz à gauche (resp., à droite) sur $X$. 
		
		\item Utiliser que $D_{n}$ est paire pour montrer que 
		\[     \int_{-\delta}^{\delta} \big(f(x-t) - \operatorname{Av}_{f}(x)\big) D_{n}(t) dt = \int_{-\delta}^{\delta}  \bigg(\frac{f(x-t)+f(x+t)}{2} - \operatorname{Av}_{f}(x)\bigg) D_{n}(t) dt     \]
		et en déduire que, si $f$ satisfait la condition de Lipschitz (à gauche et à droite) sur $X$, alors il existe $\delta, C, C' > 0$ tels que
		\[     \bigg|\int_{-\delta}^{\delta} \big(f(x-t) - \operatorname{Av}_{f}(x)\big) D_{n}(t) dt\bigg| 
		\leq C \int_{-\delta}^{\delta} \frac{|t/2|}{|\sin(t/2)|} dt \leq \delta C',     \]
		pour tout $x \in X$. 
		
		\item On continue avec les hypothèses de l'item précédent. 
		%Si $|X|$ est fini ou $f|_{X}$ est continue, 
		Montrer que le lemme de Riemann-Lebesgue uniforme \eqref{eq:rlu} implique que 
		\[     \int_{-\pi}^{-\delta} \big(f(x-t) - \operatorname{Av}_{f}(x)\big) D_{n}(t) dt \text{ et } \int^{\pi}_{\delta} \big(f(x-t) - \operatorname{Av}_{f}(x)\big) D_{n}(t) dt     \]
		convergent vers $0$ quand $n$ tend vers $+ \infty$ uniformément pour $x \in X$. 
		
		\item En déduire que si $f \in \mathbb{E}$ satisfait la condition de Lipschitz (à gauche et à droite) sur $X$%et $|X|$ est fini ou $f|_{X}$ est continue
		, alors $S_{n,f} = D_{n} * f$ converge uniformément vers $\operatorname{Av}_{f}$ sur $X$. 
		
		\item En déduire le \emph{principe de localisation de Riemann}: si $f \in \mathbb{E}$ satisfait que $f|_{[a,b]}$ est nulle, alors 
		$S_{f,n}$ converge uniformément vers $0$ sur tout sous-intervalle fermé de $\hskip 0.6mm ] \hskip 0.6mm a,b \hskip 0.6mm [ \hskip 0.6mm$. 
	\end{enumerate}
	
	\begin{preuve}
		Il s'agit des résultats du cours, même si la solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{}\label{exercice:7} Soient $f, g \in \mathbb{E}$ les fonctions qui satisfont que $f|_{[0,2\pi \hskip 0.6mm [ \hskip 0.6mm}$ coïncide avec $(\pi-x)^{2}/4$ et 
	$g|_{[0,2\pi \hskip 0.6mm [ \hskip 0.6mm}$ coïncide avec $(\pi-x)/2$
	\begin{enumerate}
		\item Montrer que les séries de Fourier trigonométriques de $f$ et de $g$ sont  
		\[     \frac{\pi^{2}}{12} + \sum_{k=1}^{\infty} \frac{\cos(kx)}{k^{2}} \text{ et } \sum_{k=1}^{\infty} \frac{\sin(kx)}{k},     \]
		respectivement.
		\item Montrer que la série de Fourier trigonométrique de $f$ converge uniformément vers $f$. 
		\item Montrer que l'on peut différentier la série de Fourier trigonométrique de $f$ terme à terme sur tout intervalle de la forme 
		$[\delta,2 \pi - \delta]$, avec $0 < \delta < \pi$, et 
		\[     \frac{\pi-x}{2} = \sum_{k=1}^{\infty} \frac{\sin(kx)}{k},     \]
		pour tout $x \in \hskip 0.6mm ] \hskip 0.6mm 0,2\pi \hskip 0.6mm [ \hskip 0.6mm$. 
	\end{enumerate}
	
	\begin{preuve}
		La solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soit $f \in \mathbb{E}$ la fonction qui satisfait que $f|_{[0,2\pi \hskip 0.6mm [ \hskip 0.6mm}$ coïncide avec $e^{a x}$, où $a \in \C \setminus i \Z$.  
	\begin{enumerate} 
		\item Calculer la série de Fourier exponentielle de $f$. 
		\item Soit $x \in \hskip 0.6mm ] \hskip 0.6mm 0,2\pi \hskip 0.6mm [ \hskip 0.6mm$. 
		Montrer que 
		\[     \pi e^{a x} = (e^{2a\pi} - 1) \bigg( \frac{1}{2a} + \sum_{k=1}^{+\infty} \frac{a \cos(k x) - k \sin(k x)}{k^{2}+a^{2}} \bigg).     \]
		\item Soit $x \in \hskip 0.6mm ] \hskip 0.6mm 0,2\pi \hskip 0.6mm [ \hskip 0.6mm$ et $b \in \RR \setminus \Z$. 
		Montrer que 
		\[     \pi \cos(b x) = \frac{\sin(2b \pi)}{2b} + \sum_{k=1}^{+\infty} \frac{b \sin(2 b \pi) \cos(k x) + k \big(\cos(2 b \pi) -1\big) \sin(k x)}{b^{2}-k^{2}}.     \]
		En déduire que 
		\[     \frac{b \pi}{\sin(b \pi)} = 1 + 2 b^{2} \sum_{k=1}^{+\infty} \frac{(-1)^{k}}{b^{2}-k^{2}}.     \]
	\end{enumerate}
	
	\begin{preuve}
		La solution suit directement des indications détaillées dans l'exercice. 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{Le phénomène de Gibbs} 
	\begin{enumerate} 
		\item Soit $g \in \mathbb{E}$ la fonction de l'exercice \ref{exercice:7} et soit $\Delta_{n} : \hskip 0.6mm ] \hskip 0.6mm 0,\pi \hskip 0.6mm [ \hskip 0.6mm \rightarrow \RR$ la fonction donnée par $\Delta_{n}(x) = S_{g,n}(x) - g(x)$, 
		pour tout $n \in \NN$. 
		Montrer que $S_{g,n}(0)$ converge vers $\operatorname{Av}_{g}(0) = 0$ quand $n$ tend vers $+ \infty$. 
		
		\item Montrer que $\Delta_{n}$ est différentiable et 
		\[     \Delta_{n}'(x) = \frac{1}{2} + \sum_{k=1}^{n} \cos(k x) = \frac{\sin\big((2n+1) x/2\big)}{2 \sin(x/2)},     \] 
		pour tout $x \in \hskip 0.6mm ] \hskip 0.6mm 0,\pi \hskip 0.6mm [ \hskip 0.6mm$. 
		Conclure que les points critiques de $\Delta_{n}$ sont de la forme $x_{n,j} = j \pi/(n+1/2)$, pour $j \in \{ 1, \dots, n\}$. 
		%\item Montrer que $x_{n} = \pi/(n+1/2)$, pour $n \in \NN$, est un maximum de $\Delta_{n}$. 
		Noter que $x_{n,j}$ converge vers $0+$ quand $n$ tend vers $+ \infty$. 
		
		\item Montrer que 
		\[     \underset{n \rightarrow + \infty}{\lim}S_{g,n}(x_{n,j}) = \int_{0}^{j\pi} \frac{\sin(x)}{x} dx.     \]
		En déduire que 
		\[     b_{j} = \underset{n \rightarrow + \infty}{\lim}\Delta_{n}(x_{n,j}) = \int_{0}^{j \pi} \frac{\sin(x)}{x} dx - \frac{\pi}{2}.     \]
		\\
		\textbf{Indication :} Considérer une somme de Riemann de $\sin(x)/x$ sur l'intervalle $[0,j\pi]$ 
		associée à la subdivision $\{ k j \pi/(n+1/2) : k \in \{ 1, \dots, n\} \}$. 
		
		\item Montrer que la suite $(b_{j})_{j  \in \NN}$ est alternée et que $( |b_{j}| )_{j \in \NN}$ est strictement décroissante (de limite $0$). 
		En conséquence, la valeur $b_{1} \simeq 0,281$ est maximale.
		
		\item Soit $f \in \mathbb{E}$ de classe $C^{1}$ par morceaux et soit $\{ y_{1}, \dots, y_{n} \} \subseteq [0,2\pi \hskip 0.6mm [ \hskip 0.6mm$ l'ensemble de discontinuités de saut de $f|_{[0,2\pi \hskip 0.6mm [ \hskip 0.6mm}$. 
		On pose $d_{i} = (f(y_{i}+) - f(y_{i}-))/\pi$ et $\tilde{f}(x) = f(x) - \sum_{i=1}^{n} d_{i} g(x-y_{i})$, pour $x \in \RR$. 
		Noter que, comme $\tilde{f}$ est de classe $C^{1}$ par morceaux et continue, $S_{\tilde{f},n}$ converge uniformément vers 
		$\tilde{f}$. 
		Utiliser le principe de localisation de Riemann ainsi que les items précédents pour montrer qu'il existe une suite $(x_{n,i})_{n \in \NN}$ qui converge vers $y_{i}+$ telle que la limite de $|S_{f,n}(x_{n,i}) - f(y_{i}+)|$ quand $n$ tend vers $+ \infty$ est $b_{1} d_{i}$ pour tout $i \in \{ 1, \dots, n\}$. 
		%\footnote{Il s'agit d'un fait remarquable que, pour toute fonction $f \in \mathbb{E}$ de classe $C^{1}$ par morceaux avec une %discontinuité de saut en $x_{0}$, il existe une suite $(x_{n})_{n \in \NN}$ qui converge vers $x_{0}+$ (resp., $x_{0}-$) telle que 
		%la limite de $|S_{f,n}(x_{n}) - f(x_{0}+)|$ (resp., $|S_{f,n}(x_{n}) - f(x_{0}-)|$) soit égale au produit de $|f(x_{0}+) - f(x_{0}-)|$ et $b_{1}/\pi \simeq 0,0893$.} 
	\end{enumerate}


\begin{preuve}
	La solution suit directement des indications détaillées dans l'exercice. 
\end{preuve}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



On dit que $g$ satisfait le \emph{phénomène de Gibbs à droite (resp., à gauche)} en $x_{0}$ s'il existe une suite $(x_{n})_{n \in \NN} \in \RR^{\NN}$ telle que $x_{n} > x_{0}$ (resp., $x_{n} < x_{0}$) pour tout $n \in \NN$ et 
\[     \hskip - 1.5cm \operatorname{sgn}(d) \big(\underset{n \rightarrow + \infty}{\lim} S_{g,n}(x_{n}) - g(x_{0}+)\big) > 0 \hskip 1mm \Big(\text{resp.,} \operatorname{sgn}(d) \big( \underset{n \rightarrow + \infty}{\lim} S_{g,n}(x_{n}) - g(x_{0}-)\big) < 0 \Big).     \]
Un résultat remarquable de la théorie de Fourier est que toute fonction $g \in \mathbb{E}$ de classe $C^{1}$ par morceaux satisfait le phénomène de Gibbs (à droite et à gauche) en toute discontinuité de saut. 

\newpage \section{Intégrales curvilignes}


	
	%%%%%%%%%%%%%%%
	\exo{} Donner la longueur de la courbe $\alpha : \RR \rightarrow \RR^{2}$ donnée par 
	\[     \alpha(t)=\Big( a\big(2\cos (t)-\cos (2t)\big), a\big(2\sin (t)-\sin (2t)\big) \Big),     \]
	où $a>0$.
	
	\begin{preuve}
		C'est clair que $\alpha$ est périodique de période $2 \pi$, puisque les fonctiones $\sin$ et $\cos$ son périodiques de période $2 \pi$. 
		En outre, on affirme que la la période minimale $p > 0$ de $\alpha$ est $2 \pi$. 
		En effet, si $P$ est une période de $\alpha$, alors $P$ est aussi une période de la fonction $t \mapsto \| \alpha(t) \|^{2}$ définie sur $\RR$. 
		Comme
		\[     \| \alpha (t) \|^{2} = a \big(5 - 4 \cos(t) \big),     \]
		pour tout $t \in \RR$, on voit que $P$ doit être un multiple entier de $2 \pi$. 
		Cela implique que la longueur de la courbe définie par $\alpha$ est 
		\[     \ell = \int_{0}^{2 \pi} \| \alpha'(t) \| dt = \int_{0}^{2 \pi} 2\sqrt{2} a \sqrt{1 - \cos(t)} dt = \int_{0}^{2 \pi} 4 a |\sin(t/2)| dt = 4^{2} a,     \]
		où $\| \phantom{x} \|$ est la norme euclidienne et l'on a utilisé les identités 
		\[     \cos(2t) \cos(t) + \sin(2t) \sin(t) = \cos(t) \text{ et } 1 - \cos(t) = 2 \sin^{2}(t/2).     \] 
	\end{preuve}               
	
	%%%%%%%%%%%%%%%
\exo{} Soient $a, b > 0$. Calculer la longueur de l'arc de la chainette $y=a\cosh (x/a)$ 
	compris entre le sommet $(0,a)$ et le point $(b,h)$, où $h = a \cosh(b/a)$. 
	
	\begin{preuve} 
		La courbe indiquée est donnée par $\alpha(t) = (t , a\cosh (t/a))$. 
		On voit bien que la longueur demandée est 
		\begin{align*}
		\ell &= \int_{0}^{b} \| \alpha'(t) \| dt = \int_{0}^{b} \sqrt{1+\sinh^{2}(t/a)} dt = \int_{0}^{b} \cosh(t/a) dt 
		\\
		&= \bigg[ a \sinh(t/a) \bigg]_{0}^{b} = a \sinh(b/a),     
		\end{align*}
		où $\| \phantom{x} \|$ est la norme euclidienne. 
	\end{preuve}  
	
	%%%%%%%%%%%%%%%
	\exo{} Calculer la longueur de la cardioïde décrite en polaires par $r=a(1+\cos (\theta))$, où $a > 0$.
	
	\begin{preuve} 
		C'est clair que la courbe indiquée $\alpha : \RR \rightarrow \RR^{2}$ est donnée par $\alpha(\theta) = (a(1+\cos (\theta))\cos(\theta) , a(1+\cos (\theta))\sin(\theta))$. 
		C'est clair que $\alpha$ est périodique de période $2 \pi$, puisque les fonctiones $\sin$ et $\cos$ son périodiques de période $2 \pi$. 
		En outre, vu que $(2a,0) = \alpha(0) \neq \alpha(\pi) = (0,0)$, la période minimale de $\alpha$ est $2 \pi$. 
		On voit bien que la longueur demandée est 
		\[    \ell = \int_{0}^{2 \pi} \| \alpha'(t) \| dt = \int_{0}^{2 \pi} \sqrt{2} a \sqrt{1 + \cos(t)} dt = \int_{0}^{2 \pi} 2 a |\cos(t/2)| dt = 8 a,     \]
		où $\| \phantom{x} \|$ est la norme euclidienne et l'on a utilisé les identités 
		\[     \cos(2t) \cos(t) + \sin(2t) \sin(t) = \cos(t) \text{ et } 1 + \cos(t) = 2 \cos^{2}(t/2).     \] 
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} Soient $A$ et $B$ deux points de $\RR^2$ tels que la distance entre $A$ et $B$ est $a > 0$. 
	Sans perte de généralité on considère que $A= (0,0)$ et $B = (a,0)$.
	Soit $\varphi : [0,1] \rightarrow \RR^{2}$ une courbe paramétrée du plan de classe $C^1$ telle que $\varphi(0)=A$ et $\varphi(1)=B$.
	On note $\ell$ la longueur de la courbe paramétrée par $\varphi$.
	\begin{enumerate}
		\item Rappeler la formule permettant de calculer $\ell$.
		\item Montrer que $\ell \geq a$. Est-ce étonnant ?
		\item Montrer que $\ell=a$ si et seulement s'il existe une fonction $x : [0,1] \rightarrow \RR$ monotone telle que $\varphi(t) = (x(t),0)$, pour tout $t \in [0,1]$.
	\end{enumerate}
	
	\begin{preuve} 
		\begin{enumerate}
			\item On rappelle que 
			\[          \ell = \int_{0}^{1} \| \varphi'(t) \| dt,     \]
			où $\| \phantom{x} \|$ est la norme euclidienne.
			
			\item On écrit $\varphi(t) = (x(t), y(t))$. 
			Alors $\| \varphi'(t) \| = \sqrt{x'(t)^{2} + y'(t)^{2} } \geq \sqrt{x'(t)^{2}} = |x'(t)|$. 
			Cela implique que 
			\[          \ell = \int_{0}^{1} \| \varphi'(t) \| dt \geq \int_{0}^{1} |x'(t)| dt \geq \bigg| \int_{0}^{1} x'(t) dt \bigg| = |x(1) - x(0)| = a.     \]
			
			\item On suppose qu'il existe une fonction $x : [0,1] \rightarrow \RR$ monotone telle que $\varphi(t) = (x(t),0)$, pour tout $t \in [0,1]$. 
			Comme $x(1) = a > x(0) = 0$, $x$ est croissante et sa dérivée est donc non négative. 
			Alors, 
			\[          \ell = \int_{0}^{1} \| \varphi'(t) \| dt = \int_{0}^{1} |x'(t)| dt = \int_{0}^{1} x'(t) dt = x(1) - x(0) = a.     \]
			Réciproquement, on suppose que $\ell = a$. 
			Cela implique que 
			\[     \int_{0}^{1} \big(\| \varphi'(t) \| - |x'(t)|\big) dt = 0.     \]
			Comme $t \mapsto \| \varphi'(t) \| - |x'(t)|$ est une fonction continue, non négative et son intégrale est nulle, on conclut qu'elle est la fonction nulle. 
			Cela implique que $y'(t) = 0$, pour tout $t \in \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
			Comme $y(0)=0$, on conclut que $y(t) = 0$, pour tout $t \in [0 , 1]$, \textit{i.e.} $\varphi(t) = (x(t),0)$, pour tout $t \in [0,1]$. 
			En outre, la condition $\ell = a$ implique que 
			\[     \int_{0}^{1} |x'(t)| dt \geq \bigg| \int_{0}^{1} x'(t) dt \bigg|,     \]
			ce qui dit en particulier que la dérivée de $x$ est toujours non négative sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$ ou toujours non positive sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
			Comme $x(1)=a > 0 = x(0)$, on conclut que $x$ est croissante. 
		\end{enumerate}
	\end{preuve}
	
	%%%%%%%%%%%%%%%
	\exo{} On va s'intéresser aux courbes de longueur minimale tracées sur la sphère 
	\[     S^2 = \big\{ (x,y,z) \in \RR^{3} : x^{2} + y^{2} + z^{2} = 1 \big\}  \subseteq \RR^{3}     \] 
	de rayon $1$ et de centre $O = (0,0,0)$. 
	On appelle \emph{grand cercle} de la sphère toute intersection de la sphère avec un plan passant par $O$.
	
	Soient $A$ et $B$ deux points de $S^2$. 
	Sans perte de généralité, on considère que $A = (0,0,1)$ et $B = (\sin(\psi_0),0,\cos (\psi_0))$, 
	avec $0 \leq \psi_0\leq \pi$.
	
	On considère une courbe tracée sur la sphère 
	$M : [0,1] \rightarrow S^2$ donnée par $$t \mapsto (\cos(\theta(t))\sin(\psi(t)),\sin(\theta(t))\sin(\psi(t)),\cos (\psi(t))$$ telle que les fonctions $\theta$ et $\psi$ soient de classe $C^1$, $\theta(0)=0$, $\psi(0)=0$, $\theta(1)=0$ et $\psi(1)=\psi_0$. On a donc $M(0)=A$ et $M(1)=B$.
	On note $\ell$ la longueur de la courbe paramétrée précédente.
	
	\begin{enumerate}
		\item Montrer que 
		\[     \ell=\int_0^1\sqrt{\big(\theta'(t)\big)^2\sin^2\big(\psi(t)\big)+\big(\psi'(t)\big)^2}dt.     \]
		\item En déduire que la longueur de $\ell$ est plus grande que la longueur de l'arc de cercle du grand cercle tracé sur la sphère reliant $A$ à $B$.
		\item En déduire que pour tous points $A,B$ sur la sphère, le chemin sur la sphère le plus court reliant $A$ à $B$ est donné par l'arc de cercle reliant $A$ à $B$ d'un grand cercle de la sphère.
	\end{enumerate}
	
	\begin{preuve} 
		\begin{enumerate}
			\item La formule de la longueur $\ell$ est un calcul directe. 
			
			\item C'est clair que 
			\[          \ell=\int_0^1\sqrt{\big(\theta'(t)\big)^2\sin^2\big(\psi(t)\big)+\big(\psi'(t)\big)^2}dt \geq \int_0^1\sqrt{\big(\psi'(t)\big)^2}dt = \psi_0,     \]
			qui est la longueur de l'arc de cercle du grand cercle tracé sur la sphère reliant $A$ à $B$. 
			
			\item On remarque que, étant donnés deux points $A$ et $B$ sur la sphère, on peut toujours faire une rotation du repère de coordonnées pour tomber sur la situation décrite dans l'énoncé pour $A$ et $B$. 
			La question est alors une conséquence directe de l'item précédent. 
		\end{enumerate}
	\end{preuve}
	

\newpage \section{Intégrales doubles }

	
	
		
		%%%%%%%%%%%%%%%
		%\exo{Aire d'une surface de $\RR^3$}
		%Vous avez vu dans le cours que si $\mathcal{C}$ est un arc de classe $C^1$ de $\RR^2$ paramétré par $(I,f)$ avec $I=[a,b]$, alors la longueur de l'arc est donné par $\int_a^b\|f'(t)\|dt$.
		%De plus cette longueur peut être obtenue comme limite des longueurs de lignes polygonales approchant $\mathcal{C}$ uniformément.
		%
		%La question suivante est alors assez naturelle. Si nous considérons maintenant une surface $S$ de $\RR^3$ paramétrée par 
		%$(V,F)$ où $V\subseteq \RR^2$ est bien régulier (\textit{e.g.}, un pavé) et $F:V\rightarrow \RR^3$, peut-on définir l'aire de $S$ en triangulisant la surface, \textit{i.e.} on remplace les segments du cas de la courbe par des triangles ou plus généralement en approchant $S$ uniformément par une suite de surfaces polyédriques dont les sommets appartiennent à $S$ ?
		%Le but de cet exercice est de montrer que la réponse (due à Hermann A. Schwarz\footnote{Voir Schwarz, H. A. \textit{Sur une définition erronée de l'aire d'une surface courbe}. Gesammelte mathematische Abhandlungen, Band II, 309--311. Verlag von Julius Springer, Berlin, 1890. Nachdruck in einem Band der Auflage von 1890. Chelsea Publishing Co., Bronx, N.Y., 1972. Band I: xiv+338 pp. (vier Tafeln); Band II: vii+370 pp. (\url{https://archive.org/details/gesammeltemathem02schwuoft/page/309}).}) est négative.
		%
		%On suppose que la surface $S$ est donnée par une paramétrisation régulière $(V,F)$, c'est à dire que $F$ est de classe $C^1$ et que pour tout $(u,v)\in V$, les dérivées partielles 
		%\[     \bigg\{ \frac{\partial F}{\partial u}(u,v), \frac{\partial F}{\partial v}(u,v) \bigg\} \subseteq \RR^{3}     \] 
		%forment un ensemble libre. 
		%Ceci permet de définir le vecteur normal à $S$ en $F(u,v)$ par le produit vectoriel
		%\[     N(u,v)=\frac{\partial F}{\partial u}(u,v)\wedge \frac{\partial F}{\partial v}(u,v).     \]
		%
		%On définit alors l'aire de $S$ par $\int_V\|N(u,v)\|dudv$.
		%\begin{enumerate}
		%\item Après avoir donné des paramétrisations, calculer les aires des surfaces suivantes: 
		%\begin{enumerate}[label=(\roman*)]
		%\item un cylindre de hauteur $h$ et de rayon $r$,
		%\item un cone de hauteur $h$ et de rayon $r$,
		%\item une sphère de rayon $r$.
		%\end{enumerate}
		%
		%\exo{Paradoxe de Schwarz ou paradoxe du lampion}
		%Soit $C$ un cylindre de hauteur $h$ et de rayon $1$. 
		%On considère la paramétrisation $M : [0,2\pi] \times [0,h] \rightarrow \RR^{3}$ de $C$ donnée par 
		%\[     M(u,v) =\big( \cos (u), \sin (u), v\big).     \] 
		%On note $M(u,v)=(x(u,v),y(u,v),z(u,v))$.
		%
		%Soient $n$ et $m$ deux entiers. 
		%On pose $v_i=i h/m$ pour tout $i \in \{0, \dots, m\}$ et $u_j=2\pi j/n$, pour tout $j \in \{0, \dots, n\}$.
		%Pour $i \in \{0, \dots, m-1\}$, on considère les triangles définis de la façon suivante:
		%\begin{enumerate}[label=(\arabic*)]
		%\item pour $i$ pair et $j \in \{0, \dots, n-1\}$, on considère les triangles 
		%\[     \big(M(u_j,v_i),M(u_{j+1},v_i),M(u_j+\pi/n,v_{i+1})\big)     \] 
		%et 
		%\[     \big(M(u_j+\pi/n,v_{i+1}),M(u_{j+1}+\pi/n,v_{i+1}),M(u_{j+1},v_{i})\big);     \]
		%\item pour $i$ impair et $j \in \{0, \dots, n-1\}$, on considère les triangles 
		%\[     \big(M(u_j,v_{i+1}),M(u_{j+1},v_{i+1}),M(u_j+\pi/n,v_{i})\big)     \] 
		%et 
		%\[     \big(M(u_j+\pi/n,v_{i}),M(u_{j+1}+\pi/n,v_{i}),M(u_{j+1},v_{i+1})\big).     \]
		%\end{enumerate}
		%
		%\begin{enumerate}[label=(\roman*)]
		%\item Faire un dessin représentant le cylindre et les triangles.
		%\item Combien y a-t-il de triangles ?
		%\item Quelle est l'aire d'un triangle ? Soit $A(m,n)$ la somme des aires des triangles et $A$ l'aire du cylindre. 
		%\item Montrer en calculant différents passages à la limite que $A(m,n)$ peut avoir plein de limites différentes quand $m$ et $n$ tendent vers $+\infty$.
		%\end{enumerate}
		%\end{enumerate}
		%
		%\begin{preuve}
		%La solution suit directement des indications détaillées dans l'exercice. 
		%\end{preuve}                    
		
		
		
		%%%%%%%%%%%%%%%
		\exo{} Calculer les intégrales doubles
		\[     I = \iint_D \frac{(x+y)^2}{x^2+y^2+1}dxdy \text{ $\phantom{x}$ et $\phantom{x}$ } J=\iint_\Delta(x^2+y^2)dxdy,     \]
		où $D=\{ (x,y)\in\RR^2 : x^2+y^2\leq 1\}$ et $\Delta=\{ (x,y) \in \RR^2 : \;x^2+y^2\leq 1, x\geq 0, y\geq 0 \}$.
		
		\begin{preuve}
			On voit bien que 
			\begin{align*}
			I &= \iint_D \frac{(x+y)^2}{x^2+y^2+1}dxdy = \int_{0}^{1} \int_{0}^{2 \pi} \frac{r^3+r^{2} \sin(2 \theta)}{r^2+1} d\theta dr 
			\\
			&= 2 \pi \int_{0}^{1} \frac{r^3}{r^2+1} dr + \int_{0}^{1}\frac{r}{r^2+1} dr \underset{= 0}{\underbrace{\bigg[ - \frac{\cos(2 \theta)}{2}\bigg]_{0}^{2\pi}}} 
			\\
			&= 2 \pi \bigg[ \frac{r^{2}}{2} - \frac{\ln(1+r^{2})}{2} \bigg]_{0}^{1} = \pi \big(1 - \ln(2)\big),      
			\end{align*}
			et 
			\[     J = \iint_\Delta (x^2+y^2) dxdy = \int_{0}^{1} \int_{0}^{\pi/2} r^3 d\theta dr 
			= \frac{\pi}{2} \bigg[ \frac{r^{4}}{4} \bigg]_{0}^{1} = \frac{\pi}{8},     \]
			où l'on a fait le changement de variables des coordonnées polaires. 
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $T \subseteq \RR^{2}$ le triangle ayant pour sommets les points de coordonnées $(0,0)$, $(1,0)$ et $(0,1)$. 
		Montrer que
		\[     \iint_T e^{\frac{x-y}{x+y}}dxdy=\frac{1}{2}\sinh(1).     \]
		
		\begin{preuve}
			On considère le changement de variables $F : \RR^{2} \rightarrow \RR^{2}$ donné par 
			\[     F(u,v) = \Big( \frac{u+v}{2}, \frac{v-u}{2}  \Big).     \]
			C'est clair que $F$ est bijectif, de classe $C^{1}$ avec réciproque de classe $C^{1}$. 
			En plus, le déterminant de la matrice jacobienne de $F$ est $1/2$. 
			Soit $D = \{ (u,v) \in \RR^{2} : v \in [0,1], |u| \leq v \}$. 
			On voit bien aussi que $F(D) = T$.
			En conséquence, 
			\begin{align*}
			\iint_T e^{\frac{x-y}{x+y}}dxdy &= \frac{1}{2} \iint_D e^{u/v}dudv
			= \frac{1}{2} \int_{0}^{1} \int_{-v}^{v} e^{u/v} du dv = \frac{1}{2} \int_{0}^{1} \bigg[ v e^{u/v} \bigg]_{-v}^{v} dv 
			\\
			&= \frac{1}{2} \int_{0}^{1} \bigg[ v e^{u/v} \bigg]_{-v}^{v} dv = \sinh(1) \int_{0}^{1} v dv = \frac{\sinh(1)}{2}. 
			\end{align*}
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Montrer que l'aire de la région bornée $R$ comprise entre la droite $y=x$ et la parabole $y^2=2x$ vaut $2/3$.
		
		\begin{preuve}
			La région demandée est décrite par 
			\[     R = \{ (x,y) \in \RR^{2} : x \in [0,2], x \leq y \leq \sqrt{2x} \}.     \]        
			L'aire demandée est donnée par l'intégrale 
			\[     \iint_{R} dx dy = \int_{0}^{2} \int_{x}^{\sqrt{2x}} dy dx = \int_{0}^{2} (\sqrt{2} \sqrt{x} - x) dx = \bigg[ 2 \sqrt{2} \frac{x^{3/2}}{3}  - \frac{x^{2}}{2} \bigg]_{0}^{2} = \frac{2}{3}.       \]
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $R = \{ (x,y) \in \RR^{2}_{> 0} : 1 \leq xy \leq 3, 1\leq x^{2} - y^{2} \leq 4 \}$. 
		Calculer 
		\[     \iint_{R} (x^{2}+y^{2}) dx dy.     \]
		\\
		\textbf{Indication :} utiliser le changement de variables donné par $t = x^{2}-y^{2}$ et $s=xy$. 
		
		\begin{preuve}
			L'indication équivaut à considérer le changement de variables $F : \RR_{>0} \times \RR \rightarrow \RR^{2}_{>0}$ donné par 
			\[     F(s,t) = \bigg( \sqrt{\frac{t+\sqrt{t^{2}+4s^{2}}}{2}} , \sqrt{\frac{-t+\sqrt{t^{2}+4s^{2}}}{2}} \bigg).     \]
			C'est clair que $F$ est bijectif, avec application réciproque $G : \RR^{2}_{>0} \rightarrow \RR_{>0} \times \RR$ donnée par
			\[     G(x,y) = (xy , x^{2}-y^{2}).     \]
			En plus, $F$ et $G$ sont de classe  $C^{1}$. 
			Le déterminant de la matrice jacobienne de $F$ est $-1/(2\sqrt{t^{2}+4s^{2}})$. 
			Soit $D = \{ (s,t) \in \RR^{2}_{>0} : s \in [1,3], t \in [1,4] \}$. 
			On voit bien aussi que $F(D) = R$. 
			Si l'on écrit $f(x,y) = x^{2} + y^{2}$, alors $(f \circ F) (s,t) = \sqrt{t^{2}+4s^{2}}$. 
			En conséquence, 
			\begin{align*}
			\iint_{R} (x^{2}+y^{2}) dx dy &=  \iint_D \frac{\sqrt{t^{2}+4s^{2}}}{2\sqrt{t^{2}+4s^{2}}} dsdt = \frac{1}{2} \iint_D dsdt = 3.
			\end{align*}
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $a >0$ et $H = \{ (x,y) \in \RR^{2} : x \geq a \}$. 
		Montrer que 
		\[     \iint_{H} e^{-(x^{2}+y^{2})} dx dy = a e^{-a^{2}} \int_{0}^{+\infty} \frac{e^{-t^{2}}}{a^{2}+t^{2}} dt.     \]
		\\
		\textbf{Indication :} utiliser le changement de variables donné par $x^{2}+y^{2} = t^{2} + a^{2}$ et $y=sx$. 
		
		\begin{preuve}
			Soit $U = \{ (x,y) \in \RR^{2} : x > 0, x^{2} + y^{2} > a^{2} \}$. 
			L'indication équivaut à considérer le changement de variables $F : \RR \times \RR_{>0} \rightarrow U \subseteq \RR_{>0} \times \RR$ donné par 
			\[     F(s,t) = \bigg( \sqrt{\frac{t^{2}+ a^{2}}{s^{2}+1}} , s \sqrt{\frac{t^{2}+ a^{2}}{s^{2}+1}} \bigg).     \]
			C'est clair que $F$ est bijectif, avec application réciproque $G : U \subseteq \RR_{>a} \times \RR \rightarrow \RR \times \RR_{>0}$ donnée par
			\[     G(x,y) = \big(y/x , \sqrt{x^{2}+y^{2}-a^{2}}\big).     \]
			En plus, $F$ et $G$ sont de classe  $C^{1}$. 
			Soit $D = \{ (s,t) \in \RR \times \RR_{>0} : t > a |s| \}$. 
			On voit vien que $F(D) = H$. 
			Le déterminant de la matrice jacobienne de $F$ est $t/(s^{2}+1)$. 
			Si l'on écrit $f(x,y) = e^{-(x^{2}+y^{2})}$, alors $(f \circ F) (s,t) = e^{-a^{2}} e^{-t^{2}}$. 
			En conséquence, 
			\begin{align*}
			\iint_{H} &e^{-(x^{2}+y^{2})} dx dy =  \iint_{D} \frac{e^{-a^{2}} e^{-t^{2}} t}{s^{2}+1} dsdt 
			= e^{-a^{2}} \int_{0}^{+ \infty} \int_{-t/a}^{t/a} e^{-t^{2}} t \frac{1}{s^{2}+1}  ds dt 
			\\
			&= e^{-a^{2}} \int_{0}^{+ \infty} e^{-t^{2}} t \bigg[ \arctan(s) \bigg]_{-t/a}^{t/a}  dt 
			= - e^{-a^{2}} \int_{0}^{+ \infty} \arctan(t/a) (- 2) t e^{-t^{2}} dt 
			\\
			&= - e^{-a^{2}} \underset{=0}{\underbrace{\underset{M \rightarrow + \infty} {\lim} \bigg[ e^{-t^{2}} \arctan(t/a) \bigg]_{0}^{+ \infty}}} + e^{-a^{2}} \int_{0}^{+ \infty} \frac{e^{-t^{2}}}{a^{2}+t^{2}} dt
			\\
			&= a e^{-a^{2}} \int_{0}^{+ \infty} \frac{e^{-t^{2}}}{a^{2}+t^{2}} dt,
			\end{align*}
			où l'on a fait une intégration par parties avec $u(t) = \arctan(t/a)$ et $v'(t) = - 2 t e^{-t^{2}}$ (\textit{i.e.} $v(t) = e^{-t^{2}}$) dans la cinquième égalité.
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $\epsilon \in \hskip 0.6mm ] \hskip 0.6mm 0, 1 \hskip 0.6mm [ \hskip 0.6mm$ et $A_{\epsilon} = \{ (x,y) \in \RR^{2} : \epsilon^{2} \leq x^{2} + y^{2} \leq 1 \}$. 
		\begin{enumerate}
			\item Calculer  
			\[     I_{\epsilon} = \iint_{A_{\epsilon}} \frac{1}{\sqrt{x^{2} + y^{2}}} dx dy.     \]
			\item Montrer que la limite 
			\[     \underset{\epsilon \rightarrow 0^{+}}{\lim} I_{\epsilon}     \]
			existe. 
			Conclure que la fonction $(x,y) \mapsto 1/\sqrt{x^{2} + y^{2}}$ est intégrable dans un voisinage de l'origine et 
			comparer avec la fonction $x \mapsto 1/|x|$. 
			%\\
			%\textbf{Indication :} utiliser le changement de variables des coordonnées polaires. 
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item 
				On voit bien que 
				\[     I_{\epsilon} = \iint_{A_{\epsilon}} \frac{1}{\sqrt{x^{2} + y^{2}}} dx dy =  \int_{0}^{2 \pi} \int_{\epsilon}^{1} d\theta dr = 2 \pi \bigg[ r  \bigg]_{\epsilon}^{1} = 2 \pi (1-\epsilon).       \]
				
				\item C'est clair alors que 
				\[     \underset{\epsilon \rightarrow 0^{+}}{\lim} I_{\epsilon} = 2 \pi.      \]
				Par contre, 
				\[     \int_{-1}^{-\epsilon} \frac{1}{|x|} dx + \int_{\epsilon}^{1} \frac{1}{|x|} dx = 2 \int_{\epsilon}^{1} \frac{1}{|x|} dx = 2 \bigg[ \ln(|x|)   \bigg]_{\epsilon}^{1} = 2 \ln(1/\epsilon)     \]
				diverge quand $\epsilon$ tend vers zéro. 
			\end{enumerate}
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{} Montrer que l'intégrale de $f(x,y)=x$ sur le disque $x^2+y^2-2x\leq 0$ vaut $\pi$.
		
		\begin{preuve}
			On voit bien que $x^2+y^2-2x\leq 0$ équivaut à $(x-1)^{2} + y^{2} \leq 1$. 
			On définit $R = \{ (x,y) \in \RR^{2} : (x-1)^{2} + y^{2} \leq 1\}$. 
			On considère le changement de variables $F(u,v) = (u+1,v)$. 
			Soit $R' = \{ (u,v) \in \RR^{2} : u^{2} + v^{2} \leq 1\}$.
			C'est clair que $F$ est bijectif, de classe $C^{1}$, avec réciproque de classe $C^{1}$, le déterminant de la matrice jacobienne est $1$, et $F(R') = R$. 
			Alors, 
			\begin{align*}
			\iint_{R} x dx dy &= \iint_{R'} (u+1) du dv = \int_{0}^{1} \int_{0}^{2 \pi} (r \cos(\theta) + 1) r d\theta dr 
			\\
			&= \int_{0}^{1} \bigg(r^{2} \underset{=0}{\underbrace{\bigg[ \sin(\theta) \bigg]_{0}^{2\pi}}} + 2 \pi r\bigg) dr = \pi \bigg[ r^{2} \bigg]_{0}^{1} = \pi,    
			\end{align*}
			où l'on a utilisé le changement de variables des coordonnées polaires.
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} On note $\Delta=[0,1]^2$ et pour tout entier $n \in \NN$ on pose
		\[     I_n=\iint_\Delta \frac{(xy)^ndxdy}{1+xy}.     \]
		\begin{enumerate} 
			\item Déterminer 
			\[     \underset{n \rightarrow +\infty}{\lim} I_n.     \]
			
			\item En déduire la valeur de 
			\[     \iint_\Delta\frac{dxdy}{1+xy}.     \]
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate} 
				\item 
				Soit $f_{n} : \Delta \rightarrow \RR$ la fonction donnée par $f_{n}(x,y) = (xy)^{n}/(1+xy)$, pour $n \in \NN$. 
				C'est clair que $f_{n}$ est intégrable sur $\Delta$, pour tout $n \in \NN$, 
				puisqu'il s'agit de la restriction de la fonction continue sur un compact. 
				Soit $g_{n} = f_{n}|_{\Delta^{\circ}}$, où $\Delta^{\circ}$ est l'intérieur de $\Delta$. 
				On remarque d'abord que 
				\[     I_n=\iint_{\Delta^{\circ}} \frac{(xy)^ndxdy}{1+xy},     \]
				pour tout $n \in \NN$.
				C'est clair que $g_{n}$ converge simplement vers la fonction nulle de $\Delta^{\circ}$. 
				En outre, $|g_{n}(x,y)| \leq g_{0} = 1/(1+xy)$, pour tout $(x,y) \in \Delta^{\circ}$, et $g_{0} = 1/(1+xy)$ est intégrable sur $\Delta^{\circ}$. 
				Le théorème de convergence dominée nous dit que 
				\[     \underset{n \rightarrow +\infty}{\lim} I_{n} = \underset{n \rightarrow +\infty}{\lim} \iint_{\Delta^{\circ}} \frac{(xy)^ndxdy}{1+xy} = \iint_{\Delta^{\circ}} 0 dx dy = 0.     \]
				
				\item On voit bien que 
				\begin{align*}
				I_n&=\iint_\Delta \frac{(xy)^ndxdy}{1+xy} = \iint_\Delta \frac{(1+ xy) (xy)^ndxdy}{1+xy} - \iint_\Delta \frac{(xy)^{n+1} dxdy}{1+xy} 
				\\
				&= \iint_\Delta x^{n} y^{n} dxdy - I_{n+1} = \frac{1}{(n+1)^{2}} - I_{n+1},    
				\end{align*} 
				pour tout $n \in \NN$. 
				Un argument par récurrence simple nous dit que 
				\[     I_{n} = (-1)^{n} I_{0} + (-1)^{n} \sum_{m=1}^{n} \frac{(-1)^{m}}{m^{2}},     \]
				pour tout $n \in \NN$. 
				Comme la limite de $I_{n}$ est zéro quand $n$ tend vers $+ \infty$, on conclut que 
				\begin{align*} 
				\iint_\Delta\frac{dxdy}{1+xy} &= I_{0} = \sum_{m=1}^{+\infty} \frac{(-1)^{m+1}}{m^{2}} = \sum_{k=1}^{+\infty} \frac{1}{(2k+1)^{2}} - \sum_{k=1}^{+\infty} \frac{1}{(2k)^{2}} 
				\\
				&= \sum_{m=1}^{+\infty} \frac{1}{m^{2}} - 2 \sum_{k=1}^{+\infty} \frac{1}{(2k)^{2}} = \frac{1}{2} \sum_{m=1}^{+\infty} \frac{1}{m^{2}} = \frac{\pi^{2}}{12},     
				\end{align*}
				où l'on a utilisé que
				\[    \sum_{m=1}^{+\infty} \frac{1}{m^{2}} = \frac{\pi^{2}}{6}     \]
				(voir l'exercice \textbf{7} de la fiche $4$). 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Calculer l'aire de $D=\{(x,y)\in\RR^2 : |x|\leq x^2+y^2\leq 1\}$ et l'intégrale
		\[     \iint_D\frac{dx dy}{1+x^2+y^2}.     \]
		
		\begin{preuve}
			On note d'abord que $(x,y) \in D$ si et seulement si $(-x,y) \in D$. 
			Il suffit donc de déterminer $D \cap (\RR_{\geq 0} \times \RR)$. 
			Soit $(x,y) \in D$ avec $x \geq 0$. 
			Cela veut dire que $x\leq x^2+y^2\leq 1$, \textit{i.e.} $x^2+y^2 \leq 1$ et $x\leq x^2+y^2$. 
			La dernière condition est équivalente à $(1/2)^{2} \leq (x-1/2)^2+y^2$. 
			En conséquence, 
			\[     D= \bar{B}_{1}(0,0) \setminus \big(B_{1/2}(1/2,0) \cup B_{1/2}(-1/2,0)\big),     \] 
			où $B_{r}(x_{0},y_{0}) = \{ (x,y)\in \RR^{2} : (x-x_{0})^{2} + (y-y_{0})^{2} < r^{2} \}$ est la boule ouverte de centre $(x_{0},y_{0})$ et rayon $r > 0$ et $\bar{B}_{r}(x_{0},y_{0})$ est son adhérence. 
			L'aire de $D$ est par conséquent $\pi - 2 \pi (1/2)^{2} = \pi/2$. 
			
			
			
			\begin{center}
				\begin{tikzpicture}[x=3em,y=3em,>=stealth]
				\tikzmath{%
					\d= 1;
					\r=0.5;
				}
				
				%\draw[-,fill=gray] (1,1) -- (1,-1) -- (-1,-1) -- (-1,1) -- (1,1);
				\draw[-,fill=gray] (0,0) circle [radius=\d];
				\draw[-,fill=gray!30!white] (0.5,0) circle [radius=\r];
				\draw[-,fill=gray!30!white] (-0.5,0) circle [radius=\r];
				%\draw[-,fill=gray!20!white] (0,-1) -- (1,0) -- (0,1) -- (-1,0) -- (0,-1);
				\draw [thick,->] (0, 0) -- (2*\d, 0);
				\node at (\d-0.5, -0.1) {\begin{tiny}$x$\end{tiny}};
				\node at (1.1,0.2) {\begin{tiny}$1$\end{tiny}};
				\node at (1,0) {$\bullet$};
				\draw [thick,->] (0, 0) -- (0,2*\d);
				\node at (-0.1, \d-0.5) {\begin{tiny}$y$\end{tiny}};
				\node at (0.2, 1.1) {\begin{tiny}$1$\end{tiny}};
				\node at (0,1) {$\bullet$};
				\node at (0,0) {$\bullet$} node [below left] {\begin{tiny}$(0,0)$\end{tiny}};
				%\fill circle [radius=0.125] node [below left] {\begin{tiny}$(0,0)$\end{tiny}};
				%\draw [dashed] (-\r,0) -- (\r,0) (0,-\r) -- (0,\r);
				%\fill [fill=gray] (\x, \y) circle [radius=0.125] 
				%  
				\end{tikzpicture}
			\end{center}
			
			Par ailleurs, 
			\begin{align*} 
			\iint_D &\frac{dx dy}{1+x^2+y^2} = \iint_{\bar{B}_{1}(0,0)} \frac{dx dy}{1+x^2+y^2} - \iint_{B_{1/2}(1/2,0)} \frac{dx dy}{1+x^2+y^2} 
			\\
			&- \iint_{B_{1/2}(-1/2,0)} \frac{dx dy}{1+x^2+y^2} 
			\\
			&= \iint_{\bar{B}_{1}(0,0)} \frac{dx dy}{1+x^2+y^2} - 2 \iint_{B_{1/2}(1/2,0)} \frac{dx dy}{1+x^2+y^2},     
			\end{align*}
			puisque la fonction $1+x^2+y^2$ est invariante sous la transformation $(x,y) \mapsto (-x,y)$. 
			
			C'est facile à vérifier que 
			\[     \iint_{\bar{B}_{1}(0,0)} \frac{dx dy}{1+x^2+y^2} = \int_{0}^{1} \int_{0}^{2 \pi} \frac{r d\theta dr}{1+r^2} = \pi \bigg[ \ln(1+r^{2}) \bigg]_{0}^{1} = \pi \ln(2).     \]
			En outre, si l'on utilise des coordonnées polaires centrées autour du $(1/2,0)$ (\textit{i.e.} $x = r \cos(\theta) +1/2$ et $y = r \sin(\theta)$) on trouve que 
			\begin{align*}
			\hskip -0.5cm \iint_{B_{1}(1/2,0)} &\frac{dx dy}{1+x^2+y^2} = \int_{0}^{1/2} \int_{0}^{2 \pi} \frac{d\theta dr}{\cos(\theta) + r + 5/(4r)} 
			= \int_{0}^{1/2} \frac{2 \pi}{\sqrt{\big(r + 5/(4r)\big)^{2}-1}} dr
			\\
			&= \int_{0}^{1/2} \frac{8 \pi r}{\sqrt{16 r^{4} + 24 r^{2}+ 25}} dr
			= \int_{0}^{1} \frac{\pi}{\sqrt{s^{2} + 6 s + 25}} ds = \int_{3/4}^{1} \frac{\pi}{\sqrt{t^{2} + 1}} ds 
			\\
			&= \pi \bigg[ \ln\Big(\big|t+\sqrt{1+t^{2}}\big|\Big) \bigg]_{3/4}^{1} = \pi \big( \ln(1+\sqrt{2}) - \ln(2) \big),     
			\end{align*}
			où l'on a utilisé l'identité 
			\[     \int_{0}^{2 \pi} \frac{d \theta}{a + \cos(\theta)} = \frac{2 \pi}{\sqrt{a^{2}-1}},     \]
			pour tout $a > 1$ (voir l'exercice \textbf{15} de la fiche $0$) dans la deuxième égalité, et la substitution $s = 4 r^{2}$ dans la quatrième égalité et la substitution $t = (s + 3)/4$ dans la cinquième. 
			
			En conséquence, 
			\[          \iint_D \frac{dx dy}{1+x^2+y^2} = \pi \ln(2) - 2 \pi \big( \ln(1+\sqrt{2}) - \ln(2) \big) = 3 \pi \ln(2) - 2 \pi \ln(1+\sqrt{2}).     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		\label{exo:12}
		Calculer l'aire $A$ de l'intérieur de l'ellipse
		\[     \left (\frac{x}{a}\right)^2+\left (\frac{y}{b}\right )^2\leq 1     \]
		au moyen du changement de variables $x=au\cos(v)$ et $y=bu\sin(v)$.
		\begin{preuve}
			Soit $E = \{ (x,y) \in \RR^{2} : (x/a)^{2}+(y/b)^{2} < 1 \}$. 
			On voit bien que 
			\[     A = \iint_{E} dx dy = \int_{0}^{1} \int_{0}^{2 \pi} a \,b \, u \, dv du = 2 \pi a b \int_{0}^{1} v dv = \pi a b \bigg[ v^{2} \bigg]_{0}^{1} = \pi a b.      \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $D=\{(x,y) \in \RR^2 : |x| \leq 1, |y| \leq x^2\}$. 
		Représenter $D$, calculer son aire et l'intégrale
		\[     \iint_D(x^2-y)dxdy.     \]
		
		\begin{preuve}
			L'aire de $D$ est donnée par 
			\[     \iint_D dxdy = \int_{-1}^{1} \int_{-x^{2}}^{x^{2}} dy dx = \int_{-1}^{1} 2 x^{2} dx =  2 \bigg[ \frac{x^{3}}{3} \bigg]_{-1}^{1} = \frac{4}{3}.     \]
			Par ailleurs, 
			\begin{align*}
			\iint_D(x^2-y) dxdy &= \int_{-1}^{1} \int_{-x^{2}}^{x^{2}} (x^2-y) dy dx = \int_{-1}^{1} \bigg[ \frac{2 x^{2} y - y^{2}}{2} \bigg]_{-x^{2}}^{x^{2}} 
			\\
			&= 2 \int_{-1}^{1} x^{4} dx = 2 \bigg[ \frac{x^{5}}{5} \bigg]_{-1}^{1} = \frac{4}{5}.     
			\end{align*}     
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Pour $n \in \NN^{*}$ et $R \geq 0$, soit $\omega_n(R)$ le volume de la boule 
		\[     \bar{B}_{n}(R) = \big\{ (x_{1},\dots, x_{n}) \in \RR^{n} : x_{1}^{2} + \dots + x_{n}^{2} \leq R^{2} \big\} \subseteq \RR^n     \] 
		de rayon $R$. 
		Le but de cet exercice est de calculer $\omega_n(R)$. 
		\begin{enumerate}
			\item Montrer que $\omega_{n}(R) = R^{n} \omega_{n}(1)$, pour tout $R \geq 0$ et $n \in \NN^{*}$.
			
			\item Redémontrer les formules classiques
			\[     \omega_1(R)=2R, \omega_2(R)=\pi R^2 \text{ et } \omega_3(R)=\frac{4}{3}\pi R^3.     \]
			
			\item On suppose désormais $n \geq 2$. 
			Soit $g : \RR^{n} \rightarrow \RR$ la fonction indicatrice de $\bar{B}_{n}(1)$ et soit $R_{n-2} = [-1,1]^{n-2} \subseteq \RR^{n-2}$. 
			On écrit $x = x_{1}$ et $y = x_{2}$. 
			Montrer que 
			\[     \omega_{n}(1) = \int_{-1}^{1} \int_{-1}^{1} \bigg( \int_{R_{n-2}} g(x,y,x_{3},\dots,x_{n}) dx_{3} \dots dx_{n}\bigg) dx dy.     \]
			
			\item Montrer que, si $x^{2}+y^{2} > 1$, $g(x,y,x_{3},\dots,x_{n}) = 0$, tandis que, si $x^{2}+y^{2} \leq 1$, la fonction $(x_{3},\dots,x_{n}) \mapsto g(x,y,x_{3},\dots,x_{n})$ (avec $x$ et $y$ fixes) coïncide avec la fonction indicatrice de la boule $\bar{B}_{n-2}(\sqrt{1-x^{2}-y^{2}})$. 
			En déduire que 
			\[     \int_{R_{n-2}} g(x,y,x_{3},\dots,x_{n}) dx_{3} \dots dx_{n} = (1-x^{2}-y^{2})^{(n-2)/2} \omega_{n-2}(1),     \]
			et en conséquence 
			\[     \omega_{n}(1) = \omega_{n-2}(1) \iint_{\bar{B}_{2}(1)} \big(1-x^{2}-y^{2}\big)^{(n-2)/2} dx dy.     \]
			
			\item Utiliser le changement de variables des coordonnées polaires pour calculer la dernière intégrale. 
			En déduire que 
			\[     \omega_{2n}(1) = \frac{\pi^{n}}{n!} \text{ et } \omega_{2n-1}(1) = \frac{2^{n} \pi^{n-1}}{(2n-1)!!},     \]
			pour tout $n \in \NN^{*}$, où l'on rappelle que $(2n-1)!! = (2n-3)!! . (2n-1)$ et $1!! = 1$.
			
			\item Montrer que 
			\[     \omega_{n}(1) = \frac{\pi^{n/2}}{\Gamma(1+n/2)},     \]
			pour tout $n \in \NN^{*}$. 
		\end{enumerate}
		
		\begin{preuve}
			La solution suit directement des indications détaillées dans l'exercice. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}  
		\begin{enumerate}
			\item L'intégrale $J=\int_0^{+\infty} e^{-x^2} dx$ est-elle convergente?
			
			\item Pour $a>0$, on pose $D_a=[0,a] \times [0,a]$, $\Delta_a= \{ (x,y)\in \RR^2_{\geq 0} : x^2+y^2\leq a^2 \}$, 
			\[     Q(a)=\iint_{D_a}f(x)f(y)dxdy \text{ et } C(a)=\iint_{\Delta_a}f(x)f(y)dxdy,     \] 
			où $f(x)=e^{-x^2}$. 
			Encadrer $Q(a)$ à l'aide de la fonction $C(a)$. 
			En déduire la valeur de $J$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item 
				On sait que la limite de $e^{-x^2} (1+x^{2})$ est zéro quand $x$ tend vers $+ \infty$. 
				Cela implique qu'il existe $M > 0$ tel que $e^{-x^2} (1+x^{2}) \leq 1$ pour tout $x \geq M$. 
				En outre, comme  $e^{-x^2} (1+x^{2})$ est continue, elle est bornée sur $[0,M]$, \textit{i.e.} il existe 
				$C' > 0$ tel que $e^{-x^2} (1+x^{2}) \leq C'$ pour tout $x \in [0,M]$. 
				Soit $C = \operatorname{max}(1,C') > 0$. 
				Alors $e^{-x^2} (1+x^{2}) \leq C'$ pour tout $x \in \RR_{\geq 0}$. 
				En conséquence, 
				\[     J=\int_0^{+\infty} e^{-x^2} dx \leq C \int_0^{+\infty} \frac{1}{1+x^{2}} dx.     \]
				Comme $e^{-x^2}$ est une fonction positive, on conclut que l'intégrale $J$ est convergente. 
				
				\item Comme $\Delta_a \subseteq D_{a} \subseteq \Delta_{\sqrt{2} a}$ et $e^{-x^2}$ est une fonction positive, 
				\[    C(a) \leq Q(a) \leq C(\sqrt{2} a).     \]
				Or, 
				\[     C(a)=\iint_{\Delta_a}f(x)f(y)dxdy = \int_{0}^{\pi/2} \int_{0}^{a} e^{-r^{2}} r dr d\theta = \frac{\pi}{2} \bigg[ -\frac{e^{-r^{2}}}{2} \bigg]_{0}^{a} = \frac{\pi (1 - e^{-a^{2}})}{4},     \]
				et 
				\[     Q(a) = \iint_{D_a}f(x)f(y)dxdy = \int_{0}^{a} e^{-x^{2}} dx \int_{0}^{a} e^{-y^{2}} dy = \bigg( \int_{0}^{a} e^{-x^{2}} dx \bigg)^{2},     \]
				pour tout $a > 0$.
				En particulier, 
				\[     \underset{a \rightarrow + \infty} {\lim} C(a) = \frac{\pi}{8} \text{ et } \underset{a \rightarrow + \infty} {\lim} Q(a) = J^{2}.     \]
				En conséquence, 
				\[     \frac{\pi}{4} = \underset{a \rightarrow + \infty} {\lim} C(a) \leq \underset{a \rightarrow + \infty} {\lim} Q(a) =J^{2} \leq \underset{a \rightarrow + \infty} {\lim} C(\sqrt{2} a) = \frac{\pi}{4},     \]
				et $J = \sqrt{\pi}/2$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		%\exo{Formule de Green-Riemann} Soient $a<b$, $\varphi_1, \varphi_2 : [a,b]\rightarrow \RR$ fonctions continues telles que $\varphi_1(a)=\varphi_2(a)$, $\varphi_1(b)=\varphi_2(b)$ et $\varphi_1(t)\leq \varphi_2(t)$, pour tout $t \in [a,b]$.
		%Soient $c<d$, $\psi_1,\psi_2:[c,d]\rightarrow \RR$ fonctions continues telles que $\psi_1(c)=\psi_2(c)$, $\psi_1(d)=\psi_2(d)$ et $\psi_1(t)\leq \psi_2(t)$, pour tout $t \in [c,d]$.
		%
		%Soit $D$ un domaine du plan tel que 
		%\begin{equation*}
		%\begin{split}
		%D &= \big\{(x,y) \in \RR^{2} : a \leq x \leq b, \varphi_1(x)\leq y\leq \varphi_2(x) \big\} 
		%\\
		%&= \big\{(x,y) \in \RR^{2} : c\leq y\leq d, \psi_1(y)\leq x\leq \psi_2(y)\big\}.
		%\end{split}
		%\end{equation*}          
		%Faire un dessin.
		%Soit $F:\RR^2\rightarrow \RR^2$ de classe $C^1$. 
		%On écrit $F(x,y) = (P(x,y),Q(x,y))$. 
		%
		%Soit $\gamma:[0,1]\rightarrow\RR^2$ la courbe du plan définie par 
		%\[     \gamma(t) = \begin{cases} 
		%\big( (1-2t)a+2tb,\varphi_1((1-2t)a+2tb)\big), &\text{si $t \in [0,1/2]$},
		%\\
		%\big((2t-1)a+2(1-t)b,\varphi_2((2t-1)a+2(1-t)b)\big), &\text{si $t \in [1/2,1]$}.
		%\end{cases} 
		%\]
		%On voit que $\gamma([0,1])$ est le bord de $D$.
		%
		%\begin{enumerate}
		%\item Montrer que 
		%\[     - \iint_D \frac{\partial P}{\partial y}(x,y) dxdy=\int_\gamma P\vec{i},     \] 
		%où la deuxième intégrale est une intégrale curviligne et $(\vec{i},\vec{j})$ est la base canonique de $\RR^2$.
		%
		%\item Montrer de même que 
		%\[     \iint_D\frac{\partial Q}{\partial x}(x,y) dxdy=\int_\gamma Q\vec{j},     \] 
		%d'où
		%\[     \iint_D \bigg(\frac{\partial Q}{\partial x}(x,y)-\frac{\partial P}{\partial y}(x,y)\bigg) dxdy=\int_\gamma F.     \]
		%Cette identité est appelée \emph{formule de Green-Riemann} et elle est valable pour tout domaine $D$ dont le bord est une courbe régulière par morceaux bien orientée.
		%
		%\item Si $P(x,y)=y$ et $Q(x,y)=x$, on obtient
		%\[     \iint_D  dxdy=\frac{1}{2} \int_0^1\big(\gamma_1(t)\gamma'_2(t)-\gamma_2(t)\gamma'_1(t)\big)dt     \] 
		%si le bord de $D$ est paramétrée par la courbe $\gamma : [0,1]\rightarrow \RR^2$ orientée positivement.
		%
		%\item Retrouver ainsi l'aire d'une ellipse de demi-axes de longueurs $a>0$ et $b>0$.
		%\end{enumerate}
		%
		%\begin{preuve}
		%La solution suit directement des indications détaillées dans l'exercice. 
		%\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Montrer que 
		\[     B(u,v)=\frac{\Gamma(u)\Gamma(v)}{\Gamma(u+v)},     \]
		pour tout $(u,v) \in \mathcal{D}$ (voir les exercices \textbf{30} et \textbf{32} de la fiche 3). 
		
		\begin{preuve}
			On sait que  
			\[     \Gamma(u)\Gamma(v) = \int_{0}^{+ \infty} \int_{0}^{+ \infty} e^{-t-s} s^{u-1} t^{v-1} ds dt.     \]
			On considère le changement de variables donné par l'application $F : \RR_{>0} \times \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [\hskip 0.6mm \rightarrow \RR^{2}_{>0}$ définie par $F(x,y) = (x(1-y), xy)$. 
			C'est clair que $F$ est bijectif et de classe $C^{1}$ avec réciproque de classe $C^{1}$. 
			Le déterminant de la matrice jacobienne de $F$ est $x$. 
			En outre, si $f(s,t) = e^{-t-s} t^{u-1} s^{v-1}$, on voit bien que $(f \circ F)(x,y) = e^{-x} x^{u + v -2} (1-y)^{u-1} y^{v-1}$. 
			Cela implique que 
			\begin{align*}
			\Gamma(u)\Gamma(v) &= \int_{0}^{+ \infty} \int_{0}^{+ \infty} e^{-t-s} t^{u-1} s^{v-1} dt ds 
			\\
			&= \int_{0}^{1} \int_{0}^{+ \infty} e^{-x} x^{u + v - 1} (1-y)^{u-1} y^{v-1} dx dy = \Gamma(u+v) B(u,v).     
			\end{align*}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $c > 0$. 
		Déterminer le volume du simplexe de $\RR^n$ donné par 
		\[     \Delta_n(c)= \big\{(x_1,\ldots,x_n)\in\RR^n : x_i \geq 0, x_1+\cdots+x_n \leq c \big\}.     \]
		
		\begin{preuve}
			On voit bien que 
			\begin{align*}
			\int_{\Delta_n(c)} &dx_{n} \dots dx_{1} 
			= \int_{0}^{c} \int_{0}^{x_{1}} \dots \int_{0}^{x_{n-3}} \int_{0}^{x_{n-2}} \int_{0}^{x_{n-1}} dx_{n} dx_{n-1} dx_{n-2} \dots dx_{2} dx_{1} 
			\\
			&= \int_{0}^{c} \int_{0}^{x_{1}} \dots \int_{0}^{x_{n-3}} \int_{0}^{x_{n-2}} x_{n-1} dx_{n-1} dx_{n-2} \dots dx_{2} dx_{1} 
			\\
			&= \int_{0}^{c} \int_{0}^{x_{1}} \dots \int_{0}^{x_{n-3}} \frac{x_{n-2}^{2}}{2} dx_{n-2} \dots dx_{2} dx_{1} = \frac{c^{n}}{n!}.     
			\end{align*}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Intégrales de Fresnel} 
		On pose 
		\[     I=\int_0^{+\infty}\cos (t^2) dt \text{ et } J=\int_0^{+\infty}\sin (t^2) dt.     \]
		\begin{enumerate}
			\item Soient 
			\[     I'=\int_0^{+\infty}\frac{\cos (s)}{\sqrt{s}}ds \text{ et } J'=\int_0^{+\infty}\frac{\sin (s)}{\sqrt{s}}ds.     \]
			Montrer que $I= I'/2$ et $J=J'/2$.
			
			\item Montrer que $I$ et $J$ sont bien définies.
			
			\item En utilisant 
			\[     \frac{\sqrt{\pi}}{2}=\int_0^{+\infty}e^{-x^2}dx,     \] 
			montrer que 
			\[     \frac{1}{\sqrt{t}}=\frac{2}{\sqrt{\pi}}\int_0^{+\infty}e^{-tx^2}dx,     \]
			pour tout $t>0$.  
			
			\item Justifier que  
			\[     \int_0^{+\infty}\bigg(\int_A^Be^{it}e^{-tx^2} dt\bigg)dx=\int_A^B\bigg(\int_0^{+\infty}e^{it}e^{-tx^2}dx\bigg)dt,     \]
			pour tous $0<A<B$. 
			En déduire que 
			\[     \int_A^B\frac{e^{it}}{\sqrt{t}}dt=\frac{2}{\sqrt{\pi}}\int_0^{+\infty}\bigg(\int_A^Be^{it}e^{-tx^2} dt\bigg)dx.     \]
			
			\item Montrer par passage à la limite que 
			\[     \int_0^{+\infty}\frac{e^{it}}{\sqrt{t}}dt=\frac{2}{\sqrt{\pi}}\int_0^{+\infty}\frac{i+u^2}{1+u^4}du.     \]
			
			\item En déduire que 
			\[     I=J=\frac{1}{2} \sqrt{\frac{\pi}{2}}.     \]
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On voit bien que $I'/2$ et $J'/2$ s'obtiennent de $I$ et $J$, respectivement, à partir de la substitution $s = t^{2}$. 
				
				\item D'après l'item précédent il suffit de montrer que $I'$ et $J'$ convergent, ou, de façon équivalente, 
				il suffit de montrer que l'intégrale
				\[      K' = \int_0^{+\infty}\frac{e^{is}}{\sqrt{s}}ds     \]
				converge. 
				On remarque qu'il ne s'agit pas de convergence absolue dans ce cas ($K'$ ne converge pas absolument). 
				Comme $e^{is}/\sqrt{s}$ est absolument intégrable sur $ \hskip 0.6mm ] \hskip 0.6mm 0,1]$, puisque 
				$|e^{is}|/\sqrt{s} = 1/ \sqrt{s}$ est absolument intégrable sur $ \hskip 0.6mm ] \hskip 0.6mm 0,1]$, il suffit de démontrer que 
				\[      K'' = \int_{1}^{+\infty}\frac{e^{is}}{\sqrt{s}}ds     \]
				converge. 
				Or, une intégration par parties avec $u(s) = 1/\sqrt{s}$ et $v'(s) = e^{is}$ nous dit que 
				\[      K'' = \int_{1}^{+\infty}\frac{e^{is}}{\sqrt{s}}ds = \underset{M \rightarrow + \infty}{\lim}  \bigg[ \frac{e^{is}}{i \sqrt{s}} \bigg]_{1}^{M} + \frac{1}{2i} \int_{1}^{+\infty}\frac{e^{is}}{\sqrt{s^{3}}}ds = i e^{i} + \frac{1}{2i} \int_{1}^{+\infty}\frac{e^{is}}{\sqrt{s^{3}}}ds.      \]
				Comme $|e^{is}|/\sqrt{s^{3}} = 1/ \sqrt{s^{3}}$ est absolument intégrable sur $\RR_{>1}$, on conclut que $K''$ converge. 
				En conséquence, $I$ et $J$ convergent. 
				
				\item On voit bien que 
				\[     \frac{2}{\sqrt{\pi}}\int_0^{+\infty}e^{-tx^2}dx = \frac{2}{\sqrt{\pi}\sqrt{t}}\int_0^{+\infty}e^{-y^2}dy = \frac{1}{\sqrt{t}},     \]
				où l'on a fait la substitution $y = \sqrt{t} x$. 
				
				\item On note que l'application $x \mapsto e^{it}e^{-tx^2}$ est absolument intégrable sur $\RR_{> 0}$, pour tout $t \in \RR_{>0}$, 
				et l'application $t \mapsto \int_{0}^{+\infty} |e^{it}e^{-tx^2}| dx$ est intégrable sur $[A,B]$. 
				La première identité de cet item est alors une conséquence du théorème de Fubini. 
				En plus, 
				\begin{align*}
				\frac{2}{\sqrt{\pi}}\int_0^{+\infty}\bigg(\int_A^B e^{it}e^{-tx^2} dt\bigg)dx &= \frac{2}{\sqrt{\pi}} \int_A^B\bigg(\int_0^{+\infty}e^{it}e^{-tx^2}dx\bigg)dt 
				\\
				&= \frac{2}{\sqrt{\pi}} \int_A^B e^{it} \bigg(\int_0^{+\infty}e^{-tx^2}dx\bigg)dt = \int_A^B\frac{e^{it}}{\sqrt{t}}dt.     
				\end{align*} 
				
				\item On voit que 
				\begin{equation}
				\label{eq:ab}
				\int_A^B e^{it}e^{-tx^2} dt = \bigg[ \frac{e^{t(i-x^2)}}{(i-x^2)} \bigg]_{A}^{B} = \frac{e^{B(i-x^2)} - e^{A(i-x^2)}}{(i-x^2)}    
				\end{equation}      
				et sa valeur absolue est majorée par $(e^{- B x^{2}} + e^{- A x^{2}} )/|i-x^{2}|$, qui est majorée par $2/|i-x^{2}|$, qui est intégrable sur $\RR_{>0}$. 
				La limite simple de \eqref{eq:ab} quand $A$ tend vers zéro et $B$ tend vers $+ \infty$ est $1/(i-x^{2}) = (i+x^{2})/(1+x^{4})$. 
				En conséquence, le théorème de convergence dominée nous dit que 
				\[     \int_0^{+\infty}\frac{e^{it}}{\sqrt{t}}dt=\frac{2}{\sqrt{\pi}} \underset{A \rightarrow 0}{\lim} \underset{B \rightarrow + \infty}{\lim} \int_0^{+\infty}\bigg(\int_A^Be^{it}e^{-tx^2} dt\bigg)dx = \frac{2}{\sqrt{\pi}} \int_0^{+\infty} \frac{i+x^2}{1+x^4} dx.     \]
				
				\item
				Comme les racines de $1 + u^{4}$ sont les racines primitives de l'unité d'ordre $4$, on voit que 
				\[     u^{4} + 1 = (u - \zeta) (u - \zeta^{3}) (u - \zeta^{5}) (u - \zeta^{7}),     \]
				où $\zeta = e^{2 \pi i / 8} = e^{i \pi/4}$. 
				Comme $\zeta^{7} = \bar{\zeta}$ et $\zeta^{5} = \bar{\zeta}^{3}$, on conclut que 
				\begin{align*} 
				u^{4} + 1 &= (u - \zeta) (u - \bar{\zeta}) (u - \zeta^{3}) (u - \bar{\zeta}^{3}) = (u^{2} - 2 \operatorname{Re}(\zeta) u + 1) (u^{2} - 2 \operatorname{Re}(\zeta^{3}) u + 1)) 
				\\
				&= (u^{2} - \sqrt{2} u + 1) (u^{2} + \sqrt{2} u + 1),      
				\end{align*}
				où l'on a utilisé que $\operatorname{Re}(\zeta) = \cos(\pi/4) = \sqrt{2}/2$ et $\operatorname{Re}(\zeta^{3}) = \cos(3\pi/4) = -\sqrt{2}/2$. 
				En outre, on peut vérifier que 
				\begin{align*}
				\frac{1}{u^{4}+1} &= - \frac{1}{4 \sqrt{2}} \frac{2u - \sqrt{2}}{(u^{2}-\sqrt{2}u+1)} + \frac{1}{4 \sqrt{2}} \frac{2u + \sqrt{2}}{(u^{2}+\sqrt{2}u+1)} 
				\\
				&+ \frac{1}{4 \sqrt{2}} \frac{\sqrt{2}}{(u^{2}-\sqrt{2}u+1)} + \frac{1}{4 \sqrt{2}} \frac{\sqrt{2}}{(u^{2}+\sqrt{2}u+1)}.     
				\end{align*}
				En conséquence, 
				\begin{align*}    
				\int_0^{+\infty} \frac{1}{1+u^4} du &= \underset{M \rightarrow + \infty}{\lim} \frac{1}{4 \sqrt{2}}\bigg[ \ln\bigg(\bigg|\frac{u^{2}+\sqrt{2}u+1}{u^{2}-\sqrt{2}u+1}\bigg|\bigg) 
				\\
				&+ 2 \arctan(\sqrt{2} u - 1) + 2 \arctan(\sqrt{2} u + 1)\bigg]_{0}^{M} = \frac{\pi}{2 \sqrt{2}}.     
				\end{align*}
				De façon analogue, on trouve que 
				\begin{align*}
				\frac{u^{2}}{u^{4}+1} &= \frac{1}{4 \sqrt{2}} \frac{2u - \sqrt{2}}{(u^{2}-\sqrt{2}u+1)} - \frac{1}{4 \sqrt{2}} \frac{2u + \sqrt{2}}{(u^{2}+\sqrt{2}u+1)} 
				\\
				&+ \frac{1}{4 \sqrt{2}} \frac{\sqrt{2}}{(u^{2}-\sqrt{2}u+1)} + \frac{1}{4 \sqrt{2}} \frac{\sqrt{2}}{(u^{2}+\sqrt{2}u+1)}.     
				\end{align*}
				et
				\begin{align*}
				\int_0^{+\infty} \frac{u^{2}}{1+u^4} du &= \underset{M \rightarrow + \infty}{\lim} \frac{1}{4 \sqrt{2}}\bigg[ \ln\bigg(\bigg|\frac{u^{2}-\sqrt{2}u+1}{u^{2}+\sqrt{2}u+1}\bigg|\bigg) 
				\\
				&+ 2 \arctan(\sqrt{2} u - 1) + 2 \arctan(\sqrt{2} u + 1)\bigg]_{0}^{M} = \frac{\pi}{2 \sqrt{2}}.     
				\end{align*}
				Cela implique que $I' = J' = \sqrt{\pi/2}$ et 
				\[     I=J=\frac{1}{2} \sqrt{\frac{\pi}{2}}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Soient $A_R=[0,R]\times[0,R]$ et $D_R=\{(x,y)\in\RR^2_{>0} : x^2+y^2\leq R^2\}$.
		Calculer les limites des intégrales
		\[     \iint_{A_R}\sin\big(x^2+y^2\big)dxdy \text{ et } \iint_{D_R}\sin\big(x^2+y^2\big)dxdy     \]
		quand $R$ tend vers $+ \infty$, si elles existent. 
		
		\begin{preuve}
			On voit bien que 
			\begin{align*}  
			\iint_{D_R}\sin\big(x^2+y^2\big)dxdy &= \int_{0}^{R} \int_{0}^{\pi/2} \sin(r^{2}) r d\theta dr = \frac{\pi}{4} \bigg[ - \cos(r^{2}) \bigg]_{0}^{R} 
			\\
			&= \frac{\pi}{4} \big(1 - \cos(R^{2})\big),     
			\end{align*}
			ce qui implique que 
			\[     \underset{R \rightarrow + \infty}{\lim} \iint_{D_R}\sin\big(x^2+y^2\big)dxdy      \]
			n'existe pas. 
			
			Par ailleurs, 
			\begin{align*}
			\iint_{A_R}\sin\big(x^2+y^2\big)dxdy &= \int_{0}^{R} \int_{0}^{R} \Big( \sin\big(x^2\big) \cos\big(y^2\big) + \sin\big(y^2\big) \cos\big(x^2\big) \Big) dx dy 
			\\ 
			&= 2 \int_{0}^{R} \int_{0}^{R} \sin\big(x^2\big) \cos\big(y^2\big) dx dy 
			\\
			&= 2 \bigg(\int_{0}^{R}  \sin\big(x^2\big) dx\bigg) \bigg(\int_{0}^{R}\cos\big(y^2\big) dy\bigg).     
			\end{align*}
			L'exercice précédent nous dit que la limite quand $R$ tend vers $+ \infty$ existe et elle vaut $2 . I . J = \pi/4$.
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Calcul d'intégrales doubles à l'aide de lignes de niveau}
		\begin{enumerate}
			\item Soit $\Delta \subseteq \RR^{2}$ un fermé et $F:\Delta \rightarrow \RR$ une fonction continue. 
			Soient $a \leq b$ deux réels fixes. 
			On suppose que $\Omega = \{ (x,y) \in \Delta : a \leq F(x,y)\leq b \}$ est un compact de $\RR^2$. 
			
			Pour tout $k\in [a,b]$, on définit $\Omega_k=\{ (x,y) \in \Delta : a\leq F(x,y)\leq k\}$. 
			On considère les fonctions $A, I : [a,b] \rightarrow \RR$ 
			données par 
			\[     A(k)=\iint_{\Omega _k}dxdy \text{ et } I(k)=\iint_{\Omega _k}F(x,y)dxdy.     \] 
			On suppose que la fonction $A$ est dérivable sur $[a,b]$ avec dérivée continue.  
			Montrer que $I$ est dérivable de dérivée $I'(k)=kA'(k)$. 
			En déduire que  
			\[     I(b)=\iint_{\Omega}F(x,y)dxdy=\int_a^b I'(k)dk.     \]
			
			\item On va considérer l'application suivante. 
			Calculer 
			\[     \iint_\Omega \bigg( \frac{x^2}{\alpha^2}+\frac{y^2}{\beta^2}\bigg) dxdy \text{ et } 
			\iint_D \frac{dxdy}{(1+x+y)^3},     \]
			où 
			\[     \Omega = \big\{ (x,y)\in \RR^2 : x^2/\alpha^2+y^2/\beta^2 \leq 1 \big\} \text{ et } D = \big\{ (x,y)\in \RR^2_{\geq 0} : x+y\leq 1\big\}.     \]
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item 
				On écrit 
				\[     \frac{I(k+h) - I(k)}{h} = \frac{\iint_{\Omega_{k+h} \setminus \Omega_{k}^{\circ}} F(x,y) dx dy}{h}.     \]
				Comme  
				\begin{align*}
				k \frac{A(k+h) - A(k)}{h} &= k \frac{\iint_{\Omega_{k+h} \setminus \Omega_{k}^{\circ}} dx dy}{h} \leq \frac{\iint_{\Omega_{k+h} \setminus \Omega_{k}^{\circ}} F(x,y) dx dy}{h} 
				\\
				&\leq (k+h) \frac{\iint_{\Omega_{k+h} \setminus \Omega_{k}^{\circ}} dx dy}{h} = (k+h)  \frac{A(k+h) - A(k)}{h},
				\end{align*}          
				On conclut que la limite de $(I(k+h) - I(k))/h$ converge vers $k A'(k)$ quand $h$ tend vers zéro. 
				En conséquence, $I$ est dérivable et $I'(k) = k A'(k)$, pour tout $k \in [a,b]$. 
				La dernière égalité est une conséquence immédiate. 
				
				\item Pour la première intégrale on aura $\Delta = \RR^{2}$, $F(x,y) = (x/\alpha)^2+(y/\beta)^2$ et $[a,b] = [0,1]$. 
				Or, on sait que 
				\[     A_{\Omega}(k)=\iint_{\Omega_k}dxdy = \pi \alpha \beta k     \]
				(voir l'exercice \ref{exo:12}). 
				En conséquence, $A'_{\Omega}(k) = \pi \alpha \beta$ est une fonction constante. 
				D'après l'item précédent, on conclut que 
				\[     \iint_\Omega \bigg( \frac{x^2}{\alpha^2}+\frac{y^2}{\beta^2}\bigg) dxdy = \int_0^1 \pi \alpha \beta k dk= \frac{\pi \alpha \beta}{2}.     \]
				
				En outre, pour la deuxième intégrale on aura $\Delta = \RR^{2}_{\geq 0}$, $F(x,y) = 1/(1+x+y)^3$ et $[a,b] = [1/8,1]$. 
				C'est facile à voir que 
				\[     A_{D}(k)=\iint_{D_k}dxdy = \frac{1}{\sqrt[3]{k}} - \frac{1}{2\sqrt[3]{k^{2}}}.     \]
				En conséquence, $A'_{D}(k) = (k^{-5/3} - k^{-4/3})/3$. 
				D'après l'item précédent, on conclut que 
				\[     \iint_D \frac{dxdy}{(1+x+y)^3} = \int_{1/8}^{1} \frac{k^{-2/3} - k^{-1/3}}{3} dk= \frac{1}{8}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Montrer que 
		\[     -\frac{1}{2} = \int_0^1\int_0^1\frac{x-y}{(x+y)^3}dx dy \neq \int_0^1\int_0^1\frac{x-y}{(x+y)^3}dy dx = \frac{1}{2}.     \]
		Y a-t-il une contradiction avec le théorème de Fubini ?
		\begin{preuve}
			On voit bien que 
			\begin{align*}
			\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dx dy &= \int_0^1 \int_0^1 \bigg( \frac{1}{(x+y)^2} - \frac{2 y}{(x+y)^3} \bigg) dx dy 
			\\
			&= \int_0^1 \bigg[ -\frac{1}{x+y} + \frac{y}{(x+y)^2} \bigg]_{0}^{1} dy 
			\\
			&= - \int_0^1 \frac{1}{(1+y)^{2}} dy = \bigg[ \frac{1}{1+y} \bigg]_{0}^{1} = -\frac{1}{2},     
			\end{align*}
			tandis que 
			\begin{align*}
			\int_0^1\int_0^1\frac{x-y}{(x+y)^3}dy dx &= - \int_0^1 \int_0^1 \bigg( \frac{1}{(x+y)^2} - \frac{2 x}{(x+y)^3} \bigg) dy dx 
			\\
			&= - \int_0^1 \bigg[ -\frac{1}{x+y} + \frac{x}{(x+y)^2} \bigg]_{0}^{1} dx 
			\\
			&= \int_0^1 \frac{1}{(1+x)^{2}} dx = - \bigg[ \frac{1}{1+x} \bigg]_{0}^{1} = \frac{1}{2}.     
			\end{align*}
			Il n'y a aucune contradiction avec le théorème de Fubini. 
			Pour le voir, on note $f : [0,1]^{2} \setminus \{ (0,0) \} \rightarrow \RR$ l'application $f(x,y) = (x-y)/(x+y)^3$ et $f_{y}$ celle donnée par $x \mapsto (x-y)/(x+y)^3$. 
			Pour tout $y \in \hskip 0.6mm ] \hskip 0.6mm 0,1]$, l'application $|f_{y}|$ donnée par $x \mapsto |(x-y)/(x+y)^3|$ est intégrable sur $[0,1]$. 
			Par contre, l'application $y \mapsto \int_{0}^{1} |f_{y}(x)| dx$ n'est pas intégrable. 
			En effet, 
			\begin{align*}
			\int_0^1\int_0^1 \bigg|\frac{x-y}{(x+y)^3}\bigg| dx dy &\geq  \int_0^1 \int_0^y \frac{|x-y|}{(x+y)^3} dx dy = 
			\int_0^1 \int_0^y \frac{y-x}{(x+y)^3} dx dy 
			\\ 
			& = \int_0^1 \bigg[ \frac{1}{x+y} - \frac{y}{(x+y)^2} \bigg]_{0}^{y}dy = \int_0^1 \frac{1}{4y} dy
			\end{align*}
			diverge. 
		\end{preuve}
		
		%%%%%%%%%%%%%%
		
		\exo{} Dans cet exercice, on s'intéresse à la transformée de Fourier $F$ de la fonction Gaussienne $\begin{array}{ccccc}
		f & : & \RR & \to & \RR \\
		& & x & \mapsto & e^{-\pi x^2}
		\end{array}$.
		
		\begin{enumerate}
			
			\item En appliquant le théorème de Fubini, montrer que
			\begin{equation*}
			\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{-\pi(x^2 + y^2)} dx dy = \left (\int_{-\infty}^{+\infty} f(x)dx \right)^2.
			\end{equation*}
			
			\item En déduire la valeur de $\int_{-\infty}^{+\infty} f(x)dx$.
			
			\item Soit $\begin{array}{ccccc}
			F & : & \RR & \to & \RR \\
			& & \xi &\mapsto & \int_{-\infty}^{+\infty} e^{-\pi x^2} e^{-2\pi i x \xi} dx
			\end{array}$.
			
			\begin{enumerate}
				
				\item Montrer que $F$ est dérivable et que, pour tout $\xi \in \RR$, 
				\begin{equation*}
				F'(\xi) = i \int_{-\infty}^{+\infty} f'(x) e^{-2\pi i x \xi } dx.
				\end{equation*}
				
				\item Montrer que $\int_{-\infty}^{+\infty} f'(x) e^{-2\pi i x \xi } dx = 2 \pi i \xi \int_{-\infty}^{+\infty} f(x) e^{-2\pi i x \xi } dx$
				
				\item En déduire que $F(\xi) = e^{-\pi \xi^2}$ pour tout $\xi \in \RR$.
				
			\end{enumerate}
			
		\end{enumerate}
		
		%%%%%%%%%%%%%%%
		\exo{}  Soit $P$ un rectangle de $\RR^2$. 
		On appelle \emph{partie pavable} de $P$ toute réunion finie de rectangles inclus dans $P$.
		On note $\mathcal{P}$ l'ensemble des parties pavables de $P$.
		
		\begin{enumerate} 
			\item Montrer que si $A_1$ et $A_2$ appartiennent à $\mathcal{P}$, alors $A_1\cup A_2$ appartient à $\mathcal{P}$ et $P\setminus A_1$ aussi. 
			Faire des dessins.
			Montrer de plus que tout élément de $\mathcal{P}$ peut s'écrire comme réunion finie de rectangles deux à deux disjoints.
			
			\item Soit $X$ une partie de $\RR^2$ incluse dans $P$. 
			On note
			\[     J^+(X)=\inf\big\{\operatorname{mes}(A) : A \in \mathcal{P}, X \subseteq A \big\} 
			\text{ et } J^-(X)=\sup \big\{\operatorname{mes}(A) : A \in \mathcal{P}, A\subseteq X\big\}.     \]
			Montrer que $J^+(X)\geq I^+(\mathbb{1}_X)$ et $ J^-(X)\leq I^-(\mathbb{1}_X)$.
			
			\item Montrer que si $v$ est un fonction en escalier telle que $v \geq \mathbb{1}_X$, alors il existe $A\in \mathcal{P}$ tel que $v\geq \mathbb{1}_A \geq \mathbb{1}_X$.
			En déduire que $J^+(X)=I^+(\mathbb{1}_X)$. 
			De manière analogue, montrer que $J^-(X)= I^-(\mathbb{1}_X)$.
			
			\item Montrer que $X$ est cubable si et seulement si $J^+(X)=J^-(X)$. 
			Si $X$ est cubable, alors $\operatorname{mes}(X)=J^+(X)=J^-(X)$.
			
			\item $X$ est négligeable si pour tout $\epsilon>0$, il existe $A \in \mathcal{P}$ tel que $X \subseteq A$ et $\operatorname{mes}(A) \leq \epsilon$.
			Montrer qu'un ensemble négligeable est cubable de mesure nulle.
			
			\item Soit $\varphi : [0,1]\rightarrow \RR^2$ telle que il existe $M \geq 0$ tel que 
			\[     \| \varphi(s)-\varphi(t) \|_\infty \leq M|s-t|,     \]
			pour tout $s,t\in [0,1]$. 
			Montrer que $\varphi([0,1])$ est négligeable.
			
			\item Soit $D$ une partie fermée de $P$ telle que la frontière de $D$ soit donnée par une courbe $C^1$ par morceaux. 
			Notons $X$ la frontière de $D$. Soit $\epsilon>0$. 
			D'après la question précédente, il existe $A \in \mathcal{P}$ tel que $X \subseteq A$ et $\operatorname{mes}(A) \leq \epsilon$. Alors $P \setminus A$ est une réunion de rectangles $R_1,\dots,R_N$. 
			
			Montrer par un raisonnement par l'absurde et en utilisant un argument de connexité que, pour tout $i$, $R_i\subseteq D$ ou $R_i\subseteq P\setminus D$.
			
			En déduire qu'il existe $I \subseteq \{1,\ldots,N\}$ tel que 
			\[     \bigcup_{i\in I}R_i\subseteq D\subseteq  \bigcup_{i\in I}R_i\cup A.     \]
			Montrer que $D$ est cubable.
		\end{enumerate}
		
		\begin{preuve}
			La solution suit directement des indications détaillées dans l'exercice. 
		\end{preuve} 
		
		
	

\newpage \section{Espaces probabilisés, variables aléatoires discrètes}
	
	
		
		\subsection{Espaces de probabilité}
		
		%%%%%%%%%%%%%%%
		\exo{} Soit $\mathbb{P}$ une probabilité sur $(\NN,\mathscr{P}(\NN))$, où l'on rappelle que, étant donné un ensemble 
		$\Omega$, $\mathscr{P}(\Omega) = \{ A : A \subseteq \Omega\}$. 
		Montrer que $\mathbb{P}(\{n\})$ tend vers 0 quand $n$ tend vers $+\infty$.
		
		\begin{preuve}
			D'après la définition d'espace probabilisé, on voit bien que 
			\[     1 = \mathbb{P}(\NN) = \mathbb{P}\bigg(\bigsqcup_{n \in \NN} \{ n \}\bigg) = \sum_{n \in \NN} \mathbb{P}\big(\{n\}\big).     \]
			Comme la dernière série est (absolument) convergente, $\mathbb{P}(\{n\})$ tend vers 0 quand $n$ tend vers $+\infty$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo 
		Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace de probabilité et soient $A, B \in \mathscr{A}$. 
		Montrer que $\mathbb{P}(A\cap B) \leq \operatorname{min}(\mathbb{P}(A),\mathbb{P}(B))$. 
		
		\begin{preuve}
			Il suffit de montrer que, étant donnés $A, C \in \mathscr{A}$ tels que $C \subseteq A$, alors 
			$\mathbb{P}(C) \leq \mathbb{P}(A)$. 
			En effet, comme $A = C \sqcup (A \setminus C)$, alors $\mathbb{P}(C) + \mathbb{P}(A\setminus C) = \mathbb{P}(A)$, 
			ce qui nous dit que $\mathbb{P}(C) \leq \mathbb{P}(A)$, vu que $\mathbb{P}(A \setminus C) \geq 0$.
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		On a un alphabet de 5 lettres $\{a,b,c,d,e\}$ et on considère l'ensemble des mots de $25$ lettres.
		On tire au hasard un mot dans cet ensemble.
		Quelle est la probabilité qu'il comporte $5$ $a$, $5$ $b$, $5$ $c$, $5$ $d$ et $5$ $e$ ?
		
		\begin{preuve}
			L'univers de l'espace probabilisé est $\Omega = \{a,b,c,d,e\}^{25}$, avec tribu $\mathscr{P}(\Omega)$. 
			Cet espace est équiprobable, \textit{i.e.} la probabilité $\mathbb{P}$ est définie classiquement par
			\[     \mathbb{P}(A) = \frac{\#(A)}{\#(\Omega)},     \]
			pour tout $A \subseteq \Omega$. 
			C'est clair que $\#(\Omega) = 5^{25}$.
			Pour tout $\bar{x} = (x_{1},\dots,x_{25}) \in \Omega$ et $\ell \in \{a,b,c,d,e\}$, on définie $I_{\ell}(\bar{x}) = \{ i \in [\![ 1, 25 ]\!] : x_{i} = \ell \}$ et $N_{\ell}(\bar{x}) = \#(I_{\ell}(\bar{x}))$, où l'on rappelle que $[\![ i, j ]\!] = \{ n \in \Z : i \leq n \leq j \}$, pour tout $i, j \in \Z$. 
			L'évènement demandé est donné par 
			\[     E = \big\{ \bar{x} \in \Omega : N_{\ell}(\bar{x}) = 5, \text{ pour tout }\ell \in \{a,b,c,d,e\} \big\}.     \]
			Pour $n \in \NN$ et $X$ un ensemble, on écrit $\mathscr{P}_{n}(X) = \{ A : A \subseteq X \text{ et } \#(A) = n \}$.
			On voit bien que l'application $\varphi : E \rightarrow \mathscr{P}_{5}([\![ 1, 25 ]\!])^{5}$ donnée par
			\[     \varphi(\bar{x}) = \big(I_{a}(\bar{x}),I_{b}(\bar{x}),I_{c}(\bar{x}),I_{d}(\bar{x}),I_{e}(\bar{x})\big)     \]
			est injective, avec image formée par les uplets $(J_{a},J_{b},J_{c},J_{d},J_{e}) \in \mathscr{P}_{5}([\![ 1, 25 ]\!])^{5}$ tels 
			que 
			\[     \bigsqcup_{\ell \in \{a,b,c,d,e\}} J_{\ell} = [\![ 1, 25 ]\!].     \]
			C'est clair que l'image de $\varphi$ a cardinal $25!/(5!)^{5}$, et en conséquence 
			\[     \mathbb{P}(E) = \frac{25!}{(5!)^{5} 5^{25}}.     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} 
		On fait $2$ lancers avec trois dés.
		Quelle est la probabilité d'avoir les mêmes résultats si 
		\begin{enumerate}
			\item les dés sont de trois couleurs différentes ? (Dans ce cas le résultat d'un lancer est un triplet donnant le résultat de chaque dé)
			
			\item les dés sont indiscernables ? (Dans ce cas le résultat d'un lancer est l'ensemble des résultats avec leur multiplicité, par exemple deux $1$ et un $5$)
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item L'univers de l'espace probabilisé est $\Omega_{a} = [\![ 1, 6 ]\!]^{6}$, avec tribu $\mathscr{P}(\Omega_{a})$. 
				Cet espace est équiprobable. 
				C'est clair que $\#(\Omega_{a}) = 6^{6}$.
				Un élément $(\bar{x},\bar{y}) = (x_{1},x_{2},x_{3},y_{1},y_{2},y_{3}) \in \Omega_{a}$ (\textit{i.e.} $\bar{x}, \bar{y} \in [\![ 1, 6 ]\!]^{3}$) représente la situation où le $i$-ème dé a la face marquée par $x_{i}$ au premier lancer et $y_{i}$ au deuxième lancer.    
				L'évènement demandé est donné par 
				\[     E_{a} = \big\{ (\bar{x},\bar{y}) \in \Omega_{a} : \bar{x} = \bar{y} \big\}.     \]
				On voit bien que l'application $\varphi : E_{a} \rightarrow [\![ 1, 6 ]\!]^{3}$ donnée par
				\[     \varphi(\bar{x},\bar{y}) = \bar{x}    \]
				est bijective. 
				En conséquence $\#(E_{a}) = 6^{3}$ et 
				\[     \mathbb{P}(E_{a}) = \frac{1}{6^{3}}.     \]
				
				\item On définit $X = \{ \bar{x} = (x_{1},\dots,x_{6}) \in \NN^{6} : \sum_{i=1}^{6} x_{i} = 3 \}$. 
				Pour compter les éléments $\bar{x}$ de $X$, on divise en trois cas (disjoints): 
				\begin{enumerate}[label=(\roman*)]
					\item il existe $i \in [\![ 1 , 6 ]\!]$ tel que $x_{i} = 3$;
					\item il existe $i \in [\![ 1 , 6 ]\!]$ tel que $x_{i} = 2$;
					\item pour tout $i \in [\![ 1 , 6 ]\!]$, $x_{i} \leq 1$.
				\end{enumerate}
				On a $C_{i} = 6$ éléments dans le cas (i), $C_{ii} = 6.5 = 30$ éléments dans le cas (ii) et $C_{iii} = 6!/(3! 3!) = 20$ dans le dernier. 
				On considère $X$ muni de la tribu $\mathscr{P}(X)$. 
				Par contre, il ne s'agit pas d'un espace équiprobable, si l'on veut qu'il modélise le tirage de $3$ dés indiscernables: la probabilité $\mathbb{P}'(\{\bar{x}\})$ d'un élément $\bar{x}$ dans le cas (i) est $1/6^{3}$, celle d'un élément $\bar{x}$ dans (ii) est $3/6^{3}$ et la probabilité d'un élément $\bar{x}$ du type (iii) est $1/6^{2}$. 
				On vérifie bien que 
				\[     \sum_{\bar{x} \in X} \mathbb{P}'\big(\{\bar{x}\}\big) = C_{i} \frac{1}{6^{3}} + C_{ii} \frac{3}{6^{3}} + C_{iii} \frac{1}{6^{2}} = 1.     \]
				L'univers de l'espace probabilisé est $\Omega_{b} = X \times X$, avec tribu $\mathscr{P}(\Omega_{b})$. 
				La probabilité $\mathbb{P}(\{ (\bar{x},\bar{y})\} )$ de $(\bar{x},\bar{y}) \in \Omega_{b}$ est $\mathbb{P}'(\{\bar{x}\}) \mathbb{P}'(\{\bar{y}\})$. 
				L'évènement demandé est donné par 
				\[     E_{b} = \big\{ (\bar{x},\bar{y}) \in \Omega_{b} : \bar{x} = \bar{y} \big\}.     \]
				Dans ce cas, on trouve 
				\[     \mathbb{P}(E_{b}) = \mathbb{P}\bigg( \bigcup_{\bar{x} \in X} \big\{ (\bar{x},\bar{x})\big\} \bigg) 
				= \sum_{\bar{x} \in X} \mathbb{P}'\big(\{\bar{x}\}\big)^{2} = C_{i} \Big( \frac{1}{6^{3}} \Big)^{2} + C_{ii} \Big( \frac{3}{6^{3}} \Big)^{2} + C_{iii} \Big( \frac{1}{6^{2}} \Big)^{2} = \frac{83}{3.6^{4}}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		Après avoir bien mélangé un jeu de $52$ cartes, on en fait une pile.
		\begin{enumerate}
			\item Quelle est la probabilité que le $2$ de c{\oe}ur soit à la dernière place ?
			
			\item Quelle est la probabilité que l'as de pique se trouve au dessus de l'as de c{\oe}ur ? 
			
			\item Quelle est la probabilité que l'as de pique soit au dessus de l'as de c{\oe}ur et que celui-ci soit au dessus de l'as de carreau ?
			
			\item Quelle est la probabilité que l'as de pique et l'as de trèfle ne soient pas adjacents ?
		\end{enumerate}
		
		\begin{preuve}
			On suppose que les cartes son numérotées de $1$ à $52$: par exemple, l'as de pique est le $1$, l'as de c{\oe}ur est le $2$, l'as de carreau est le $3$, l'as de trèfle est le $4$, le $2$ de pique est le $5$, le $2$ de pique est le $6$, etc. 
			La position d'une carte dans la pile est déterminée de façon relative à la table sur laquelle la pile est posée, la première position étant celle du dessous et la dernière celle du dessus.
			L'univers de l'espace probabilisé est $\Omega = \mathbb{S}_{52} = \{ \sigma : [\![ 1, 52 ]\!] \rightarrow [\![ 1, 52 ]\!] : \sigma \text{ est bijectif} \}$, avec tribu $\mathscr{P}(\Omega)$. 
			Cet espace est équiprobable. 
			L'élément $\sigma \in \Omega$ représente la situation où la $i$-ème carte est dans la $\sigma(i)$-ème position dans la pile. 
			C'est clair que $\#(\Omega) = 52!$. 
			
			\begin{enumerate}
				\item L'évènement demandé est donné par 
				\[     E_{a} = \big\{ \sigma \in \Omega : \sigma(6) = 52 \big\}.     \]
				C'est clair que $\#(E_{a}) = 51!$, ce qui implique que 
				\[     \mathbb{P}(E_{a}) = \frac{1}{52}.     \]
				
				\item  L'évènement demandé est donné par 
				\[     E_{b} = \big\{ \sigma \in \Omega : \sigma(1) > \sigma(2) \big\}.     \]
				On voit bien que l'application $\varphi : E_{b} \rightarrow \Omega \setminus E_{b}$ donnée par
				\[     \varphi(\sigma) = \sigma \circ (12)    \]
				est bijective, où $(12) \in \Omega$ est la transposition qui envoie $1$ dans $2$ et $2$ dans $1$.
				En conséquence 
				\[     \#(\Omega) = \#(E_{b}) + \#(\Omega \setminus E_{b}) = 2 \#(E_{b}),     \]
				ce qui implique que 
				\[     \mathbb{P}(E_{b}) = \frac{1}{2}.     \]
				
				\item L'évènement demandé est donné par 
				\[     E_{c} = \big\{ \sigma \in \Omega : \sigma(1) > \sigma(2) > \sigma(3) \big\}.     \]
				Pour tout $\tau \in \mathbb{S}_{3}$, on défini l'application $\tau_{*} : E_{c} \rightarrow \Omega$ donnée par
				\[     \tau_{*}(\sigma) = \sigma \circ \tau.    \]
				On voit bien qu'elle est injective. 
				Soit $E_{c}^{\tau}$ l'image de $\tau_{*}$. 
				On note que $E_{c}^{\tau} = E_{c}$, si $\tau \in \mathbb{S}_{3}$ est l'identité, et que $\#(E_{c}^{\tau}) = \#(E_{c})$, pour tout $\tau \in \mathbb{S}_{3}$. 
				De plus, 
				\[     \bigsqcup_{\tau \in \mathbb{S}_{3}} E_{c}^{\tau} = \Omega,     \]
				ce qui implique que $\#(\Omega) = \#(\mathbb{S}_{3}). \#(E) = 6 \#(E)$.  
				En conséquence 
				\[     \mathbb{P}(E_{c}) = \frac{1}{6}.     \]
				
				\item L'évènement complémentaire à l'évènement $E_{d}$ demandé est donné par 
				\[     \Omega \setminus E_{d} = \big\{ \sigma \in \Omega : | \sigma(1) - \sigma(4)| = 1 \big\}.     \]
				Soit $\Omega' = \{ \sigma : [\![ 2, 52 ]\!]  \rightarrow [\![ 2, 52 ]\!] : \sigma \text{ est bijectif} \}$. 
				C'est clair que $\#(\Omega') = 51!$.  
				On considère l'application $\varphi_{1} : \Omega \setminus E_{d}  \rightarrow \Omega'$ donnée par
				\[     \varphi_{1}(\sigma)(i) = \begin{cases}
				\sigma(i)+1, &\text{si $\sigma(i) \leq \operatorname{min}\big(\sigma(1), \sigma(4)\big)$,}
				\\
				\sigma(4), &\text{si $i=4$ et $\sigma(1) < \sigma(4)$,}
				\\
				\sigma(4)+1, &\text{si $i=4$ et $\sigma(1) > \sigma(4)$,}
				\\
				\sigma(i), &\text{si $\sigma(i) > \operatorname{max}\big(\sigma(1), \sigma(4)\big)$,}
				\end{cases}
				\]
				pour tout $i \in [\![ 2 , 52 ]\!]$. 
				Soit l'application $\varphi_{2} : \Omega \setminus E_{d}  \rightarrow \{ \pm 1\}$ donnée par 
				$\psi(\sigma) = \operatorname{sgn}(\sigma(1) - \sigma(4))$. 
				On voit bien que l'application $\varphi : \Omega \setminus E_{d}  \rightarrow  \Omega' \times \{ \pm 1\}$ définie via 
				$\varphi(\sigma) = (\varphi_{1}(\sigma),\varphi_{2}(\sigma))$ est bijective. 
				Cela implique que $\#(\Omega \setminus E_{d}) = 2. 51!$, ce qui nous dit que 
				\[     \mathbb{P}(E_{d}) = 1 - \frac{2}{52} = \frac{25}{26}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Premier problème du chevalier de Méré}
		Quel est le plus probable: jouant avec un dé, obtenir au moins une
		fois $6$ en $4$ coups, ou bien jouant avec deux dés (discernables), obtenir au moins une
		fois un double $6$ en $24$ coups?
		
		\begin{preuve}
			L'univers de l'espace probabilisé dans la première situation est $\Omega_{1} = [\![ 1, 6 ]\!]^{4}$, avec tribu $\mathscr{P}(\Omega_{1})$. 
			Cet espace est équiprobable. 
			C'est clair que $\#(\Omega_{1}) = 6^{4}$. 
			Un élément $\bar{x} = (x_{1},\dots,x_{4}) \in \Omega$ représente la situation où l'on a obtenu la face marquée par $x_{i}$ au $i$-ème lancer.    
			L'évènement complémentaire à l'évènement $E_{1}$ demandé est donné par 
			\[     \Omega_{1} \setminus E_{1} = [\![ 1, 5 ]\!]^{4}.     \]
			En conséquence $\#(\Omega_{1} \setminus E_{1}) = 5^{4}$ et 
			\[     \mathbb{P}(E_{1}) = 1 - \frac{5^{4}}{6^{4}}.     \]
			
			L'univers de l'espace probabilisé dans la deuxième situation est $\Omega_{2} = [\![ 1, 6 ]\!]^{48}$, avec tribu $\mathscr{P}(\Omega_{2})$. 
			Cet espace est équiprobable. 
			C'est clair que $\#(\Omega_{2}) = 6^{48}$. 
			Un élément $\bar{x} = (x_{1},\dots,x_{48}) \in \Omega$ représente la situation où l'on a obtenu la face marquée par $x_{2i-1}$ au $i$-ème lancer pour le premier dé et la face marquée par $x_{2i}$ au $i$-ème lancer pour le deuxième dé.    
			L'évènement complémentaire à l'évènement $E_{2}$ demandé est donné par 
			\[     \Omega_{2} \setminus E_{2} = X^{24},     \]
			où $X = [\![ 1, 6 ]\!]^{2} \setminus \{ (6,6) \}$. 
			Comme $\#(X) = 35$, $\#(\Omega_{2} \setminus E_{2}) = 35^{24}$ et 
			\[     \mathbb{P}(E_{2}) = 1 - \frac{35^{24}}{36^{24}}.     \]
			Un calcul simple nous dit que $\mathbb{P}(E_{1}) > \mathbb{P}(E_{2})$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Second problème du chevalier de Méré} 
		Le chevalier de Méré avait posé à Pascal le problème suivant: deux joueurs jouent à un jeu de hasard en plusieurs
		parties; celui qui, le premier, a gagné trois parties emporte la totalité de l'enjeu. 
		On considère que la probabilité de gagner une partie est la même pour chaque joueur . 
		Si les joueurs doivent arrêter le jeu alors qu'il ne manque au premier joueur, pour l'emporter, qu'une
		partie, et au second que deux parties, comment doit-on répartir équitablement l'enjeu?
		
		\begin{preuve}
			On note $A$ et $B$ les deux joueurs. 
			On suppose que le joueur $A$ a gagné deux parties et que $B$ a gagné une seule partie. 
			Les possibles façons de finir le jeu seraient: 
			$A$, $BA$ et $BB$, où l'on écrit les lettres qui correspondent au joueur qui a gagné. 
			La probabilité de $A$ est $1/2$, celle de $BA$ est $1/4$ et celle de $BB$ est $1/4$. 
			Alors, la probabilité totale de l'évènement ``le joueur $A$ a emporté l'enjeu'' est $3/4$, tandis que probabilité totale de l'évènement ``le joueur $B$ a emporté l'enjeu'' est $1/4$. 
			Une façon équitable de faire la répartition de l'enjeu serait alors $75\%$ pour $A$ et $25\%$ pour $B$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Problème des anniversaires} 
		Quelle est la probabilité pour que $n$ personnes prises au hasard aient toutes des jours d'anniversaire différents? 
		On supposera que tous les jours de naissance sont équiprobables et on ne tiendra pas compte des années bissextiles.
		%Déterminer la plus petite valeur de $n$ telle que cette probabilité soit inférieure à $50\%$.
		
		\begin{preuve}
			L'univers de l'espace probabilisé est $\Omega = [\![ 1, 365 ]\!]^{n}$, avec tribu $\mathscr{P}(\Omega)$. 
			Cet espace est équiprobable. 
			C'est clair que $\#(\Omega) = 365^{n}$. 
			Un élément $\bar{x} = (x_{1},\dots,x_{n}) \in \Omega$ représente la situation où l'anniversaire de la $i$-ème personne est le $x_{i}$-ème jour de l'année. 
			L'évènement demandé est donné par 
			\[     E = \{ \bar{x} \in \Omega : x_{i} \neq x_{j}, \text{ pour tout } i \neq j \}.     \]
			C'est clair que $\#(E) = 0$, si $n > 365$, et $\#(E) = 365!/(365-n)!$, si $n \leq 365$. 
			Cela nous dit que $\mathbb{P}(E) = 0$ si $n > 365$, et 
			\[     \mathbb{P}(E) = 1 - \frac{365!}{(365-n)! 365^{n}}.     \]
			si  $n \leq 365$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Formule de Poincaré, problème des rencontres}
		\begin{enumerate}
			\item Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace probabilisé et soient 
			$A_1, \dots, A_n \in \mathscr{A}$ des événements. 
			Montrer que, si $A$ désigne la réunion de ces $n$ événements, alors
			\[     \mathbb{P}(A)=\sum_{k=1}^{n}(-1)^{k+1} \sum_{i_1<\dots<i_k} \mathbb{P}(A_{i_1} \cap \dots \cap A_{i_k}).     \]
			
			\item On tire sans remise $n$ boules numérotées de 1 à $n$. 
			Déterminer la probabilité $p_n$ pour qu'il existe un entier $k$ tel que la boule portant le numéro $k$ 
			soit tirée au tirage numéro $k$.
			
			\item Déterminer la limite de $p_n$ quand $n$ tend vers l'infini.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On va procéder par récurrence sur $n$. 
				Si $n = 1$, il n'y a rien à démontrer. 
				On suppose que l'identité de Poincaré est vérifié pour toute réunion de $n$ éléments. 
				Soient $A_1, \dots, A_{n+1} \in \mathscr{A}$ des événements, et soit $B = \cup_{i=1}^{n} A_{i}$.
				Alors, 
				\begin{align*}
				\mathbb{P}\bigg(\bigcup_{i=1}^{n+1} A_{i} \bigg) 
				&= \mathbb{P}(B \cup A_{n+1}) 
				= \mathbb{P}(B) + \mathbb{P}(A_{n+1}) - \mathbb{P}(B \cap A_{n+1}) 
				\\
				&= \sum_{k=1}^{n}(-1)^{k+1} \sum_{i_1<\dots<i_k} \mathbb{P}(A_{i_1} \cap A_{i_2} \cap \ldots\cap A_{i_k}) + \mathbb{P}(A_{n+1})
				\\
				& \phantom{=} - \sum_{k=1}^{n}(-1)^{k+1} \sum_{i_1<\dots<i_k} \mathbb{P}(A_{i_1} \cap A_{i_2} \cap \ldots\cap A_{i_k} \cap A_{n+1})
				\\
				&= \sum_{k'=1}^{n+1}(-1)^{k'+1} \sum_{j_1<\dots<j_{k'}} \mathbb{P}(A_{j_1} \cap \dots \cap A_{j_{k'}}),
				\end{align*}
				où l'on a utilisé que 
				\[     B \cap A_{n+1} = \bigcup_{i=1}^{n} (A_{i} \cap A_{n+1})     \]
				ainsi que l'hypothèse de la récurrence dans la troisième égalité. 
				Dans la dernière égalité on a utilisé que les opérandes dans la dernière somme avec $j_{k'} < n+ 1$ (et donc forcément $k' \leq n$) 
				correspondent précisément aux opérandes dans la première somme du quatrième membre, $\mathbb{P}(A_{n+1})$ correspond au cas 
				$k' = 1$ et $j_{k'} = n+ 1$, et que les opérandes dans la dernière somme avec $k' > 1$ et $j_{k'} = n+ 1$ coïncident exactement avec les opérandes avec $k = k'-1$ et $(i_{1},\dots,i_{k}) = (j_{1},\dots,j_{k})$ dans la dernière somme du quatrième membre.  
				
				\item L'univers de l'espace probabilisé est $\Omega_{n} = \mathbb{S}_{n}$, avec tribu $\mathscr{P}(\Omega_{n})$. 
				Cet espace est équiprobable. 
				C'est clair que $\#(\Omega_{n}) = n!$. 
				Un élément $\sigma \in \Omega_{n}$ représente la situation où l'on obtient la boule $\sigma(i)$ au $i$-ème tirage. 
				Soit $A_{i} = \{ \sigma \in \Omega_{n} : \sigma(i) = i \}$, pour tout $i \in [\![ 1 , n ]\!]$. 
				L'évènement demandé est donné par 
				\[     E_{n} = \bigcup_{i=1}^{n} A_{i}.     \]
				C'est facile à voir que $\#(A_{i_1} \cap \dots \cap A_{i_{k}}) = (n-k)!$, pour tous $1 \leq i_1<\dots<i_k \leq n$.
				D'après la formule de Poincaré, on trouve alors que 
				\[     p_{n} = \mathbb{P}(E_{n}) = \sum_{k=1}^{n}(-1)^{k+1} \sum_{i_1<\dots<i_k} \frac{(n-k)!}{n!} = \sum_{k=1}^{n}(-1)^{k+1} \begin{pmatrix}n \\ k\end{pmatrix} \frac{(n-k)!}{n!} = \sum_{k=1}^{n} \frac{(-1)^{k+1}}{k!}.     \] 
				
				\item On voit bien que 
				\[     \underset{n \rightarrow + \infty}{\lim} p_{n} = -\underset{n \rightarrow + \infty}{\lim} \sum_{k=1}^{n} \frac{(-1)^{k}}{k!} = 1 - e^{-1}.     \]
			\end{enumerate}
		\end{preuve}
		
		
		%%%%%%%%%%%%%%%
		\exo{Balles et paniers}
		On a $r \in \NN^{*}$ balles et $n$ paniers numérotés de $1$ à $n$, avec $n \geq 2$.
		On répondra aux questions dans les deux cas suivants:
		\begin{enumerate}[label=(C\arabic*)]
			\item Les $r$ balles sont discernables (par exemple, parce qu'elles sont de couleurs différentes).
			
			\item Les $r$ balles sont indiscernables.
		\end{enumerate}
		\begin{enumerate}
			\item Quel est le nombre de répartitions possibles (un panier peut contenir plusieurs balles)?
			On suppose qu'on a équiprobabilité.
			
			\item Quelle est la probabilité $p_k$ qu'un panier fixé (par exemple, le premier panier) contienne exactement $k$ balles, pour $k \in [\![ 0, r ]\!]$.
			Étudier aussi la monotonie de la suite $(p_k)_{k \in \{ 0, \dots, r\}}$.
			
			\item On suppose que $n$ et $r$ tendent vers l'infini et que $r/n$ tend vers $\lambda$. 
			Montrer que chaque terme $p_k$ admet une limite et calculer celle-ci.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item L'univers de l'espace probabilisé pour le cas (C1) est $\Omega_{1} = \{ f : [\![ 1, r ]\!] \rightarrow [\![ 1, n ]\!] \}$, avec tribu $\mathscr{P}(\Omega_{1})$. 
				Par ailleurs, l'univers de l'espace probabilisé pour le cas (C2) est $\Omega_{2} = \{ (x_{1}, \dots, x_{n}) \in \NN^{n} : \sum_{i=1}^{n} x_{i} = r \}$, avec tribu $\mathscr{P}(\Omega_{2})$. 
				Les deux espaces sont équiprobables. 
				C'est clair que $\#(\Omega_{1}) = n^{r}$, tandis que 
				\[     \#(\Omega_{2}) = \begin{pmatrix} r+n-1 \\ r \end{pmatrix}.     \] 
				
				\item On suppose désormais que le panier fixé, qui contient précisément $k$ balles, est le premier. 
				L'évènement demandé pour le cas (C1) est 
				\[     E_{1} = \bigg\{ f \in \Omega_{1} : \#\Big(f^{-1}\big(\{1\}\big)\Big)= k  \bigg\}.     \]
				C'est facile à voir que 
				\[     \#(E_{1}) = \begin{pmatrix} r \\ k \end{pmatrix} (n-1)^{r-k},      \]
				ce qui nous dit que 
				\[     p_{k,1} = \mathbb{P}(E_{1}) = \begin{pmatrix} r \\ k \end{pmatrix} \frac{(n-1)^{r-k}}{n^{r}}.      \]
				L'évènement demandé pour le cas (C2) est 
				\[     E_{2} = \big\{ (x_{1}, \dots, x_{n}) \in \Omega_{2} : x_{1}= k  \big\}.     \]
				C'est facile à voir que 
				\[     \#(E_{2}) = \begin{pmatrix} r-k+n-2 \\ r-k \end{pmatrix},      \]
				ce qui nous dit que 
				\[     p_{k,2} = \mathbb{P}(E_{2}) = \frac{r! (r-k+n-2)!  (n-1)}{(r-k)! (r+n-1)!}.      \]
				En conséquence,  
				\[     \frac{p_{k+1,1}}{p_{k,1}} = \frac{(r-k)}{(k+1) (n-1)},     \]
				et 
				\[     \frac{p_{k+1,2}}{p_{k,2}} = \frac{r-k}{r-k+n-2},     \]
				pour $k \in [\![ 0, r-1 ]\!]$.
				Cela nous dit que, pour $k \in [\![ 0, r-1 ]\!]$, $p_{k+1,1} \geq p_{k,1}$ si et seulement si $k \leq (r-n+1)/n$, et 
				$p_{k+1,2} \leq p_{k,2}$.
				
				\item La formule de Stirling (voir l'exercice \textbf{11} de la fiche 1) et un calcul long mais élémentaire nous disent que 
				\[     \underset{\text{\begin{tiny}$\begin{matrix} n, r \rightarrow + \infty \\ r/n \rightarrow \lambda \end{matrix}$\end{tiny}}}{\lim} p_{k,1} = \frac{e^{-\lambda}}{k! \lambda^{k}} \text{ $\phantom{x}$ et $\phantom{x}$ } \underset{\text{\begin{tiny}$\begin{matrix} n, r \rightarrow + \infty \\ r/n \rightarrow \lambda \end{matrix}$\end{tiny}}}{\lim} p_{k,2} = \frac{\lambda^{k}}{(1+ \lambda)^{k+1}}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Problème du scrutin}
		Lors d'un vote opposant deux candidats A et B, A obtient $a$ voix et B obtient $b$ voix. 
		On suppose que $a<b$. 
		Quelle est la probabilité pour qu'au cours du dépouillement, B ait toujours été strictement en tête ?
		\\
		\textbf{Indication :} représenter le dépouillement par un chemin du plan constitué de segments horizontaux ou verticaux de longueur 1 joignant l'origine au point de coordonnées $(a,b)$ et compter le nombre de chemins situés strictement au dessus de la diagonale.
		
		\begin{preuve} 
			L'univers de l'espace probabilisé est 
			\[     \Omega = \Big\{ \bar{x} = (x_{1},\dots,x_{a+b}) \in \{ u , r \}^{a+b} : \#\big(\{ i : x_{i} = u \}\big) = b \Big\},     \]
			avec tribu $\mathscr{P}(\Omega)$. 
			Cet espace est équiprobable. 
			C'est clair que 
			\[     \#(\Omega) = \frac{(a+b)!}{a! b!}.     \] 
			Pour $\bar{x} \in \Omega$ et $i \in [\![ 1,a+b ]\!]$, on définit $N_{u,i}(\bar{x}) = \#(\{ j \in [\![ 1,i ]\!] : x_{j} = u \})$ 
			et $N_{r,i}(\bar{x}) = \#(\{ j \in [\![ 1,i ]\!] : x_{j} = r \})$. 
			Noter que $N_{u,i}(\bar{x}) + N_{r,i}(\bar{x})  = i$.
			L'évènement complémentaire de l'évènement demandé $E$ est 
			\[     \Omega \setminus E = \Big\{ \bar{x} \in \Omega : \text{ il existe } i \in [\![ 1,a+b ]\!] \text{ tel que }  N_{u,i}(\bar{x}) = N_{r,i}(\bar{x})  \Big\}.     \]
			Pour $\bar{x} \in \Omega \setminus E$, on définit 
			\[     \iota(\bar{x}) = \operatorname{min}\Big\{ i \in [\![ 1,a+b ]\!] : N_{u,i}(\bar{x}) = N_{r,i}(\bar{x})  \Big\}.     \]
			C'est clair que $\iota(\bar{x})$ est pair. 
			On définit l'application $\varphi : \Omega \setminus E \rightarrow \Omega \setminus E$ via $\varphi(\bar{x}) = \bar{y}$ avec 
			\[     y_{i} = \begin{cases} 
			r, &\text{si $x_{i} = u$ et $i \leq \iota(\bar{x})$},
			\\
			u, &\text{si $x_{i} = r$ et $i \leq \iota(\bar{x})$},
			\\
			x_{i}, &\text{si $i > \iota(\bar{x})$}.
			\end{cases}
			\]
			On laisse au lecteur la vérification simple du fait que $\varphi$ est une application bijective. 
			En outre, on définit $F = \{ \bar{x} \in \Omega \setminus E : x_{1} = r \}$ et $G = \{ \bar{x} \in \Omega \setminus E : x_{1} = u \}$. 
			On note que $\varphi$ établit une bijection entre $F$ et $G$, et 
			\[      F = \Big\{ \bar{x} \in \Omega : x_{1} = r  \Big\}.     \]
			Cela implique que $\#(\Omega \setminus E) = 2 \#(F)$. 
			Par ailleurs, c'est facile à voir que 
			\[     \#(F) = \frac{(a+b-1)!}{(a-1)! b!},     \]
			ce qui implique que 
			\[     \#(E) = \frac{(a+b-1)! (b-a)}{a! b!} \text{ $\phantom{x}$ et $\phantom{x}$  } \mathbb{P}(E) = \frac{(b-a)}{(a+b)}.     \]
		\end{preuve}
		
		\subsection{Probabilités conditionnelles et indépendance}
		
		%%%%%%%%%%%%%%%
		\exo{Indépendance de deux évènements}
		Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace probabilisé et soient $A, B \in \mathscr{A}$ deux évènements.
		Soient $\mathcal{A}$ et $\mathcal{B}$ les tribus engendrées par $A$ et $B$, respectivement, 
		\textit{i.e.} $\mathcal{A}=\{ \emptyset, A, \Omega \setminus A, \Omega\}$ et $\mathcal{B}=\{\emptyset, B, \Omega \setminus B, \Omega \}$.  
		Montrer que $A$ et $B$ sont indépendants si et seulement si pour tout $C \in \mathcal{A}$ et pour tout $D \in \mathcal{B}$, 
		$\mathbb{P}(C \cap D)=\mathbb{P}(C) \mathbb{P}(D)$.
		
		\begin{preuve}
			C'est clair que, si pour tout $C \in \mathcal{A}$ et pour tout $D \in \mathcal{B}$, 
			$\mathbb{P}(C \cap D)=\mathbb{P}(C) \mathbb{P}(D)$, alors $A$ et $B$ sont indépendants. 
			On va démontrer la réciproque. 
			On suppose alors que $A$ et $B$ sont indépendants. 
			Si $C$ est l'ensemble vide, alors 
			\[     \mathbb{P}(C \cap D) = \mathbb{P}(\emptyset) = 0 = 0 . \mathbb{P}(D) = \mathbb{P}(\emptyset) \mathbb{P}(D) = \mathbb{P}(C) \mathbb{P}(D),     \]
			et de même si $D$ est l'ensemble vide. 
			Si $C$ est $\Omega$, alors 
			\[     \mathbb{P}(C \cap D) = \mathbb{P}(D) = \mathbb{P}(\Omega) \mathbb{P}(D) = \mathbb{P}(C) \mathbb{P}(D),     \]
			et de même si $D$ est $\Omega$. 
			Si $\{C,D\} = \{ A, B\}$, le résultat $\mathbb{P}(C \cap D)=\mathbb{P}(C) \mathbb{P}(D)$ suit directement de l'indépendance de $A$ et de $B$. 
			Si $\{C,D\} = \{ A, \Omega \setminus B\}$, alors 
			\begin{align*}
			\mathbb{P}(C \cap D) &= \mathbb{P}\big(A \setminus (A \cap B)\big) = \mathbb{P}(A) - \mathbb{P}(A \cap B) = 
			\mathbb{P}(A) - \mathbb{P}(A) \mathbb{P}(B) 
			\\
			&= \mathbb{P}(A) \big(1 - \mathbb{P}(B)\big) = \mathbb{P}(C) \mathbb{P}(D).
			\end{align*}  
			Le même argument montre le cas $\{C,D\} = \{ B, \Omega \setminus A\}$. 
			Finalement, le cas $\{C,D\} = \{ \Omega \setminus A, \Omega \setminus B\}$ est donné par 
			\begin{align*}
			\mathbb{P}(C \cap D) &= \mathbb{P}\big(\Omega \setminus (A \cup B)\big) = 1 - \mathbb{P}(A \cup B) = 
			1 - \mathbb{P}(A) - \mathbb{P}(B) + \mathbb{P}(A \cap B) 
			\\
			&= 1 - \mathbb{P}(A) - \mathbb{P}(B) + \mathbb{P}(A) \mathbb{P}(B)  = \big( 1 - \mathbb{P}(A)\big) \big( 1 - \mathbb{P}(B)\big) = \mathbb{P}(C) \mathbb{P}(D).     
			\end{align*}  
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Indépendance d'une famille d'évènements}
		\label{exo:13}
		Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace probabilisé. 
		Considérons les propriétés suivantes 
		\begin{enumerate}[label=(P\arabic*)]
			\item les événements $(A_i)_{1\leq i\leq n} \in \mathscr{A}^{n}$  sont indépendants;
			
			\item pour toute famille $(B_i)_{1\leq i\leq n} \in \mathscr{A}^{n}$ telle que $B_i \in \{ A_i,\Omega \setminus A_i\}$,
			\[     \mathbb P\bigg(\bigcap_{1\leq i\leq n} B_i\bigg)=\prod_{1\leq i\leq n} \mathbb{P}(B_i);     \] 
			\item pour toute famille $(B_i)_{1\leq i\leq n} \in \mathscr{A}^{n}$ telle que 
			$B_i \in \{ \emptyset, A_i, \Omega \setminus A_i,\Omega \}$,
			\[     \mathbb{P}\bigg(\bigcap_{1\leq i\leq n} B_i\bigg) =\prod_{1\leq i\leq n} \mathbb{P}(B_i).     \]
		\end{enumerate}
		Montrer que les propriétés (P1), (P2) et (P3) sont équivalentes.
		
		\begin{preuve}
			Il s'agit d'une conséquence immédiate de l'exercice précédent et d'un argument par récurrence. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		\label{exo:14}
		Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace probabilisé et soit $(A_n)_{n \in \NN} \in \mathscr{A}^{\NN}$ une suite d'évènements indépendants.
		\begin{enumerate}
			\item Montrer que 
			\[     \mathbb{P}\bigg(\bigcup_{k\in \NN} A_k\bigg) = 1- \underset{N\rightarrow+\infty}{\lim} \prod_{k=0}^N \mathbb{P}(\Omega \setminus A_k).     \]
			
			\item Montrer que les assertions suivantes sont équivalentes :
			\begin{enumerate}[label=(\roman*)]
				\item $\mathbb{P}(\cup_{k\in \NN} A_k)=1$;
				\item $\sum_{k=0}^{+\infty} \ln(\mathbb{P}(\Omega \setminus A_k))$ diverge;
				\item $\sum_{k=0}^{+\infty}\mathbb{P}(A_k)$ diverge.
			\end{enumerate}
		\end{enumerate}      
		
		\begin{preuve}
			\begin{enumerate}
				\item On remarque d'abord que la définition de probabilité nous dit que 
				\begin{equation}
				\label{eq:inf}
				\mathbb{P}\bigg(\bigcup_{k\in \NN} A_k\bigg) = \underset{N \rightarrow +\infty}{\lim} \mathbb{P}\bigg(\bigcup_{k=0}^{N} A_k\bigg).
				\end{equation}         
				Par ailleurs, 
				\begin{equation}
				\label{eq:inf2}
				\begin{split}
				\mathbb{P}\bigg(\bigcup_{k=0}^{N} A_k\bigg) &= \mathbb{P}\bigg(\Omega \setminus \Big(\Omega \setminus \Big(\bigcup_{k=0}^{N} A_k\Big)\Big)\bigg) 
				= \mathbb{P}\bigg(\Omega \setminus \Big(\bigcap_{k=0}^{N} (\Omega \setminus A_k)\Big)\bigg) 
				\\
				&= 1 - \mathbb{P}\bigg(\bigcap_{k=0}^{N} (\Omega \setminus A_k)\bigg) = 1 - \prod_{k=0}^N \mathbb{P}(\Omega \setminus A_k),     
				\end{split}
				\end{equation} 
				d'après l'exercice précédent. 
				Le résultat suit de prendre la limite quand $N$ tend vers $+ \infty$ et \eqref{eq:inf}. 
				
				\item Les identités dans \eqref{eq:inf2} nous disent que la condition (i) est équivalente à 
				\begin{equation}
				\label{eq:inf3}
				\tag{$\mathrm{iv}$}
				\underset{N \rightarrow +\infty}{\lim} \prod_{k=0}^{N} \mathbb{P}(\Omega \setminus A_k) = 0,
				\end{equation} 
				ce qui est équivalent à 
				\begin{equation}
				\label{eq:inf4}
				\tag{$\mathrm{v}$}
				\underset{N \rightarrow +\infty}{\lim} \sum_{k=0}^{N} \ln\big(\mathbb{P}(\Omega \setminus A_k)\big) = \underset{N \rightarrow +\infty}{\lim} \sum_{k=0}^{N} \ln\big(1 - \mathbb{P}(A_k)\big) = - \infty.
				\end{equation} 
				Cela nous dit en particulier que les conditions (i) et (ii) sont équivalentes. 
				En outre, l'inégalité $\ln(1-x) \leq -x$, pour tout $x \in [0,1 \hskip 0.6mm [ \hskip 0.6mm$, nous dit que (iii) implique (ii), tandis que la 
				l'inégalité $\ln(1-x) \geq -2 x$, pour tout $x \in [0,1/2]$, nous dit que (ii) implique (iii). 
				La preuve de ces deux inégalités suit de montrer les mêmes inégalités pour les dérivées respectives et que les fonctions initiales coïncident en $x = 0$. 
			\end{enumerate}  
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		Dans une usine d'écrous, trois machines A, B et C produisent $25\%$, $35\%$ et $40\%$ du total de la production, respectivement.
		Elles produisent $5\%$, $4\%$ et $2\%$ de pièces défectueuses, respectivement. 
		Un écrou est tiré au hasard et s'avère défectueux. 
		Quelle est la probabilité qu'il ait été produit par la machine A? B? ou C?
		
		\begin{preuve} 
			Soit $\mathscr{A}$ (resp., $\mathscr{B}$, $\mathscr{C}$) l'évènement ``l'écrou tiré au hasard a été produit par la machine A (resp., B, C)'' et soit $\mathscr{D}$ l'évènement ``l'écrou tiré au hasard est défectueux''. 
			Alors $\mathbb{P}(\mathscr{A}) = 0,25$, $\mathbb{P}(\mathscr{B}) = 0,35$ et $\mathbb{P}(\mathscr{C}) = 0,4$. 
			L'énoncé dit aussi que $\mathbb{P}(\mathscr{D} | \mathscr{A}) = 0,05$, $\mathbb{P}(\mathscr{D} | \mathscr{B}) = 0,04$ et 
			$\mathbb{P}(\mathscr{D} | \mathscr{C}) = 0,02$. 
			Pour simplifier, on va écrire $\mathscr{A}_{1} = \mathscr{A}$, $\mathscr{A}_{2} = \mathscr{B}$ et $\mathscr{A}_{3} = \mathscr{C}$.
			Les probabilités demandées sont données par la formule de Bayes
			\[     \mathbb{P}(\mathscr{A}_{i} | \mathscr{D}) = \frac{\mathbb{P}(\mathscr{D} | \mathscr{A}_{i}) \mathbb{P}(\mathscr{A}_{i})}{\sum_{j=1}^{3}\mathbb{P}(\mathscr{D} | \mathscr{A}_{j}) \mathbb{P}(\mathscr{A}_{j})},     \]
			pour tout $i \in \{ 1, 2, 3\}$. 
			Cela nous donne 
			\[     \mathbb{P}(\mathscr{A} | \mathscr{D}) \simeq 0,36, \hskip 0.5cm \mathbb{P}(\mathscr{B} | \mathscr{D}) \simeq 0,41 \hskip 0.5cm \text{ et } \hskip 0.5cm \mathbb{P}(\mathscr{C} | \mathscr{D}) \simeq 0,23.    \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} 
		Une urne contient $n$ boules noires et $n$ boules rouges. 
		On tire deux par deux sans remise, toutes les boules de l'urne. 
		Quelle est la probabilité d'obtenir à chaque tirage deux boules de couleurs différentes ?
		
		\begin{preuve} 
			On peut supposer sans perte de généralité que les boules sont numérotées de $1$ à $2n$, les boules impaires sont noires et les boules paires sont rouges. 
			On suppose en plus que pour chaque tirage de deux boules, on tire d'abord une boule et après une autre boule. 
			L'univers de l'espace probabilisé est 
			\[     \Omega = \mathbb{S}_{2n},     \]
			avec tribu $\mathscr{P}(\Omega)$. 
			Un élément $\sigma \in \mathbb{S}_{2n}$ représente la situation où l'on a tiré la $\sigma(i)$-ème boule au $i$-ème tirage. 
			Pour chaque $n \in \Z$, on note $\bar{n} \in \Z/2 \Z$ la classe de $n$. 
			Cet espace est équiprobable. 
			C'est clair que 
			\[     \#(\Omega) = (2n)!.     \] 
			L'évènement demandé est 
			\[     E = \Big\{ \sigma \in \Omega : \{ \overline{\sigma(2j-1)},  \overline{\sigma(2j)} \}= \{ \bar{0},\bar{1} \}, \text{ pour tout } j \in [\![ 1 , n ]\!] \Big\}.     \]
			C'est facile à voir que $\#(E) = n!^{2} 2^{n}$, ce qui nous dit que 
			\[     \mathbb{P}(E) = \frac{2^{n} n!^{2}}{(2n)!}.     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} 
		On dispose de $N+1$ urnes numérotées de $0$ à $N$.
		L'urne numéro $k$ contient $k$ boules rouges et $N-k$ boules noires. 
		On tire une des urnes avec équiprobabilité, puis on procède avec cette urne à une série de $n$ tirages avec remise.
		\begin{enumerate}
			\item Calculer la probabilité d'avoir choisi l'urne numéro $1$ sachant qu'on a tiré $n$ boules rouges.
			
			\item Calculer la probabilité de tirer $n$ boules rouges.
			
			\item Calculer la probabilité de tirer une boule rouge au tirage $n+1$ sachant qu'on a déjà tiré $n$ boules rouges.
			
			\item Déterminer les limites des probabilités précédentes quand $N \rightarrow +\infty$.
		\end{enumerate}
		
		\begin{preuve}
			Pour $k \in [\![ 0, N ]\!]$, on note $U_{k}$ l'évènement ``on tire la $k$-ème urne'', et pour $n \in \NN$, 
			on note $R_{n}$ l'évènement ``on a obtenu $n$ boules rouges lors de $n$ tirages (avec remise)''. 
			L'énoncé nous dit que $\mathbb{P}(U_{k}) = 1/(N+1)$ et $\mathbb{P}(R_{n} | U_{k}) = (k/N)^{n}$. 
			\begin{enumerate}
				\item Dans ce cas, la formule de Bayes nous dit que  
				\[     \mathbb{P}(U_{1} | R_{n}) = \frac{\mathbb{P}(R_{n} | U_{1}) \mathbb{P}(U_{1})}{\sum_{k=0}^{N} \mathbb{P}(R_{n} | U_{k}) \mathbb{P}(U_{k})} = \frac{1}{\sum_{k=0}^{N} k^{n}}.     \]
				
				\item La probabilité demandé est $\mathbb{P}(R_{n})$, que l'on calcule à partir de 
				\[     \mathbb{P}(R_{n}) = \sum_{k=0}^{N} \mathbb{P}(R_{n} | U_{k}) \mathbb{P}(U_{k}) = \frac{1}{N+1} \sum_{k=0}^{N} \Big(\frac{k}{N}\Big)^{n}.     \]
				
				\item La probabilité demandé est précisément $\mathbb{P}(R_{n+1} | R_{n})$. 
				Par définition $R_{n+1} \cap R_{n} = R_{n+1}$, ce qui nous dit que 
				\[     \mathbb{P}(R_{n+1} | R_{n}) = \frac{\mathbb{P}(R_{n+1} \cap R_{n})}{\mathbb{P}(R_{n})} = \frac{\mathbb{P}(R_{n+1})}{\mathbb{P}(R_{n})} = \frac{1}{N} \frac{\sum_{k=0}^{N} k^{n+1}}{\sum_{k=0}^{N} k^{n}}.  \]
				
				\item C'est clair que 
				\[     \underset{N \rightarrow + \infty}{\lim} \mathbb{P}(U_{1} | R_{n}) = \underset{N \rightarrow + \infty}{\lim} \frac{1}{\sum_{k=0}^{N} k^{n}} = 0,     \]
				tandis que 
				\[     \underset{N \rightarrow + \infty}{\lim} \mathbb{P}(R_{n}) = \underset{N \rightarrow + \infty}{\lim}\frac{1}{N+1} \sum_{k=0}^{N} \Big(\frac{k}{N}\Big)^{n} = \int_{0}^{1} x^{n} dx = \frac{1}{n+1}.     \]
				Finalement, 
				\[     \underset{N \rightarrow + \infty}{\lim} \mathbb{P}(R_{n+1} | R_{n}) = \underset{N \rightarrow + \infty}{\lim} \frac{1}{N} \frac{\sum_{k=0}^{N} k^{n+1}}{\sum_{k=0}^{N} k^{n}} = \frac{n+1}{n+2}.       \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $(\Omega, \mathscr{A}, \mathbb{P})$ un espace probabilisé et soient $A_1,\dots,A_n \in \mathscr{A}$ des événements indépendants. 
		On suppose que $\mathbb{P}(A_k)=p_k$.
		Quelle est la probabilité $p$ qu'aucun de ces événements ne soit réalisé ?
		Montrer que 
		\[     p \leq e^{-\sum_{k=1}^{n} p_k}.     \]
		
		\begin{preuve}
			D'après l'exercice \ref{exo:13}, on voit bien que 
			\begin{equation*}
			\label{eq:infn}
			p = \mathbb{P}\bigg(\bigcap_{k=0}^{n} (\Omega \setminus A_k)\bigg) =  \prod_{k=0}^{n} \mathbb{P}(\Omega \setminus A_k) 
			= \prod_{k=0}^{n} \big(1 - \mathbb{P}(A_k)\big) = \prod_{k=0}^{n} \big(1 - p_{k}\big).  
			\end{equation*} 
			L'inégalité $1 - x \leq e^{-x}$, pour $x \in [0,1]$, nous dit que 
			\[     p \leq e^{-\sum_{k=1}^{n} p_k}.     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo
		Pour tout entier $n \geq 2$ fixé, soit $\mathbb{P}_n$ la probabilité uniforme sur l'ensemble $\{ 1,\dots,n \}$ et soit 
		$\mathscr{DP}_{n} \subseteq \{ 1,\dots,n \}$ l'ensemble des diviseurs premiers de $n$. 
		Pour tout diviseur $m$ de $n$ désignons par $A_m$ le sous-ensemble de $\{ 1 , \dots , n \}$ formé des multiples de $m$. 
		\begin{enumerate}
			\item Montrer que $\mathbb{P}_n(A_m)=1/m$.
			\item Montrer que les $A_p$, où $p$ parcourt les diviseurs premiers de $n$, sont des événements indépendants dans l'espace probabilisé $(\{1,\dots,n\},\mathbb{P}_n)$.
			\item En déduire que l'ensemble des entiers de $\{1,\dots,n\}$ premiers avec $n$ a une probabilité 
			\[     \prod_{p \in \mathscr{DP}_{n}}\bigg(1-\frac{1}{p}\bigg).     \]
			En déduire le cardinal de l'ensemble des entiers de $\{1,\dots,n\}$ premiers avec $n$. 
			Retrouver ainsi une formule d'Euler.
			\item On considère maintenant l'espace $(\NN^*,\mathscr{P}(\NN^*))$. 
			Soit $s>1$.
			\begin{enumerate}[label=(\roman*)]
				\item Montrer qu'il existe $\lambda_s \in \RR$ tel que 
				\[     \mathbb{P}_s\big(\{n\}\big)=\frac{\lambda_s}{n^s}     \] 
				définisse une probabilité sur $(\NN^*,\mathscr{P}(\NN^*))$. 
				\item Soit pour $m\in\NN^*$, $A_m=\{n\in\NN^* : m | n \}$. 
				Montrer que $\mathbb{P}_s(A_m)=1/m^s$.
				\item Montrer que les $A_p$, où $p$ parcourt l'ensemble $\mathscr{NP}$ des nombres premiers, sont des événements indépendants dans $(\NN^*,\mathscr{P}(\NN^*),\mathbb{P}_s)$.
				\item Montrer que 
				\[     \sum_{n=1}^{+\infty} \frac{1}{n^s}=\frac{1}{\prod\limits_{p \in \mathscr{NP}}\Big(1-\frac{1}{p^s}\Big)}.     \]
				\item Existe t-il une probabilité $\mathbb{Q}$ sur $(\NN^*,\mathscr{P}(\NN^*))$ telle que pour tout $m\geq 1$, $\mathbb{Q}(A_m)= 1/m$ ?
			\end{enumerate}
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Le résultat suit directement des identités  
				\[     [\![ 1 , n ]\!] = \bigsqcup_{k=0}^{m-1} (A_{m} + k) \text{ et } \#(A_{m}+ k) = \#(A_{m}),     \]
				pour tout $k \in [\![ 0 , m-1 ]\!]$, où $A_{m} + k = \{ x + k : x \in A_{m} \}$. 
				
				\item Il suffit de démontrer que $A_{p} \cap A_{q} = A_{p q}$, pour tous $p, q \in \NN$ diviseurs de $n$ avec $\operatorname{PGCD}(p,q)=1$, ce qui suit de la définition. 
				
				\item La probabilité demandé suit directement de l'égalité  
				\[     \{ m \in [\![ 1 , n ]\!] : \operatorname{PGCD}(m,n)=1 \}  = \bigcap_{p \in \mathscr{DP}_{n}} \big([\![ 1 , n ]\!] \setminus A_{p}\big),     \]
				qui est un conséquence des définitions. 
				Le cardinal de l'ensemble des entiers de $\{1,\dots,n\}$ premiers avec $n$ est alors 
				\[     \varphi(n) = \#\big(\{ m \in [\![ 1 , n ]\!] : \operatorname{PGCD}(m,n)=1 \}\big) = n \bigcap_{p \in \mathscr{DP}_{n}} \big([\![ 1 , n ]\!] \setminus A_{p}\big).     \]
				
				\item 
				\begin{enumerate}[label=(\roman*)]
					\item On remarque d'abord que la série 
					\[     \sum_{n = 1}^{+ \infty} \frac{1}{n^{s}}   \]
					converge absolument pour $s > 1$. 
					Soit $\lambda_{s}^{-1}$ la somme de cette série.  
					Alors, 
					\[     \sum_{n = 1}^{+ \infty} \mathbb{P}_s\big(\{n\}\big) = \sum_{n = 1}^{+ \infty} \frac{\lambda_{s}}{n^{s}} = \lambda_{s} \sum_{n = 1}^{+ \infty} \frac{1}{n^{s}} = 1.   \]
					
					\item On voit bien que 
					\[     \mathbb{P}_s\big(A_{m}\big) = \sum_{k = 1}^{+ \infty} \frac{\lambda_{s}}{m^{s} k^{s}} = \frac{\lambda_{s}}{m^{s}} \sum_{k = 1}^{+ \infty} \frac{1}{k^{s}} = \frac{1}{m^{s}}.   \]
					
					\item Il suffit de démontrer que $A_{p} \cap A_{q} = A_{p q}$, pour tous $p, q \in \NN$ avec $\operatorname{PGCD}(p,q)=1$, ce qui suit de la définition. 
					
					\item Le résultat suit directement de l'identité 
					\[     \bigcap_{p \in \mathscr{NP}} (\NN^{*} \setminus A_{p}) = \{ 1 \},     \]
					ainsi que de l'exercice \ref{exo:13}. 
					
					\item Non. S'il existait une telle probabilité $\mathbb{Q}$, alors l'exercice \ref{exo:14} nous dirait que 
					\[     \mathbb{Q}(\bigcup_{n=n_{0}}^{+\infty} A_{n}) = 1,     \]
					pour tout $n_{0} \in \NN^{*}$. 
					En outre, pour tout élément $N \in \NN^{*}$, il existe $n_{0} \in \NN^{*}$ tel que $N \notin A_{n}$, pour tout $n \geq n_{0}$. 
					En conséquence, $\mathbb{Q}(\{N\})=0$, pour tout $N \in \NN^{*}$, ce qui est absurde.  
				\end{enumerate}
			\end{enumerate}
		\end{preuve}
		
		\subsection{Variables discrètes}
		
		%%%%%%%%%%%%%%%
		\exo{Comment jouer à pile ou face avec une pièce biaisée ?}
		On considère une pièce ayant une probabilité $p \in \hskip 0.6mm]\hskip 0.6mm 0,1 \hskip 0.6mm[\hskip 0.6mm$ de tomber sur face (F).
		La probabilité de tomber sur pile (P) est donc $1-p$.
		On lance la pièce deux fois.
		Si on obtient FP, on pose $X=1$ et si on obtient PF, on pose $X=0$.
		Dans les deux autres cas, on recommence jusqu'à obtenir FP ou PF et on définit alors $X$ comme précédemment.
		Déterminer la loi de $X$.
		
		\begin{preuve}
			L'univers de l'espace probabilisé est 
			\[     \Omega = \bigcup_{n \in \NN} \{ PP, FF \}^{n} \times \{ FP, PF \},     \]
			avec tribu $\mathscr{P}(\Omega)$. 
			On a décidé d'exclure les éléments 
			\[     \omega \in \{ PP, FF \}^{\NN},     \]
			puisque, de façon intuitive, la probabilité de l'évènement qu'ils définissent est nulle. 
			En effet, notre intuition nous dit qu'il faut définir $\mathbb{P} : \mathscr{P}(\Omega) \rightarrow \mathbb{R}_{\geq 0}$ comme la seule application $\sigma$-additive telle que 
			\[     \mathbb{P}\Big(\big\{ (\omega_{0},FP) \big\} \Big) = \mathbb{P}\Big(\big\{ (\omega_{0},PF) \big\} \Big) = p^{1+2 |\omega_{0}|} (1-p)^{1+2 (n - |\omega_{0}|)},     \]
			pour tout $\omega_{0}=(w_{1},\dots,w_{n}) \in \{ PP, FF \}^{n}$ et $n \in \NN$, où 
			\[     |\omega_{0}| = \#\Big(\big\{ i \in [\![ 1, n ]\!] : w_{i} = FF \big\}\Big).     \]
			Il suffit de montrer que $\mathbb{P}$ est une probabilité, \textit{i.e.} $\mathbb{P}(\Omega) = 1$. 
			On voit bien que 
			\begin{equation*}
			\begin{split}
			\mathbb{P}(\Omega) &= \sum_{\omega \in \Omega} \mathbb{P}\big(\{ \omega \} \big) 
			= \sum_{n \in \NN} \sum_{\omega_{0} \in \{ PP, FF \}^{n}} \Bigg( \mathbb{P}\Big(\big\{ (\omega_{0},FP) \big\} \Big) + \mathbb{P}\Big(\big\{ (\omega_{0},PF) \big\} \Big) \bigg)
			\\
			&= 2 \sum_{n \in \NN} \sum_{\omega_{0} \in \{ PP, FF \}^{n}} p^{1+2 |\omega_{0}|} (1-p)^{1+2 (n - |\omega_{0}|)}
			\\
			&= 2 \sum_{n \in \NN} \sum_{k = 0}^{n} \begin{pmatrix} n \\ k \end{pmatrix} p^{1+2 k} (1-p)^{1+2 (n - k)}
			= 2 p (1-p) \sum_{n \in \NN} \sum_{k = 0}^{n} \begin{pmatrix} n \\ k \end{pmatrix} p^{2 k} (1-p)^{2 (n - k)}
			\\
			&= 2 p (1-p) \sum_{n \in \NN} \big(p^{2} + (1-p)^{2}\big)^{n} = 2 p (1-p) \sum_{n \in \NN} \big(1 - 2 p (1 - p)\big)^{n} 
			\\
			&= \frac{2 p (1-p)}{1 - \big(1 - 2 p (1 - p)\big)} = 1,
			\end{split}
			\end{equation*}
			\textit{i.e.} $\mathbb{P}$ est une probabilité. 
			
			On pose 
			\[     \Omega_{FP} = \bigcup_{n \in \NN} \{ PP, FF \}^{n} \times \{ FP\} \text{ et } \Omega_{PF} = \bigcup_{n \in \NN} \{ PP, FF \}^{n} \times \{ PF\}.     \]
			C'est clair que $\mathbb{P}(\Omega_{FP}) = \mathbb{P}(\Omega_{PF}) = 1/2$. 
			En effet, si l'on considère l'application $\varphi : \Omega \rightarrow \Omega$ donnée par 
			$\varphi(\omega_{0},PF) = (\omega_{0},FP)$ et $\varphi(\omega_{0},FP) = (\omega_{0},PF)$, pour tout 
			$\omega_{0} \in \cup_{n \in \NN} \{ PP, FF \}^{n}$. 
			C'est clair que $\varphi$ est bijectif ($\varphi^{-1} = \varphi$), $\varphi(\Omega_{PF}) = \Omega_{FP}$ et que $\mathbb{P}(\varphi(X)) = \mathbb{P}(X)$, pour tout $X \subseteq \Omega$.
			
			La variable aléatoire $X$ est définie par 
			\[     X(\omega_{0},FP) = 1 \text{ et } X(\omega_{0},PF) = 0,     \]
			pour tout $\omega_{0} \in \cup_{n \in \NN} \{ PP, FF \}^{n}$. 
			On veut calculer $p_{X}(a) = \mathbb{P}(X = a)$, pour tout $a \in \RR$, \textit{i.e.} 
			$p_{X}(a) = \mathbb{P}(\{ \omega \in \Omega : X(\omega) = a\})$. 
			Si $a \notin \{ 0, 1\}$, $p_{X}(a) = 0$. 
			En outre, 
			\[     p_{X}(0) = \mathbb{P}(\Omega_{PF}) = 1/2 \text{ et } p_{X}(1) = \mathbb{P}(\Omega_{FP}) = 1/2.     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Lancer de pièces}
		On lance une pièce $5$ fois.
		On appelle $X$ le nombre de faces obtenus.
		On appelle $Y$ le nombre de sous-suites maximales de faces dans le $5$-uplet de résultat 
		(par exemple, FFPFP comporte deux sous-suites maximales de faces, FPFPF en comporte $3$).
		On appelle $Z$ la longueur de la plus grande sous-suite maximale de faces.
		Donner les lois de $X$, $Y$, $Z$, $(X,Y)$, $(X,Z)$, $(Y,Z)$ et $(X,Y,Z)$.
		
		\begin{preuve}
			L'univers de l'espace probabilisé est 
			\[     \Omega = \{ P, F \}^{5},     \]
			avec tribu $\mathscr{P}(\Omega)$. 
			On note $\bar{x} = (x_{1},\dots,x_{5})$ un élément de $\Omega$, où $x_{i} \in \{ P, F \}$. 
			Cet espace est équiprobable. 
			C'est clair que $\#(\Omega) = 2^{5}$. 
			
			La variable aléatoire $X$ est définie par 
			\[     X(\bar{x}) = \#\big( \{ i \in [\![ 1,5 ]\!] : x_{i} = F \} \big).     \]
			On voit bien que $p_{X}(a) = \mathbb{P}(\{ \omega \in \Omega : X(\omega) = a\}) = 0$, si $a \in \RR \setminus [\![ 0, 5 ]\!]$. 
			En outre, 
			\[     p_{X}(0) = p_{X}(5) = \frac{1}{2^{5}}, p_{X}(1) = p_{X}(4) = \frac{5}{2^{5}}, \text{ et }  p_{X}(2) = p_{X}(3) = \frac{1}{2^{5}} \begin{pmatrix} 5 \\ 2 \end{pmatrix}.     \]
			
			On dit que $\bar{x} \in \Omega$ admet une \emph{sous-suite maximale de faces de longueur} $n \in [\![ 1,5 ]\!]$ qui commence en $i \in [\![ 1, 6 - n ]\!]$ si
			\begin{enumerate}[label=(\roman*)]
				\item $x_{i+j-1} = F$, pour tout $j \in [\![ 1,n ]\!]$, 
				\item $i=1$, ou $i > 1$ et $x_{i-1} = P$,
				\item $i+n > 5$, ou $i+n \leq 5$ et $x_{i+n} = P$.
			\end{enumerate} 
			On appelle l'indice $i$ ci-dessus le \emph{début d'une sous-suite maximale de faces} de $\bar{x}$. 
			Soit $\mathscr{D}_{n}(\bar{x})$ l'ensemble des débuts des sous-suites maximales de faces de longueur $n$ de $\bar{x} \in \Omega$. 
			La variable aléatoire $Y$ est donnée par 
			\[     Y(\bar{x}) = \#\bigg( \bigcup_{n=1}^{5} \mathscr{D}_{n}(\bar{x}) \bigg),     \]
			tandis que 
			\[     Z(\bar{x}) = \operatorname{max} \bigg(\{ n \in [\![ 1,5 ]\!] : \mathscr{D}_{n}(\bar{x}) \neq \emptyset \} \cup \{ 0 \}\bigg).     \]
			On voit bien que $p_{Y}(b) = 0$, si $b \in \RR \setminus [\![ 0, 3 ]\!]$, 
			\[     p_{Y}(0) = p_{Y}(3) = \frac{1}{2^{5}} \text{ et } p_{Y}(1) = p_{Y}(2) = \frac{15}{2^{5}}.     \]
			C'est clair que $p_{Z}(c) = 0$, si $c \in \RR \setminus [\![ 0, 5 ]\!]$, 
			\[     p_{Z}(0) = p_{Z}(5) = \frac{1}{2^{5}}, p_{Z}(1) = \frac{12}{2^{5}}, p_{Z}(2) = \frac{11}{2^{5}}, p_{Z}(3) = \frac{5}{2^{5}} \text{ et } p_{Z}(4) = \frac{2}{2^{5}}.     \]
			
			Par rapport au $p_{(X,Y)}(a,b) = \mathbb{P}(\{ \omega \in \Omega : X(\omega) = a \text{ et } Y(\omega) = b \})$, on voit bien que 
			$p_{(X,Y)}(a,b) = 0$, si $(a,b) \notin ([\![ 1,5 ]\!] \times [\![ 1,3 ]\!] \setminus \{ (0,0) \})$ ou $b > 3 - |a - 3|$,  
			\begin{align*}
			p_{(X,Y)}(0,0) &= p_{(X,Y)}(3,3) = \frac{1}{2^{5}}, p_{(X,Y)}(a,1) = \frac{6-a}{2^{5}},  
			\\ p_{(X,Y)}(2,2) &= p_{(X,Y)}(3,2)= \frac{6}{2^{5}} \text{ et }  p_{(X,Y)}(4,2) = \frac{3}{2^{5}},
			\end{align*}            
			pour tout $a \in [\![ 1,5 ]\!]$.  
			
			Par rapport au $p_{(X,Z)}(a,c) = \mathbb{P}(\{ \omega \in \Omega : X(\omega) = a \text{ et } Z(\omega) = c \})$, on voit bien que 
			$p_{(X,Z)}(a,c) = 0$, si $(a,b) \notin ([\![ 1,4 ]\!] \times [\![ 1,4 ]\!] \setminus \{ (0,0), (5,5) \})$ ou $a < c < a - 2$,  
			\begin{align*}
			p_{(X,Z)}(0,0) &= p_{(X,Z)}(3,1) = p_{(X,Z)}(4,2) = \frac{1}{2^{5}}, p_{(X,Y)}(a,a) = \frac{6-a}{2^{5}}, 
			\\
			p_{(X,Z)}(2,1) &= p_{(X,Z)}(3,2) = \frac{6}{2^{5}} \text{ et } p_{(X,Z)}(4,3) = \frac{2}{2^{5}}
			\end{align*}            
			pour tout $a \in [\![ 1,5 ]\!]$.  
			
			Par rapport au $p_{(Y,Z)}(b,c) = \mathbb{P}(\{ \omega \in \Omega : Y(\omega) = b \text{ et } Z(\omega) = c \})$, on voit bien que 
			$p_{(Y,Z)}(b,c) = 0$, si $(b,c) \notin ([\![ 1,2 ]\!] \times [\![ 1,3 ]\!] \setminus \{ (0,0), (3,1), (1,4), (1,5) \})$,  
			\begin{align*}
			p_{(Y,Z)}(0,0) &= p_{(Y,Z)}(3,1) = \frac{1}{2^{5}}, p_{(X,Y)}(1,c) = \frac{6-c}{2^{5}}, p_{(Y,Z)}(2,3) = \frac{2}{2^{5}},
			\\
			p_{(X,Z)}(2,1) &= \frac{6}{2^{5}} \text{ et } p_{(X,Z)}(2,2) = \frac{7}{2^{5}},
			\end{align*}            
			pour tout $c \in [\![ 1,5 ]\!]$.  
			
			Finalement, on rappelle que 
			\[     p_{(X,Y,Z)}(a,b,c) = \mathbb{P}\Big(\big\{ \omega \in \Omega : X(\omega) = a, Y(\omega) = b \text{ et } Z(\omega) = c \big\}\Big).     \] 
			On voit bien que $p_{(X,Y,Z)}(a,b,c) = 0$, sauf dans les cas suivants: 
			%si $(a,b,c) \notin ([\![ 1,5 ]\!] \times [\![ 1,3 ]\!] \times [\![ 1,5 ]\!] \setminus \{ (0,0,0) \})$ ou $a < b + c - 1$,  
			\begin{align*}
			p_{(X,Y,Z)}(0,0,0) &= p_{(X,Y,Z)}(4,2,2) = p_{(X,Y,Z)}(3,3,1) = \frac{1}{2^{5}}, p_{(X,Y,Z)}(a,1,a) = \frac{6-a}{2^{5}}, 
			\\
			p_{(X,Y,Z)}(3,2,2) &= p_{(X,Y,Z)}(1,2,1) = \frac{6}{2^{5}} \text{ et } p_{(X,Y,Z)}(4,2,3) = \frac{2}{2^{5}},
			\end{align*}            
			pour tout $a \in [\![ 1,5 ]\!]$.  
		\end{preuve}
		
		% \begin{exer}[Loi de Bendford]\begin{em}
		% Soit $E=\{1,2,\cdots,9\}$.
		% Soit pour $k\in E$, $p_k=log(1+\frac{1}{k})$ où le log est en base 10.
		% 
		% Montrer que $p_1,p_2,\cdots ,p_9$ définissent une probabilité sur $E$.
		% 
		% Dessiner son histogramme et sa fonction de répartition.
		% 
		% Relever dans le journal 1000 nombres correspondant aux chiffres de la bourse et calculer la fréquence du premier chiffre de ces nombres. Comparer avec les probabilités précédentes.
		% 
		% Soit $(X,Y)$ la \va à valeurs dans $E\times E\cup\{0\}$ définie par $\mathbb{P}((X,Y)=(x,y)=\log \left( 1+\frac{1}{10x+y}\right) $.
		% 
		% Donner la loi de $X$.
		% \eexer
		
		%%%%%%%%%%%%%%%
		\exo{} 
		La variable aléatoire $X$ suit la loi $B(2n,p)$. 
		Déterminer la loi de $Y=|X-n|$.
		
		\begin{preuve}
			C'est clair que le rang de $Y$ est $[\![ 0, n ]\!]$, et que $\{ Y = k \} = \{ X = n - k\} \cup \{ X = n + k\}$, pour tout $k \in [\![ 0, n ]\!]$. 
			Cela implique que 
			\[     \mathbb{P}\big(\{ Y = 0 \}\big) = \mathbb{P}\big(\{ X = n \}\big) = \begin{pmatrix} 2 n \\ n \end{pmatrix} p^{n} (1-p)^{n}     \]
			et
			\[  \hskip -0.8cm   \mathbb{P}\big(\{Y = k \}\big) = \mathbb{P}\big(\{ X = n  - k  \}\big) + \mathbb{P}\big(\{ X = n  + k  \}\big) = \begin{pmatrix} 2 n \\ n - k \end{pmatrix} \big( p^{n-k} (1-p)^{n+k} + p^{n+k} (1-p)^{n-k} \big),     \]
			pour tout $k \in [\![ 1, n  ]\!]$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		La variable aléatoire $N$ suit la loi uniforme sur $[\![ 1,n ]\!]$, avec $n \geq 2$ entier. 
		Déterminer la loi de $X=\cos(N\pi)$.
		
		\begin{preuve}
			Soit $m = \lfloor n/2 \rfloor$ la partie entière de $n/2$. 
			C'est clair que le rang de $X$ est $\{ \pm 1 \}$, et que $\{ X = 1 \} = \cup_{k=1}^{m} \{ N = 2k \}$. 
			Cela implique que 
			\[     \mathbb{P}\big(\{ X = 1 \}\big) = \sum_{k=1}^{m} \mathbb{P}\big(\{ N = 2k  \}\big) = \frac{m}{n} \text{ et } \mathbb{P}\big(\{ X = -1 \}\big) = 1 - \mathbb{P}\big(\{ X = 1 \}\big) = \frac{n-m}{n}.  \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		\begin{enumerate}
			\item Soit $X$ une variable aléatoire sur un espace probabilisé fini. 
			On définit les variables aléatoires $X^{+}=\max(X,0)$ et $X^{-}=\max(-X,0)$.
			\begin{enumerate}[label=(\roman*)]
				\item Exprimer $X$ et $|X|$ à l'aide de $X^{+}$ et $X^{-}$. 
				
				\item En déduire l'inégalité $|E[X]|\leq E[|X|]$.
			\end{enumerate}
			
			\item On suppose que $X$ est à valeurs dans $\{-1,0,1\}$ et on définit $\alpha =E[X]$ et $\beta =E[|X|]$.
			\begin{enumerate}[label=(\roman*)]
				\item Montrer que $|\alpha |\leq \beta \leq 1$. 
				
				\item Déterminer la loi de $X$ à l'aide de $\alpha $ et $\beta$.
			\end{enumerate}
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item
				\begin{enumerate}[label=(\roman*)]
					\item C'est clair que $X = X^{+} - X^{-}$ et $|X| = X^{+} + X^{-}$.
					\item On voit bien que 
					\[     \big|E[X]\big| = \big|E[X^{+} - X^{-}]\big| = \big|E[X^{+}] - E[X^{-}]\big|  \leq \big|E[X^{+}]\big| + \big|E[X^{-}]\big| = E[X^{+}] + E[X^{-}] = E[|X|].     \]
				\end{enumerate}
				
				\item
				\begin{enumerate}[label=(\roman*)]
					\item L'inégalité $|\alpha |\leq \beta$ est précisément $|E[X]|\leq E[|X|]$. 
					En outre, comme $|X| \leq 1$, on conclut que $E[|X|] \leq 1$, \textit{i.e.} $\beta \leq 1$. 
					\item On voit bien que $\beta = E[|X|] = \mathbb{P}(\{ X = 1 \}) +  \mathbb{P}(\{ X = - 1 \})$ et que 
					$\alpha = E[X] = \mathbb{P}(\{ X = 1 \}) -  \mathbb{P}(\{ X = - 1 \})$. 
					Cela nous dit que 
					\[     \mathbb{P}\big(\{ X = 1 \}\big) = \frac{\alpha+\beta}{2}, \mathbb{P}\big(\{ X = 1 \}\big) = \frac{\beta-\alpha}{2} \text{ et } \mathbb{P}\big(\{ X = 0 \}\big) = 1 - \beta.    \]   
				\end{enumerate}
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} Une urne renferme des boules blanches et des boules noires en proportions respectives $p$ et $1-p$ avec $0<p<1$. 
		On effectue des tirages avec remise.
		Soit $n\in \NN^*$ fixé. 
		On note $X_n$ la variable aléatoire donnée par le nombre de tirages nécessaires à l'obtention de la $n$-ème boule blanche.
		\begin{enumerate}
			\item Quelle est la loi de $X_1$ ?
			Calculer la fonction génératrice $G_1$ de $X_1$.
			
			\item On pose $Y_1=X_1$ et $Y_n=X_n-X_{n-1}$ pour tout $n>1$. 
			Montrer que
			\begin{enumerate}[label=(\roman*)]
				\item Pour tout $n \ge 0$, la variable aléatoire $Y_n$ a même loi que $X_1$.
				
				\item Pour tout $n>0$, $X_{n-1}$ est indépendante de $Y_n$
			\end{enumerate}
			
			\item En déduire la fonction génératrice $G_n$ de $X_n$.
			Que vaut $E[X_n]$ ?
			
			\item Déterminer la loi de $X_n$. 
			Comparer $\mathbb{P}(X_n=k)$ et $\mathbb{P}(X_n=k+1)$. 
			Tracer le diagramme en bâtons de la loi de $X_n$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item C'est clair que $X_{1}$ suit une loi géométrique avec paramètre $p$, \textit{i.e.} $\mathbb{P}(\{X_{1}=k\}) = p (1-p)^{k-1}$ si 
				$k \in \NN^{*}$, et zéro sinon. 
				En conséquence, sa fonction génératrice est donnée par 
				\[     G_1(t) = \sum_{k = 1}^{+ \infty} \mathbb{P}\big(\{X_{1}=k\}\big) t^{k} = \frac{p}{1-p} \sum_{k = 1}^{+ \infty} (t - p t)^{k} = \frac{p t}{p t - t +1}.      \]
				
				\item
				\begin{enumerate}[label=(\roman*)]
					\item Il s'agit d'une conséquence directe de l'énoncé. 
					\item Il s'agit d'une conséquence directe de l'énoncé. 
				\end{enumerate}
				
				\item Comme $X_{n} = X_{n-1} + Y_{n}$, et les variables aléatoires $X_{n-1}$ et $Y_{n}$ sont indépendantes, on conclut que 
				$G_{n}$ est le produit des fonctions génératrices de $X_{n-1}$ et de $Y_{n}$, \textit{i.e.} $G_{n}(t) = G_{n-1}(t) G_{1}(t)$, 
				vu que $Y_{n}$ a la même loi que $X_{1}$. 
				Cela implique que $G_{n}(t) = G_{1}(t)^{n}$, pour tout $n \in \NN^{*}$. 
				En conséquence, d'après l'égalité $E[X_{n}] = G'_{n}(1)$, on trouve que 
				\[     E[X_{n}] = \frac{n}{p}.     \]
				
				\item Comme $X_{n}$ a la fonction génératrice d'une variable aléatoire qui suit la loi de Pascal avec paramètres $n$ et $p$, on voit bien que le rang de $X_{n}$ est $\NN_{\geq n}$ et 
				\[     \mathbb{P}\big(\{X_{n}=k\}\big)  = \begin{pmatrix} k-1 \\ n-1\end{pmatrix} p^{k} (1-p)^{k-n},     \]
				pour tout $k \in \NN_{\geq n}$. 
				C'est clair que $\mathbb{P}(\{X_{n}=k+1\})/\mathbb{P}(\{X_{n}=k \}) = p (1-p) k/(k-n+1) $, pour tout $k \in \NN_{\geq n}$. 
				On laisse le diagramme en bâtons de la loi de $X_n$ au lecteur. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Loi sans mémoire}
		Soit $T$ une variable aléatoire à valeurs dans $\NN$ telle que 
		\[     \mathbb{P}\big( \{T \geq n+k \} | \{T \geq n\} \big) = \mathbb{P}\big( \{ T\geq k \}\big),     \]
		pour tous $n, k \in \NN$.
		Déterminer la loi de $T$.
		
		\begin{preuve}
			Soit $f(n) = \mathbb{P}(\{ T\geq k \})$, pour tout $n \in \NN$. 
			Comme le rang de $T$ est $\NN$, $f(0) = 1$. 
			L'inclusion $\{T \geq m \} \subseteq  \{T \geq n\}$, pour tous $n, m \in \NN$ tels que $n \leq m$, nous dit que $f : \NN \rightarrow [0,1]$ est une fonction décroissante. 
			Cela implique que, si $f(n_{0})= 0$ pour $n_{0} \in \NN^{*}$, alors $f(m) = 0$, pour tout entier $m \geq n_{0}$.
			
			Si $f(1) = 0$, alors $f(n)= 0$ pour $n \in \NN^{*}$, \textit{i.e.} $T$ suit une loi uniforme sur $\{ 0 \}$. 
			Bien que la définition standard de la probabilité conditionnelle $\mathbb{P}( \{T \geq n+k \} | \{T \geq n\} )$ n'ait pas de sens \textit{a priori}, car $\mathbb{P}(\{ T\geq n \}) = 0$ pour tout $n \in \NN^{*}$, on peut considérer que $\mathbb{P}( \{T \geq n \} | \{T \geq n\} ) = 1$ est une propriété qui devrait être vérifiée pour tout $n \in \NN$ même si $\mathbb{P}(\{ T\geq n \}) = 0$. 
			
			On suppose désormais que $f(1) \neq 0$. 
			D'abord, on va démontrer que $f(n) \neq 0$, pour tout $n \in \NN^{*}$. 
			On suppose alors que $\{ n \in \NN^{*} : f(n) = 0 \} \neq \emptyset$ et soit $n_{0} = \min \{ n \in \NN^{*} : f(n) = 0 \}$. 
			C'est clair que $n_{0} > 1$.
			On va montrer que $f(n_{0}) \neq 0$, ce qui est absurde. 
			En effet, on a $\{T \geq n_{0} \} \subseteq  \{T \geq 1\}$ et 
			\begin{align*}
			f(n_{0}) &= \mathbb{P}\big( \{T \geq n_{0} \} \big) = \mathbb{P}\big( \{T \geq n_{0} \} \cap \{T \geq 1\} \big) = \mathbb{P}\big( \{T \geq n_{0} \} | \{T \geq 1\} \big) \mathbb{P}\big( \{T \geq 1\} \big) 
			\\
			&= \mathbb{P}\big( \{T \geq (n_{0}-1)+1 \} | \{T \geq 1\} \big) \mathbb{P}\big( \{T \geq 1\} \big) 
			\\
			&= 
			\mathbb{P}\big( \{T \geq n_{0}-1\} \big) \mathbb{P}\big( \{T \geq 1\} \big) = f(n_{0}-1) f(1)\neq 0,     
			\end{align*}
			car $f(n_{0}-1), f(1) \neq 0$. 
			En conséquence, $f(n) \neq 0$ pour tout $n \in \NN$. 
			Dans ce cas, la propriété indiquée dans l'énoncé équivaut à dire que $f(n+m)= f(n) f(m)$, pour tous $n, m \in \NN$. 
			En effet, comme $\{T \geq n+m \} \subseteq  \{T \geq n\}$ pour tous $n, m \in \NN$, alors 
			\begin{align*}
			f(m) &= \mathbb{P}\big( \{ T\geq m \}\big) = \mathbb{P}\big( \{T \geq n+m \} | \{T \geq n\} \big) = \frac{\mathbb{P}\big( \{T \geq n+m \} \cap \{T \geq n\} \big)}{\mathbb{P}\big( \{T \geq n\} \big)} 
			\\
			&= \frac{\mathbb{P}\big( \{T \geq n+m \} \big)}{\mathbb{P}\big( \{T \geq n\} \big)} = \frac{f(n+m)}{f(n)}.     
			\end{align*}
			En conséquence, $f(n) = f(1)^{n}$, pour tout $n \in \NN^{*}$. 
			On remarque que $f(1) \in  \hskip 0.6mm]\hskip 0.6mm 0,1\hskip 0.6mm[\hskip 0.6mm$, puisque le cas $f(1)=1$ implique que $f(n) = 1$, pour tout $n \in \NN$, mais la limite de $f(n)$ vaut zéro quand $n$ tend vers $+ \infty$.  
			Si $\alpha = f(1) \in \hskip 0.6mm]\hskip 0.6mm 0, 1\hskip 0.6mm[\hskip 0.6mm$, alors
			\[     \mathbb{P}\big( \{ T = n \}\big) = \mathbb{P}\big( \{ T\geq n \}\big) - \mathbb{P}\big( \{ T\geq n+1 \}\big) = \alpha^{n} (1- \alpha),     \]
			pour tout $n \in \NN$. 
			Il s'agit d'une variable aléatoire qui suit la loi géométrique avec paramètre $1 - \alpha$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Boules}
		Une urne contient $N$ boules dont $N_1$ portent le numéro $1$, $N_2$ portent le numéro $2$ 
		et $N_3$ portent le numéro $3$. 
		On fait un tirage de $n$ boules avec remise. 
		Soit $X_i$ le nombre de boules tirées qui portent le numéro $i$ et $X=(X_1,X_2,X_3)$.
		\begin{enumerate}
			\item Donner la loi de $X$.
			
			\item Donner la loi de $X_i$ pour $1 \leq i \leq 3$.
			
			\item Donner la loi de $(X_1,X_2)$.
			
			\item On note $Y_r$ la variable aléatoire valant $1$ si l'on tire une boule portant le numéro $1$ au $r$-ème tirage et $0$ sinon. On note $Z_r$ la variable aléatoire valant $1$ si on tire une boule portant le numéro $2$ au $r$-ème tirage et $0$ sinon.
			\begin{enumerate}[label=(\roman*)]
				\item Exprimer $X_1$, $X_2$ et $X_3$ en fonction des $(Y_r)_{1\leq r\leq n}$ et des $(Z_r)_{1\leq r\leq n}$.
				\item Calculer l'espérance de $X_1$, la variance de $X_1$ et la covariance de $(X_1,X_2)$.
			\end{enumerate}
		\end{enumerate}
		Traiter les mêmes questions pour un tirage sans remise.
		
		\begin{preuve}
			\begin{enumerate}
				\item Pour la cas avec remise, le même argument que pour le calcul de la fonction de masse d'une variable aléatoire qui suit la loi binomiale nous dit que 
				\[     \mathbb{P}\big(\{X = (x_{1},x_{2},x_{3}) \}\big) = \frac{n!}{x_{1}! x_{2}! x_{3}!} \bigg(\frac{N_{1}}{N}\bigg)^{x_{1}} \bigg(\frac{N_{2}}{N}\bigg)^{x_{2}} \bigg(\frac{N_{3}}{N}\bigg)^{x_{3}},      \]
				pour tout $(x_{1},x_{2},x_{3}) \in \NN^{3}$ tel que $x_{1}+x_{2}+x_{3}=n$.
				Pour la cas sans remise, le même argument que pour le calcul de la fonction de masse d'une variable aléatoire qui suit la loi hypergéométrique nous dit que 
				\[     \mathbb{P}\big(\{X = (x_{1},x_{2},x_{3}) \}\big) = \frac{\begin{pmatrix} N_{1} \\ x_{1} \end{pmatrix} \begin{pmatrix} N_{2} \\ x_{2} \end{pmatrix} \begin{pmatrix} N_{3} \\ x_{3} \end{pmatrix}}{\begin{pmatrix} N \\ n \end{pmatrix}},      \]
				pour tout $(x_{1},x_{2},x_{3}) \in \NN^{3}$ tel que $x_{1}+x_{2}+x_{3}=n$ et $x_{i} \leq N_{i}$, pour tout $i \in [\![ 1, 3 ]\!]$.
				
				\item Pour la cas avec remise, on voit bien que $X_{i}$ suit une loi binomiale avec paramètres $n$ et $N_{i}/N$, pour tout $i \in [\![ 1, 3 ]\!]$, \textit{i.e.} le rang de $X_{i}$ est $[\![ 0  , n ]\!]$ et 
				\[     \mathbb{P}\big(\{X_{i} = x_{i} \}\big) = \begin{pmatrix} n \\ x_{i} \end{pmatrix} \bigg(\frac{N_{i}}{N}\bigg)^{x_{1}} \bigg(\frac{N-N_{i}}{N}\bigg)^{n-x_{i}},      \]
				pour tout $x_{i}$ dans le rang de $X_{i}$.
				Pour la cas sans remise, on voit bien que $X_{i}$ suit une loi hypergéométrique avec paramètres $N$, $N_{i}$ et $n$, pour tout $i \in [\![ 1, 3 ]\!]$, \textit{i.e.} le rang de $X_{i}$ est $[\![ \max(N_{i}+n-N,0)  , \min(N_{i},n) ]\!]$ et 
				\[     \mathbb{P}\big(\{X_{i} = x_{i} \}\big) = \frac{\begin{pmatrix} N_{i} \\ x_{i} \end{pmatrix} \begin{pmatrix} N - N_{i} \\ n - x_{i} \end{pmatrix} }{\begin{pmatrix} N \\ n \end{pmatrix}},      \]
				pour tout $x_{i}$ dans le rang de $X_{i}$.
				
				\item On va donner simultanément la réponse pour les cas avec et sans remise.
				Comme $X_{3} = n - X_{1} - X_{2}$, on voit bien que 
				\[     \mathbb{P}\big(\{(X_{1},X_{2}) = (x_{1},x_{2}) \}\big) = \mathbb{P}\big(\{X = (x_{1},x_{2},n-x_{1}-x_{2}) \}\big),      \]
				pour tout $(x_{1},x_{2}) \in \NN^{2}$ tel que $(x_{1},x_{2}, n-x_{1}-x_{3})$ soit dans le rang de $X$. 
				
				\item 
				\begin{enumerate}[label=(\roman*)]
					\item On voit bien que 
					\[     X_{1} = \sum_{r=1}^{n} Y_{r}  \text{ et } X_{2} = \sum_{r=1}^{n} Z_{r}.     \]
					En outre, $X_{3} = n - X_{1} - X_{2}$. 
					Cette réponse est valable pour les cas avec et sans remise. 
					
					\item Pour le cas avec remise, comme $X_{1}$ suit une loi binomiale avec paramètres $n$ et $N_{1}/N$, on trouve que 
					\[     E[X_{1}] = n \frac{N_{1}}{N} \text{ et }  \operatorname{Var}[X_{1}] = n \frac{N_{1}(N-N_{1})}{N^{2}}.     \]
					Pour le cas sans remise, comme $X_{1}$ suit une loi hypergéométrique avec paramètres $N$, $N_{i}$ et $n$, on trouve que 
					\[     E[X_{1}] = n \frac{N_{1}}{N} \text{ et }  \operatorname{Var}[X_{1}] = n \frac{N_{1}(N-N_{1})(N-n)}{N^{2}(N-1)}.     \]
					Pour calculer la covariance dans les deux cas, on utilise que $\operatorname{Cov}(X_{1},X_{2}) = E[X_{1} X_{2}] - E[X_{1}] E[X_{2}]$, donc 
					il suffit de calculer $E[X_{1} X_{2}]$. 
					Pour cela, on utilise que, dans les deux cas, 
					\[      E[X_{1} X_{2}] = \sum_{r=1}^{n} \sum_{s=1}^{n} E[Y_{r}Z_{s}] = \sum_{\text{\begin{tiny}$\begin{matrix}r, s \in [\![ 1, n ]\!] \\ r \neq s \end{matrix}$\end{tiny}}} E[Y_{r}Z_{s}],     \]
					où l'on a utilisé que $Y_{r}Z_{r} = 0$, pour tout $r \in [\![ 1, n ]\!]$. 
					Par ailleurs, 
					\begin{align*}
					E[Y_{r}Z_{s}] &= \mathbb{P}\big(\{Y_{r}Z_{s} = 1\}\big) = \mathbb{P}\big(\{Y_{r} = 1 \} \cap \{Z_{s} = 1\}\big) 
					\\
					&= 
					\mathbb{P}\big(\{Y_{r} = 1 \}\big) \mathbb{P}\big(\{Z_{s} = 1\}\big) = E[Y_{r}] E[Z_{s}],
					\end{align*}     
					pour tous $r \neq s$, où l'on a utilisé que les évènements $\{Y_{r} = 1 \}$ et $\{Z_{s} = 1\}$ sont indépendants si $r \neq s$. 
					Pour calculer $E[Y_{r}]$ et $E[Z_{s}]$, on utilise que les variables aléatoires $Y_{r}$ sont indépendantes et ont la même loi, et 
					de même pour les variables aléatoires $Z_{s}$. 
					En conséquence, 
					\[      E[X_{1}] = \sum_{r=1}^{n} E[Y_{r}] = n E[Y_{r}] \text{ et } E[X_{2}] = \sum_{s=1}^{n} E[Z_{s}] = n E[Y_{s}],    \]
					pour tous $r, s \in [\![ 1, n ]\!]$. 
					Cela implique que $E[Y_{r}] = N_{1}/N$ et $E[Z_{s}] = N_{2}/N$, pour tous $r, s \in [\![ 1, n ]\!]$. 
					En conséquence, 
					\[        E[X_{1}X_{2}] = n(n-1) \frac{N_{1} N_{2}}{N^{2}},     \]
					ce qui nous donne que 
					\[     \operatorname{Cov}(X_{1},X_{2}) = E[X_{1} X_{2}] - E[X_{1}] E[X_{2}] = -n \frac{N_{1} N_{2}}{N^{2}}.     \]
					Cette réponse est valable pour les cas avec et sans remise. 
				\end{enumerate}
			\end{enumerate}
		\end{preuve}
		
		
		%%%%%%%%%%%%%%%
		\exo{Loi du maximum observé} 
		Une urne contient $N$ balles numérotées de $1$ à $N$. 
		On effectue $n$ tirages avec remise. 
		Soit $X$ le plus grand nombre tiré lors des $n$ tirages.
		\begin{enumerate}
			\item Donner la fonction de répartition de $X$.
			\item Donner la loi de $X$.
			\item Calculer $E[X]$ et donner un équivalent de $E[X]$ quand $N\rightarrow +\infty$
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On voit bien que l'univers de l'espace probabilisé est $\Omega = [\![ 1 , N ]\!]^{n}$, avec tribu $\mathscr{P}(\Omega)$. 
				Cet espace est équiprobable. 
				On voit bien que $\#(\Omega) = N^{n}$ et que l'évènement demandé est $\{ X \leq k \} = [\![ 1 , k ]\!]^{n}$, pour tout 
				$k \in [\![ 1 , N ]\!]$. 
				Cela nous dit que 
				\[     \mathbb{P}\big(\{ X \leq k \}\big) = \bigg(\frac{k}{N}\bigg)^{n},     \] 
				pour tout 
				$k \in [\![ 1 , N ]\!]$. 
				\item C'est clair que
				\[     \mathbb{P}\big(\{ X = k \}\big) = \mathbb{P}\big(\{ X \leq k \}\big) - \mathbb{P}\big(\{ X \leq k-1 \}\big) = \frac{k^{n} - (k-1)^{n}}{N^{n}},     \]
				pour tout $k \in [\![ 1 , N ]\!]$, et zéro sinon. 
				\item On voit bien que 
				\begin{align*}
				E[X] &= \sum_{k=1}^{n} k \mathbb{P}\big(\{ X = k \}\big) = \sum_{k=1}^{n} k \frac{k^{n} - (k-1)^{n}}{N^{n}} 
				\\
				&= \sum_{k=1}^{n} \frac{k^{n+1} - (k-1)^{n+1} - (k-1)^{n}}{N^{n}} = N - \sum_{k=1}^{n} \bigg(\frac{k-1}{N}\bigg)^{n}.
				\end{align*}   
				Cela implique que $E[X]$ est équivalent à $N$ quand $N\rightarrow +\infty$
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Clés}
		Un homme possède $n$ clés et veut ouvrir une porte. 
		Une seule parmi les clés dont il dispose ouvre la porte. 
		Il essaie les clés au hasard. 
		Trouver l'espérance et la variance du nombre d'essais nécessaires si :
		\begin{enumerate}
			\item les clés qui ne marchent pas sont remises avec les autres;
			
			\item les clés qui ne marchent pas sont mises de coté. 
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Dans ce cas $X$ suit une loi géométrique avec paramètre $1/n$ et, en conséquence, 
				\[     E[X] = n \text{ et } \operatorname{Var}[X] = n(n-1).     \]
				\item C'est clair que le rang de $X$ est $[\![ 1 , n ]\!]$. 
				Dans ce cas, on voit bien que 
				\[     \mathbb{P}\big( \{ X = 1 \}\big) = \frac{1}{n}.     \]
				On suppose désormais que $k \in [\![ 2, n ]\!]$. 
				Dans ce cas, la loi de $X$ est déterminée par le même argument que l'on utilise pour calculer la valeur d'une loi géométrique en $k$ en remplaçant la valeur de la loi binomiale avec paramètres $k-1$ et $1/n$ en $0$ par une loi hypergéométrique avec paramètres $n$, $1$ et $k-1$ en $0$. 
				Plus précisément, si $k \in [\![ 2, n ]\!]$, soit $A$ l'évènement ``On n'a pas tiré la clé qui ouvre la porte lors des premiers $k-1$ essais''. 
				Alors $\{ X = k \} \subseteq A$ et $A = \{ Y = 0 \}$ pour $Y$ une variable aléatoire qui suit une loi hypergéométrique avec paramètres $n$, $1$ et $k-1$, ce qui nous dit que 
				\[     \mathbb{P}(A) = \mathbb{P}\big( \{ Y = 0 \}\big) = \frac{\begin{pmatrix} 1 \\ 0  \end{pmatrix} \begin{pmatrix} n-1 \\ k-1  \end{pmatrix}}{\begin{pmatrix} n \\ k-1  \end{pmatrix}}.     \]
				En conséquence, 
				\begin{align*}
				\mathbb{P}\big( \{ X = k \}\big) &= \mathbb{P}\big( \{ X = k\} \cap A \big) = \mathbb{P}\big( \{ X = k \} | A \big) \mathbb{P}(A) \\
				&= \frac{1}{n-k+1} \frac{\begin{pmatrix} 1 \\ 0  \end{pmatrix} \begin{pmatrix} n-1 \\ k-1  \end{pmatrix}}{\begin{pmatrix} n \\ k-1  \end{pmatrix}} = \frac{1}{n},      
				\end{align*}
				pour tout $k \in [\![ 2, n ]\!]$. 
				Cela implique que $X$ suit une loi uniforme sur $[\![ 1 , n ]\!]$. 
				En conséquence, 
				\[     E[X] = \frac{n+1}{2} \text{ et } \operatorname{Var}[X] = \frac{n^{2}-1}{12}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Poisson(s)} 
		Soient $X$ et $Y$ deux variables aléatoires indépendantes de lois de Poisson de paramètre $a$ et $b$.
		\begin{enumerate}
			\item Déterminer la loi de la variable aléatoire $S=X+Y$.
			
			\item Déterminer la probabilité conditionnelle $\mathbb{P}(\{X=k\} | \{S=n\})$ pour tout couple $(n,k)$ d'entiers naturels.
			
			\item (Facultatif) Soit $r \geq 1$ un entier et $(X_k)_{k=1, \dots, r+1}$ des variables aléatoires indépendantes de lois de Poisson de 
			paramètres respectifs $a_k$.
			Donner la loi conditionnelle de $(X_1,\dots ,X_{r})$ sachant $\{X_1+\dots+X_{r+1}=n\}$, pour tout $n \in \NN$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Comme $S = X + Y$, et les variables aléatoires $X$ et $Y$ sont indépendantes, on conclut que la fonction génératrice 
				$G_{S}$ de $S$ est le produit des fonctions génératrices $G_{X}$ de $X$ et $G_{Y}$ de $Y$, \textit{i.e.} $G_{S}(t) = G_{X}(t) G_{Y}(t)$. 
				Dans ce cas, $G_{X}(t) = e^{a (t-1)}$ et $G_{Y}(t) = e^{b (t-1)}$, ce qui nous dit que 
				$G_{S}(t) = e^{(a+b) (t-1)}$, \textit{i.e.} $S$ est une variable aléatoire de loi de Poisson de paramètre $a + b$. 
				
				\item On voit bien que 
				\begin{align*}
				\mathbb{P}\big(\{X=k\} | \{S=n\}\big) &= \frac{\mathbb{P}\big(\{X=k\} \cap \{S=n\}\big)}{\mathbb{P}\big( \{S=n\}\big)} = \frac{\mathbb{P}\big(\{X=k\} \cap \{Y=n-k\}\big)}{\mathbb{P}\big( \{S=n\}\big)} 
				\\
				&= \frac{\mathbb{P}\big(\{X=k\}\big) \mathbb{P}\big(\{Y=n-k\}\big)}{\mathbb{P}\big( \{S=n\}\big)} = \begin{pmatrix} n \\ k \end{pmatrix} \frac{a^{k} b^{n-k}}{(a+b)^{n}},     
				\end{align*}
				pour tout $k \in [\![ 0, n ]\!]$ et zéro sinon, où l'on a utilisé que $X$ et $Y$ sont indépendantes. 
				
				\item D'après le premier item, on voit bien que $S_{\ell} = X_1+\dots+X_{\ell}$ est une variable aléatoire de loi de Poisson de paramètre $s_{\ell} = a_1+\dots+a_{\ell}$, pour tout $\ell \in [\![ 0, r+1 ]\!]$. 
				Soit $\bar{X} = (X_1,\dots ,X_{r})$. 
				Pour $\bar{x} = (x_1,\dots ,x_{r}) \in \NN^{r}$, on pose $|\bar{x}| = x_1+\dots+x_{r}$. 
				On veut calculer 
				\begin{align*}
				&\mathbb{P}\big(\{\bar{X}=\bar{x}\} | \{S_{r+1}=n\}\big) = \frac{\mathbb{P}\big(\{\bar{X}=\bar{x}\} \cap \{T=n\}\big)}{\mathbb{P}\big( \{S_{r+1}=n\}\big)} = \frac{\mathbb{P}\big(\{\bar{X}=\bar{x}\} \cap \{X_{r+1}=n-|\bar{x}|\}\big)}{\mathbb{P}\big( \{S_{r+1}=n\}\big)} 
				\\
				&= \frac{\mathbb{P}\big(\{\bar{X}=\bar{x}\}\big) \mathbb{P}\big(\{X_{r+1}=n-|\bar{x}|\}\big)}{\mathbb{P}\big( \{S_{r+1}=n\}\big)} = \frac{n!}{(n-|\bar{x}|)! \prod_{i=1}^{r} x_{i}!} \frac{a_{r+1}^{n-|\bar{x}|} \prod_{i=1}^{r} a_{i}^{x_{i}}}{s_{r+1}^{n}},     
				\end{align*}
				pour tout $\bar{x} = (x_1,\dots ,x_{r}) \in \NN^{r}$ tel que $|\bar{x}| \leq n$, et zéro sinon, où l'on a utilisé que $(X_k)_{k=1, \dots, r+1}$ sont indépendantes. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Loi jointe}
		On effectue une suite infinie de lancers indépendants d'un dé équilibré. 
		On numérote les lancers à partir de $1$. 
		On définit $X$ comme le numéro du premier lancer qui donne $6$ 
		et $Y$ comme le nombre de $5$ obtenus avant d'obtenir le premier $6$.
		\begin{enumerate}
			\item Déterminer la loi du couple $(X,Y)$.
			
			\item Déterminer la loi conditionnelle de $Y$ sachant l'événement $\{X=n\}$. 
			
			\item Déterminer la loi de $Y$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On voit bien que l'univers de l'espace probabilisé est $\Omega = \cup_{n \in \NN^{*}} \Omega_{n}$, avec tribu $\mathscr{P}(\Omega)$, où $\Omega_{n} = [\![ 1 , 5 ]\!]^{n-1} \times \{ 6 \}$, pour tout $n \in \NN^{*}$. 
				On écrit $\bar{a} = (a_{1},\dots , a_{n}) \in \Omega_{n}$. 
				On voit bien que $\mathbb{P}(\{\bar{a}\}) = 1/6^{n}$, $X(\bar{a}) = n$ et $Y(\bar{a}) = \#(\{ i \in [\![ 1 , n ]\!] : a_{i} = 5 \})$, 
				si $\bar{a} \in \Omega_{n}$.
				Noter que $\#(\Omega_{n}) = 5^{n-1}$ et $\Omega_{n} = \{ X = n \}$, pour tout $n \in \NN^{*}$. 
				C'est facile a voir que 
				\[     \mathbb{P}\big(\{ (X,Y) = (x,y) \}\big) = \begin{pmatrix} x-1 \\ y \end{pmatrix} \frac{4^{x-1-y}}{6^{x}},     \]
				pour tous $x \in \NN^{*}$ et $y \in [\![ 0 , x-1 ]\!]$, et zéro sinon. 
				
				\item On voit bien que 
				\[     \mathbb{P}\big(\{ Y = y \} | \{ X = n \} \big) = \frac{\mathbb{P}\big(\{ (X,Y) = (n,y) \}\big)}{\mathbb{P}\big(\{ X=n \}\big)} = \begin{pmatrix} n-1 \\ y \end{pmatrix} \frac{4^{n-1-y}}{5^{n-1}},     \]
				pour tout $y \in [\![ 0 , n-1 ]\!]$. 
				
				\item C'est clair que 
				\[     \mathbb{P}\big(\{ Y = y \} \big) = \sum_{x=y+1}^{+\infty} \mathbb{P}\big(\{ (X,Y) = (x,y) \}\big) = \frac{1}{4^{y+1}} \sum_{x=y+1}^{+\infty} \begin{pmatrix} x-1 \\ y \end{pmatrix} \bigg(\frac{4}{6}\bigg)^{x},     \]
				pour tout $y \in \NN$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Espérance discrète}
		Soit $X$ une variable aléatoire à valeurs dans $\NN$.
		Montrer que $X$ admet une espérance si et seulement si la série 
		\[     \sum_{n=0}^{+\infty}\mathbb{P}\big(\{X >n\}\big)     \] 
		converge et que
		\[     E[X]= \sum_{n=0}^{+\infty}\mathbb{P}\big(\{X >n\}\big).     \]
		
		\begin{preuve}
			C'est clair que $X$ admet une espérance si et seulement si la série 
			\[     \sum_{n=0}^{+\infty} n \mathbb{P}\big(\{X = n\}\big)     \] 
			converge et que 
			\[     E[X]= \sum_{n=0}^{+\infty} n \mathbb{P}\big(\{X = n\}\big).     \]
			Soit $a _{n} = \mathbb{P}(\{X = n\})$, pour $n \in \NN$, et $A_{n,m} = \sum_{k=n}^{m} a_{k}$, pour tous $n \leq m$ entiers positifs. 
			On pose aussi $A _{n} = \mathbb{P}(\{X > n\})$, pour $n \in \NN$. 
			Noter que $( A _{n} )_{n \in \NN}$ est décroissante et que $A_{n+1,m} = \mathbb{P}(\{ m \geq X > n\}) = A_{n} - A_{m}$. 
			Alors,  
			\[     \sum_{n=0}^{N} n a_{n} = \sum_{n=1}^{N} n a_{n} = \sum_{n=1}^{N} A_{n,N} = \sum_{n=1}^{N} (A_{n} - A_{N}) = -N A_{N} + \sum_{n=1}^{N} A_{n},     \]
			pour tout $N \in \NN^{*}$. 
			
			Si $\sum_{n=0}^{+\infty} A_{n}$ converge, alors $N A_{N}$ tend vers zéro quand $N$ tend 
			vers $+ \infty$. 
			Cela suit du fait que, si une série $\sum_{n=0}^{+\infty} b_{n}$ converge et $(b _{n} )_{n \in \NN} \in \RR_{\geq 0}^{\NN}$ est décroissante, alors 
			\[     n b_{2 n + 1} \leq n b_{2 n} \leq \sum_{k=n}^{2n} b_{k}     \]
			converge vers zéro. 
			En conséquence, la série $\sum_{n=0}^{+\infty} n a_{n}$ converge, \textit{i.e.} $X$ admet une espérance, et elle a la même limite que $E[X] = \sum_{n=0}^{+\infty} A_{n}$. 
			
			Réciproquement, si la série $\sum_{n=0}^{+\infty} n a_{n}$ converge, \textit{i.e.} $X$ admet une espérance, alors 
			\[     N A_{N} = N \sum_{n=N+1}^{+\infty} a_{n} = \sum_{n=N+1}^{+\infty} N a_{n} \leq \sum_{n=N+1}^{+\infty} n a_{n}     \]
			tend vers zéro quand $N$ tend vers $+ \infty$. 
			En conséquence, la série $\sum_{n=0}^{+\infty} A_{n}$ converge et elle a la même limite que $E[X] = \sum_{n=0}^{+\infty} n a_{n}$. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Lois géométriques}
		Soient $X$ et $Y$ deux variables aléatoires indépendantes de lois géométriques de paramètres respectifs $a$ et $b$.
		Soit $Z=\operatorname{min} (X,Y)$ et $U=|X-Y|$.
		\begin{enumerate}
			\item Déterminer $\mathbb{P}(\{X \geq n\})$ et $\mathbb{P}(\{Z\geq n\})$, pour tout $n \in \NN^{*}$. 
			Préciser la loi de $Z$.
			
			\item Calculer $\mathbb{P}(\{U=0\})$.
			Déterminer la loi de $U$.
			
			\item Montrer que $Z$ et $U$ sont indépendantes.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On voit bien que 
				\[     \mathbb{P}\big(\{X \geq n\}\big) = \sum_{k=n}^{+ \infty} \mathbb{P}\big(\{ X = k \}\big) = \sum_{k=n}^{+ \infty} p (1-p)^{k-1} = (1-p)^{n-1},      \]
				pour tout $n \in \NN^{*}$.
				En outre, 
				\[     \mathbb{P}\big(\{Z \geq n\}\big) = \mathbb{P}\big(\{X \geq n\} \cap \{Y \geq n\}\big) = \mathbb{P}\big(\{X \geq n\}\big)  \mathbb{P}\big(\{Y \geq n\}\big) = (1-p)^{2(n-1)},      \]
				pour tout $n \in \NN^{*}$, où l'on a utilisé que $X$ et $Y$ sont indépendantes. 
				Cela implique que 
				\begin{align*}
				\mathbb{P}\big(\{Z = n\}\big) &= \mathbb{P}\big(\{Z \geq n\}\big) - \mathbb{P}\big(\{Z \geq n+1 \}\big) = (1-p)^{2(n-1)} - (1-p)^{2 n} 
				\\
				&= p (1-p)^{2(n-1)} (2-p),      
				\end{align*}
				pour tout $n \in \NN^{*}$. 
				
				\item C'est clair que  
				\begin{align*}
				\mathbb{P}\big(\{ U = 0 \}\big) &= \sum_{n=1}^{+ \infty} \mathbb{P}\big(\{X = n\} \cap \{Y = n\}\big) = \sum_{n=1}^{+ \infty} \mathbb{P}\big(\{X = n\}\big) \mathbb{P}\big(\{Y = n\}\big) 
				\\
				&= \sum_{n=1}^{+ \infty} p^{2} (1-p)^{2(n-1)} = \frac{p}{2-p},      
				\end{align*}
				où l'on a utilisé que $X$ et $Y$ sont indépendantes. 
				En plus, le rang de $U$ est $\NN$ et 
				\begin{align*}
				\mathbb{P}\big(\{ U = k \}\big) &= \sum_{n=1}^{+ \infty} \mathbb{P}\big(\{X = n + k \} \cap \{Y = n\}\big) + \mathbb{P}\big(\{X = n \} \cap \{Y = n + k\}\big)  
				\\
				&= \sum_{n=1}^{+ \infty} \mathbb{P}\big(\{X = n+k\}\big) \mathbb{P}\big(\{Y = n\}\big) + \mathbb{P}\big(\{X = n\}\big) \mathbb{P}\big(\{Y = n + k\}\big) 
				\\
				&= 2 \sum_{n=1}^{+ \infty} p^{2} (1-p)^{2(n-1)+k} = \frac{2 p (1-p)^{k}}{2-p},      
				\end{align*}
				pour tout $k \in \NN^{*}$. 
				
				\item On voit bien que 
				\begin{align*}
				\mathbb{P}\big(\{ U = 0, Z = n \}\big) &= \mathbb{P}\big(\{ X = n, Y = n \}\big)
				\\
				&= p^{2} (1-p)^{2(n-1)} = \frac{p}{2-p} p (1-p)^{2(n-1)} (2-p) 
				\\
				&= \mathbb{P}\big(\{ U = 0\}\big)  \mathbb{P}\big(\{Z = n \}\big)      
				\end{align*}
				et
				\begin{align*}
				\mathbb{P}\big(\{ U = m, Z = n \}\big) &= \mathbb{P}\big(\{ X = n, Y = m+n \}\big) + \mathbb{P}\big(\{ X = m + n, Y = n \}\big) 
				\\
				&= 2 p^{2} (1-p)^{m + 2(n-1)} = \frac{2p(1-p)^{m}}{2-p}  p (1-p)^{2(n-1)} (2-p)
				\\
				&= \mathbb{P}\big(\{ U = m\}\big)  \mathbb{P}\big(\{Z = n \}\big),      
				\end{align*}
				pour tous $m, n \in \NN^{*}$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Jeu de cartes}
		On considère un jeu de $n$ cartes numérotées de $1$ à $n$.
		On mélange bien ce jeu.
		Rappeler comment modéliser cette expérience.
		On suppose qu'on met les cartes en un paquet, la première position étant celle du dessus et la dernière celle du dessous.
		Pour $1\leq k\leq n$, on note $X_k$ la variable aléatoire valant $1$ si la carte portant le numéro $k$ est à la $k$-ème position et $0$ sinon.
		\begin{enumerate}
			\item Donner la loi de $X_k$.
			
			\item Donner la loi de $(X_j,X_k)$ si $k\neq j$. 
			\\
			\textbf{Indication :} calculer d'abord $\mathbb{P}((X_j,X_k)=(1,1))$.
			
			\item Soit $S_n=X_1+\dots +X_n$. 
			Que représente $S_n$ ? 
			Calculer l'espérance et la variance de $S_n$.
			
			\item Calculer $\mathbb{P}(S_n\neq 0)$. 
			Calculer la limite précédente quand $n$ tend vers l'infini.
			\\
			\textbf{Indication :} utiliser la formule de Poincaré avec les événements $\{X_k=1\}$. 
			
			\item (Question difficile) Donner la loi de $S_n$. 
			Déterminer la limite quand $n$ tend vers l'infini de $\mathbb{P}(S_n=k)$ ($k$ étant fixé).
		\end{enumerate}
		
		\begin{preuve}
			On voit bien que l'univers de l'espace probabilisé est $\Omega = \mathbb{S}_{n}$, avec tribu $\mathscr{P}(\Omega)$, où $\sigma \in \Omega$ représente la configuration où la $i$-ème carte est dans $\sigma(i)$-ème position. 
			Il s'agit d'un espace équiprobable. 
			C'est clair que $X_{k}(\sigma) = 1$ si $\sigma(k)=k$ et zéro sinon, pour tout $k \in [\![ 1 , n ]\!]$ et $\sigma \in \Omega$.
			
			\begin{enumerate}
				\item On voit bien 
				\[     \mathbb{P}\big(\{ X_{k} = 1 \}\big) = \frac{(n-1)!}{n!} = \frac{1}{n} \text{ et } \mathbb{P}\big(\{ X_{k} = 0 \}\big) = 1 - \mathbb{P}\big(\{ X_{k} = 1 \}\big) = \frac{n-1}{n},     \]
				pour tout $k \in [\![ 1 , n ]\!]$, et zéro sinon. 
				
				\item Soient $j, k \in [\![ 1 , n ]\!]$ avec $j \neq k$. 
				Le rang de $(X_{j},X_{k})$ est $\{0,1\}^{2}$. 
				C'est facile à voir que 
				\[     \mathbb{P}\big(\{ (X_{j},X_{k}) = (1,1) \}\big) = \frac{(n-2)!}{n!} = \frac{1}{n(n-1)}.     \]
				En outre, 
				\[     \mathbb{P}\big(\{ (X_{j},X_{k}) = (1,0) \}\big) = \mathbb{P}\big(\{ (X_{j},Y_{k}) = (0,1) \}\big) = \frac{(n-2)! (n-2)}{n!} = \frac{(n-2)}{n(n-1)}     \]
				et 
				\begin{align*}
				\mathbb{P}\big(\{ (X_{j},X_{k}) = (0,0) \}\big) &= 1 - \mathbb{P}\big(\{ (X_{j},X_{k}) = (1,1) \}\big)  - 2 \mathbb{P}\big(\{ (X_{j},Y_{k}) = (0,1) \}\big) 
				\\
				&= \frac{n^{2}- 3 n + 3}{n(n-1)}.
				\end{align*}    
				
				\item 
				C'est clair que $S_{n}$ est la variable aléatoire qui représente la quantité de cartes qui sont bien ordonnées dans la pile. 
				On voit bien que 
				\[     E[S_{n}] = \sum_{i=1}^{n} E[X_{i}] = \sum_{i=1}^{n} \mathbb{P}\big(\{ X_{i} = 1\} \big) = n \frac{1}{n} = 1.     \]
				En outre, comme $\operatorname{Var}[S_{n}] = E[S_{n}^{2}] - E[S_{n}]^{2}$, il suffit de calculer $E[S_{n}^{2}]$. 
				Or, 
				\begin{align*}
				E[S_{n}^{2}] &= \sum_{i=1}^{n} \sum_{j=1}^{n} E[X_{i} X_{j}] = 
				\sum_{\text{\begin{tiny}$\begin{matrix} i, j \in [\![ 1,n]\!] \\ i \neq j \end{matrix}$\end{tiny}}} E[X_{i} X_{j}] + 
				\sum_{i=1}^{n} E[X_{i} X_{i}] 
				\\\
				&=  \sum_{\text{\begin{tiny}$\begin{matrix} i, j \in [\![ 1,n]\!] \\ i \neq j \end{matrix}$\end{tiny}}} \mathbb{P}\big(\{ (X_{j},X_{k}) = (1,1) \}\big) + \sum_{i=1}^{n} E[X_{i}]  = n(n-1) \frac{1}{n(n-1)} +  n \frac{1}{n} = 2,
				\end{align*}
				où l'on a utilisé que $X_{i}^{2} = X_{i}$. 
				En conséquence, $\operatorname{Var}[S_{n}] = 2 - 1^{2} = 1$. 
				
				\item C'est clair que le rang de $S_{n}$ est $[\![ 0, n ]\!]$. 
				Comme $\{ S_{n} \geq 1 \} = \cup_{i=1}^{n} \{ X_{i} = 1 \}$, la formule de Poincaré nous dit que 
				\begin{align*}
				\mathbb{P}\big(\{ S_{n} \geq 1 \}\big) &= \sum_{k=1}^{n} \sum_{1 \leq i_{1} < \dots < i_{k} \leq n} (-1)^{k+1} 
				\mathbb{P}\big(\{ X_{i_{1}} = \dots = X_{i_{k}} = 1 \}\big) 
				\\
				&= \sum_{k=1}^{n} \sum_{1 \leq i_{1} < \dots < i_{k} \leq n} (-1)^{k+1} \frac{(n-k)!}{n!} 
				= \sum_{k=1}^{n} (-1)^{k+1} \begin{pmatrix} n \\ k \end{pmatrix} \frac{(n-k)!}{n!} 
				\\
				&= \sum_{k=1}^{n} \frac{(-1)^{k+1}}{k!}.
				\end{align*}
				En conséquence, 
				\[          \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\big(\{ S_{n} \geq 1 \}\big) = 1 - e^{-1}.     \]
				
				\item On note d'abord que 
				\[     \mathbb{P}\big(\{ S_{n} = 0 \}\big) = 1 - \mathbb{P}\big(\{ S_{n} \geq 1 \}\big) = \sum_{k=0}^{n} \frac{(-1)^{k}}{k!}.     \]
				La limite de $\mathbb{P}(\{ S_{n} = 0 \})$ quand $n$ tend vers $+ \infty$ est alors $e^{-1}$. 
				Pour tout $S \subseteq [\![ 1, n ]\!]$, on pose 
				\[     A_{S} = \bigcap_{i \in S} \{ X_{i} = 1 \}     \] 
				et, pour $k \in \NN^{*}$ avec $k \leq n$, 
				\[     \mathscr{A}_{k} = \big\{ A_{S} : S \subseteq [\![ 1, n ]\!] \text{ et } \#(S) = k \big\}.     \] 
				On remarque que le cardinal de $\mathscr{A}_{k}$ est $n!/(k!(n-k)!)$ et 
				\[     \{ S_{n} \geq k \} = \bigcup_{A \in \mathscr{A}_{k}} A.     \]
				La formule de Poincaré nous dit alors que 
				\begin{align*}
				\mathbb{P}\big(\{ S_{n} \geq k \}\big) &= \sum_{\ell=k}^{n} \sum_{A \in \mathscr{A}_{\ell}} (-1)^{\ell+1} 
				\mathbb{P}(A) 
				= \sum_{\ell=k}^{n} \sum_{A \in \mathscr{A}_{\ell}} (-1)^{\ell+1} \frac{(n-\ell)!}{n!} 
				\\
				&= \sum_{\ell=k}^{n} (-1)^{\ell+1} \begin{pmatrix} n \\ \ell \end{pmatrix} \frac{(n-\ell)!}{n!} 
				= \sum_{\ell=k}^{n} \frac{(-1)^{\ell+1}}{\ell !},
				\end{align*}
				pour $k \in \NN^{*}$ avec $k \leq n$. 
				Cela implique que 
				\[     \mathbb{P}\big(\{ S_{n} = k \}\big) = \mathbb{P}\big(\{ S_{n} \geq k \}\big) - \mathbb{P}\big(\{ S_{n} \geq k + 1 \}\big) = \frac{(-1)^{k+1}}{k !},     \]
				pour tout $k \in \NN^{*}$ avec $k \leq n$. 
				En particulier, 
				\[          \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\big(\{ S_{n} = k \}\big) = \frac{(-1)^{k+1}}{k !},     \]
				pour tout $k \in \NN^{*}$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{La médiane}
		Soit $X$ une variable aléatoire à valeurs dans $\RR$.
		Un réel $m$ est une \emph{médiane} de $X$ si $\mathbb{P}(X\leq m) \geq 1/2$ et $\mathbb{P}(X\geq m)\geq 1/2$.
		\begin{enumerate}
			\item Montrer qu'une médiane existe toujours mais qu'on a pas toujours unicité.
			
			\item Expliquer comment on trouve une médiane sur l'histogramme ou sur le graphe de la fonction de répartition.
			
			\item En utilisant l'inégalité de Tchebychev, montrer que si $X$ admet une espérance $\mu$ et un écart type $\sigma$, $(\mu-m)^2\leq \sigma^2$, où $m$ est une médiane de $X$.  
			
			\item Comparer espérance et médiane dans les exemples suivants :
			loi uniforme sur $\{1,\cdots ,n\}$, loi binomiale de paramètre $(n,p)$, loi géométrique de paramètre $p$ et loi de Poisson de paramètre $\lambda$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Soit $F_{X} : \RR \rightarrow [0,1]$ la fonction de répartition de $F$, \textit{i.e.} $F_{X}(x) = \mathbb{P}(\{X \leq x\})$, pour tout $x \in \RR$. 
				Pour $y \in [0,1]$, on définit $A_{y} = \{ x \in \RR : F_{X}(x) \geq y \} = F_{X}^{-1}([y,1])$ et $G_{X} : [0,1] \rightarrow \RR$ 
				via $G_{X}(y) = \inf A_{y}$. 
				Comme la fonction $F_{X}$ est continue à droite, on voit bien que $A_{y}$ possède un minimum et $G_{X}(y)$ est ce minimum. 
				Cela nous dit que $F_{X}(G_{X}(y)) \geq y$, \textit{i.e.} 
				\begin{equation}
				\label{eq:med1}
				\mathbb{P}\big( \{ X \leq G_{X}(y) \} \big) \geq y,
				\end{equation}          
				pour tout $y \in [0,1]$. 
				
				En outre, on affirme que 
				\begin{equation}
				\label{eq:med2}
				\mathbb{P}\big( \{ X \geq G_{X}(y) \} \big) \geq 1-y,
				\end{equation}          
				pour tout $y \in [0,1]$. 
				En effet, si $x < G_{X}(y)$, alors $F_{X}(x) < y$, vu que $G_{X}(y)$ est le minimum de $A_{y}$. 
				Pour $n \in \NN^{*}$, on pose $x_{n} = G_{X}(y) - 1/n$, ce qui dit que $F_{X}(x_{n}) < y$, \textit{i.e.}
				\begin{equation}
				\label{eq:aux}
				\mathbb{P}\big( \{ X \leq x_{n} \} \big) < y,     
				\end{equation}   
				pour tout $n \in \NN^{*}$. 
				On voit bien que la suite des évènements $( \{ X \leq x_{n} \} )_{n \in \NN^{*}}$ est croissante au sens large et que
				\[     \bigcup_{n=1}^{+ \infty} \{ X \leq x_{n} \} = \{ X < G_{X}(y) \},     \]
				ce qui nous dit que 
				\[        \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\big( \{ X \leq x_{n} \} \big) = \mathbb{P}\big( \{ X < G_{X}(y) \} \big).     \]
				En outre, \eqref{eq:aux} implique que  
				\[        \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\big( \{ X \leq x_{n} \} \big) \leq y,     \]
				ce qui nous dit que $\mathbb{P}( \{ X < G_{X}(y) \}) \leq y$, d'où \eqref{eq:med2} suit directement. 
				
				Les identités \eqref{eq:med1} et \eqref{eq:med2} nous disent que $G_{X}(1/2)$ est une médiane de $X$. 
				Pour montrer que l'on n'a pas d'unicité de médiane, considérer par exemple la variable aléatoire $X$ de loi uniforme sur $[\![ 1, 2 ]\!]$. 
				On voit bien que tout élément dans l'intervalle $[1,2]$ est une médiane de $X$. 
				C'est facile à montrer que l'ensemble de médianes $\mathscr{M}_{X} \subseteq \RR$ de $X$ est un intervalle de $\RR$: 
				il suffit de montrer que si $m_{1} < m_{2}$ sont deux médianes de $X$ alors $m \in [m_{1},m_{2}]$ est aussi une médiane de $X$, ce qui est immédiat de la définition. 
				On définit (habituellement) \textbf{la} médiane $m_{X}$ de $X$ comme le milieu de l'intervalle $\mathscr{M}_{X}$. 
				
				\item On peut prendre le minimum de $A_{1/2} = \{ x \in \RR : F_{X}(x) \geq 1/2 \} = F_{X}^{-1}([1/2,1])$. 
				
				\item Pour le démontrer, on montre d'abord l'inégalité de Markov, \textit{i.e.} si $Y$ est une variable aléatoire à valeurs dans $\RR_{\geq 0}$, alors 
				\begin{equation}
				\tag{Mar}
				\label{eq:mar}
				\mathbb{P}\big( \{ Y \geq c\} \big) \leq \frac{E[Y]}{c},
				\end{equation}
				pour tout $c > 0$. 
				On fait la preuve dans le cas discrète. 
				Soit $R_{Y} \subseteq \RR_{\geq 0}$ le rang de $Y$. 
				Alors,
				\[     E[Y] = \sum_{y \in R_{Y}} y \mathbb{P}\big( \{ Y = y \} \big) \geq \sum_{\text{\begin{tiny}$\begin{matrix}y \in R_{Y}\\y \geq c\end{matrix}$\end{tiny}}} y \mathbb{P}\big( \{ Y = y \} \big) \geq  c \sum_{\text{\begin{tiny}$\begin{matrix}y \in R_{Y}\\y \geq c\end{matrix}$\end{tiny}}} \mathbb{P}\big( \{ Y = y \} \big) = c \mathbb{P}\big( \{ Y \geq c \} \big).     \]
				
				Soit $\alpha \geq 0$. 
				On pose $Z = X - \mu + \alpha$ et $Y = Z^{2}$. 
				L'inégalité \eqref{eq:mar} nous donne que 
				\[     \mathbb{P}\big( \{ X - \mu \geq c \} \big) = \mathbb{P}\big( \{ Z \geq c + \alpha \} \big) \leq \mathbb{P}\big( \{ Y \geq (c+\alpha)^{2} \} \big)  \leq \frac{E[Y]}{(c+\alpha)^{2}} = \frac{\sigma^{2}+\alpha^{2}}{(c+\alpha)^{2}},     \]
				pour tout $c > 0$. 
				Comme l'inégalité précédente est valable pour tout $\alpha \geq 0$, elle est en particulier vraie pour $u = \sigma^{2}/c$, ce qui implique que 
				\[     \mathbb{P}\big( \{ X - \mu \geq c \} \big) \leq \frac{\sigma^{2}}{c^{2}+\sigma^{2}},     \]
				pour tout $c > 0$, ou, de façon équivalente, l'inégalité de Cantelli, \textit{i.e.}
				\begin{equation}
				\tag{Can}
				\label{eq:cant}
				\mathbb{P}\big( \{ X - \mu \geq C \sigma \} \big) \leq \frac{1}{C^{2}+1},     
				\end{equation}
				pour tout $C > 0$. 
				
				Or, si l'on utilise \eqref{eq:cant} avec $C = 1$, on trouve que 
				\[     \mathbb{P}\big( \{ X \geq \mu + \sigma \} \big) \leq \frac{1}{2}.     \] 
				Si l'on remplace $X$ par $-X$ (et $\mu$ par $-\mu$), on trouve que 
				\[     \mathbb{P}\big( \{ X \leq \mu - \sigma \} \big) \leq \frac{1}{2}.     \] 
				Cela nous dit que toute médiane $m$ de $X$ est dans l'intervalle $[\mu-\sigma,\mu+\sigma]$ et en conséquence 
				$|m - \sigma| \leq \sigma$, \textit{i.e.} 
				$(m - \sigma)^{2} \leq \sigma^{2}$. 
				
				\item Si $X$ suit une loi uniforme sur $\{1,\cdots ,n\}$, alors $\mu = m_{X} = (n+1)/2$.
				Si $X$ suit une loi binomiale de paramètre $(n,p)$, alors $\mu = np$ et $m_{X} = \lfloor np \rfloor$ ou $m_{X} = \lceil np \rceil$. 
				Si $X$ suit une loi géométrique de paramètre $p$, alors $\mu = 1/p$ et $m = \lceil -\ln(2)/\ln(1-p) \rceil$. 
				Finalement, si $X$ suit une loi de Poisson de paramètre $\lambda$, alors $\mu = \lambda$ et $m \simeq \lfloor \lambda + 1/3 - 0.02/\lambda \rfloor$.
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{L'urne de Pólya}
		Soient $a\ge0$, $b\ge0$ et $c\ge0$ des entiers avec $a+b\ge1$.  
		Une urne contient $a$ boules rouges et $b$ boules noires. 
		Si l'on tire une boule, on remet dans l'urne $c$ boules de la couleur de la boule tirée 
		(le cas du tirage avec remise simple est donnée par $c=1$ et celui du tirage sans remise par $c=0$).
		\begin{enumerate}
			\item Calculer la probabilité qu'au deuxième tirage, on tire une boule rouge.
			
			\item Calculer la probabilité qu'au troisième tirage, on tire une boule rouge.
			
			\item On note $X_i$ la variable aléatoire valant $1$ si l'on tire une boule rouge au tirage numéro $i$ et $0$ sinon.
			Les variables $X_1$ et $X_2$ sont-elles indépendantes ?
			
			\item Que représente la variable aléatoire $S_i=X_1+\dots +X_i$ ?
			
			\item Soit $k\in\NN$ tel que $\mathbb{P}(\{S_i=k\})>0$.
			Calculer $\mathbb{P}(\{X_{i+1}=1\} | \{S_i=k\})$.
			
			\item En utilisant la formule des probabilités totales montrer que
			\[     \mathbb{P}\big(\{X_{i+1}=1\}\big)=\frac{(c-1)E[S_i]+a}{a+b+i(c-1)}.     \] 
			En déduire la valeur de $\mathbb{P}(\{X_i=1\})$.
		\end{enumerate} 
		
		\begin{preuve}
			\begin{enumerate}
				\item Soient $R_{n}$ et $N_{n}$ l'évènement ``extraire un boule rouge au $n$-ème tirage'' et ``extraire un boule noire au $n$-ème tirage'', 
				pour $n \in \NN^{*}$. 
				C'est facile à voir que 
				\begin{align*}
				\mathbb{P}\big(R_{2}\big) &= \mathbb{P}\big(R_{2} | R_{1}\big) \mathbb{P}\big(R_{1}\big) + \mathbb{P}\big(R_{2} | N_{1}\big) \mathbb{P}\big(N_{1}\big) 
				\\
				&= \frac{a+c-1}{a+b+c-1} \frac{a}{a+b} + \frac{a}{a+b+c-1} \frac{b}{a+b} = \frac{a}{a+b}.
				\end{align*}       
				
				\item On voit bien que 
				\begin{align*}
				\hskip -0.6cm   \mathbb{P}\big(R_{3}\big) &= \mathbb{P}\big(R_{3} | R_{2} \cap R_{1} \big) \mathbb{P}\big(R_{2} \cap R_{1}\big) + \mathbb{P}\big(R_{3} | N_{2} \cap R_{1} \big) \mathbb{P}\big(N_{2} \cap R_{1}\big) + \mathbb{P}\big(R_{3} | R_{2} \cap N_{1} \big) \mathbb{P}\big(R_{2} \cap N_{1}\big) 
				\\
				&\phantom{=}+ \mathbb{P}\big(R_{3} | N_{2} \cap N_{1} \big) \mathbb{P}\big(N_{2} \cap N_{1}\big)
				\\
				&= \frac{a+2(c-1)}{a+b+2(c-1)} \frac{a+c-1}{a+b+c-1} \frac{a}{a+b} + \frac{a+c-1}{a+b+2(c-1)} \frac{b}{a+b+c-1} \frac{a}{a+b} 
				\\
				&\phantom{=}+ \frac{a+c-1}{a+b+2(c-1)} \frac{a}{a+b+c-1} \frac{b}{a+b} + \frac{a}{a+b+2(c-1)} \frac{b+c-1}{a+b+c-1} \frac{b}{a+b}
				\\
				&= \frac{a}{a+b}. 
				\end{align*}   
				
				\item Les variables $X_1$ et $X_2$ ne sont pas indépendantes si $c \neq 1$ et $b \neq 0$, puisque
				\[     \mathbb{P}\big(\{X_{1} = X_{2} = 1\}\big) = \frac{a+c-1}{a+b+c-1} \frac{a}{a+b} \neq \frac{a}{a+b} \frac{a}{a+b} = \mathbb{P}\big(\{X_{1} = 1\}\big) \mathbb{P}\big(\{X_{2} = 1 \}\big).     \]
				
				\item Il s'agit de la quantité de boules rouges que l'on a tirée lors de $i$ tirages. 
				
				\item On voit bien que 
				\[     \mathbb{P}\big(\{S_{i} = k \}\big) = \begin{pmatrix} i \\ k \end{pmatrix} \frac{\prod_{\ell=0}^{k-1} \big(a+\ell (c-1)\big) \prod_{m=0}^{i-k-1} \big(b+m(c-1)\big)}{\prod_{n=0}^{i-1} \big(a+b+n(c-1)\big)}     \]
				et
				\[     \mathbb{P}\big(\{X_{i+1} = 1, S_{i} = k \}\big) = \begin{pmatrix} i \\ k \end{pmatrix} \frac{\prod_{\ell=0}^{k} \big(a+\ell (c-1)\big) \prod_{m=0}^{i-k-1} \big(b+m(c-1)\big)}{\prod_{n=0}^{i} \big(a+b+n(c-1)\big)}     \]
				pour tout $k \in [\![ 0, i ]\!]$, si $c \geq 1$, et $k \in [\![ \max(0,i-b), \min(i,a) ]\!]$ si $c = 0$. 
				Les probabilités respectives sont nulles sinon. 
				Cela nous dit que 
				\[     \mathbb{P}\big(\{X_{i+1}=1\} | \{S_i=k\}\big) = \frac{a+k(c-1)}{a+b+i(c-1)},     \] 
				si $k$ est dans le rang de $S_{i}$, \textit{i.e.} $\mathbb{P}(\{S_i=k\})>0$. 
				
				\item Soit $R_{S_{i}}$ le rang de la variable aléatoire $S_{i}$. 
				C'est clair que 
				\begin{align*}
				\mathbb{P}\big(\{X_{i+1}=1\}\big)  &= \sum_{k \in R_{S_{i}}} \mathbb{P}\big(\{X_{i+1}=1\} | \{S_i=k\}\big) \mathbb{P}\big(\{S_i=k\}\big) 
				\\
				&= \sum_{k \in R_{S_{i}}} \frac{a+k(c-1)}{a+b+i(c-1)} \mathbb{P}\big(\{S_i=k\}\big) = \frac{a+(c-1)E[S_i]}{a+b+i(c-1)}.
				\end{align*}          
				Comme $E[S_{i}] = \sum_{j=1}^{i} E[X_{j}] = \sum_{j=1}^{i} \mathbb{P}(\{X_j=1\})$, alors 
				\[     \mathbb{P}\big(\{X_{i+1}=1\}\big)  = \frac{a+(c-1) \sum_{j=1}^{i} \mathbb{P}\big(\{X_j=1\}\big)}{a+b+i(c-1)}.     \]
				On va montrer par récurrence que $\mathbb{P}(\{X_j=1\}) = a/(a+b)$, pour tout $j \in \NN$. 
				C'est clair que $\mathbb{P}(\{X_1=1\}) = a/(a+b)$. 
				On suppose que c'est vrai pour tout $j \in [\![ 1, i ]\!]$. 
				Alors , 
				\begin{align*}
				\mathbb{P}\big(\{X_{i+1}=1\}\big)  &= \frac{a+(c-1) \sum_{j=1}^{i} \mathbb{P}\big(\{X_j=1\}\big)}{a+b+i(c-1)} = 
				\frac{a+ \frac{(c-1) i a}{a+b}}{a+b+i(c-1)} = \frac{a}{a+b},     
				\end{align*}   
				comme on voulait démontrer. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Dés et loi uniforme}
		On va résoudre le problème suivant. 
		Peut-on truquer deux dés de telle façon que la loi de la somme des points obtenus soit la loi uniforme sur $\{2, \dots,12\}$ ?
		\begin{enumerate}
			\item Soit $X$ une variable aléatoire à valeurs dans l'ensemble $\{1,\dots,6\}$. 
			On suppose que $\mathbb{P}(\{ X=6 \})\neq0$. 
			Soit $G_X$ sa fonction génératrice.
			Montrer qu'il existe un polynôme $H_X$ ayant au moins une racine réelle tel que $G_X(s)=sH_X(s)$, pour tout $s\in \RR$.
			
			\item Soit $Z$ une variable aléatoire de loi uniforme sur $\{2,\dots,12\}$. 
			Montrer qu'il existe un polynôme $K$ tel que $G_Z(s)=s^2K(s)$, pour tout $s\in\RR$. 
			Montrer que $K$ n'a pas de racines réelles.
			
			\item Répondre à la question initiale.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item C'est clair que 
				\[     G_X(t) = \sum_{i=1}^{6}  \mathbb{P}\big(\{X = i\}\big) t^{i} = t \bigg( \sum_{i=0}^{5}  \mathbb{P}\big(\{X = i+1\}\big) t^{i}\bigg).     \]
				Si l'on pose $H_{X}(t) = \sum_{i=0}^{5}  \mathbb{P}\big(\{X = i+1\}\big) t^{i}$, il s'agit d'un polynôme de degré $5$ (puisque $\mathbb{P}(\{ X=6 \})\neq0$). 
				Comme la limite de $H_{X}(t)$ est $\pm \infty$ quand $t$ tend vers $\pm \infty$ et $H_{X}$ est une fonction continue, 
				elle admet au moins une racine réelle. 
				
				\item On voit bien que 
				\[     G_{Z}(t) = \sum_{i=2}^{12}  \mathbb{P}\big(\{Z = i\}\big) t^{i} = t^{2}\underset{K(t)}{\underbrace{\bigg( \sum_{i=0}^{10} \frac{t^{i}}{11} \bigg)}}.     \]
				Comme 
				\[     K(t) = \sum_{i=0}^{10}  \frac{t^{i}}{11} = \frac{t^{11}-1}{11(t-1)},     \]
				les racines de $K$ sont exactement les racines primitives de l'unité d'ordre $11$, qui ne sont pas réelles. 
				
				\item Si $X$ et $Y$ sont deux variables aléatoires indépendantes à valeurs dans l'ensemble $\{1,\dots,6\}$ telles que 
				$\mathbb{P}(\{ X=6 \}), \mathbb{P}(\{ Y=6 \})\neq0$, alors 
				$X+Y$ est une variable aléatoire à valeurs dans l'ensemble $\{2,\dots,12\}$. 
				L'indépendance nous dit que $G_{X+Y}(t) = G_{X}(t) G_{Y}(t) = t^{2} H_{X}(t) H_{Y}(t)$, avec $H_{X}$ et $H_{Y}$ polynômes 
				avec au moins une racine réelle. 
				En conséquence, l'item précédent nous dit que $X+Y$ n'est pas une variable aléatoire de loi uniforme sur $\{2,\dots,12\}$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Perte au casino}
		On considère un jeu au casino qui est tel qu'à chaque partie le joueur a une probabilité $p$ de gagner 
		et une probabilité $1-p$ de perdre.
		Son gain est $+1$ s'il gagne et de $-1$ s'il perd. 
		Pour $n \in \NN$, on note $\epsilon_n$ la variable aléatoire représentant son gain à la $n$-ème  partie.
		Soit $X_n=\epsilon_1+\dots +\epsilon_n$ son gain au bout de $n$ parties.
		
		\begin{enumerate}
			\item Donner la loi de $X_n$. 
			\\
			\textbf{Indication :} poser $Z_k=\epsilon_k+1/2$ pour se ramener à une loi connue.
			
			\item Si sa fortune initiale est $i \in \NN$, sa fortune au bout de $n$ parties est donnée par $i+X_n$. 
			On suppose que le joueur s'arrête dès que sa fortune vaut $0$ (\textit{i.e.} il est ruiné) ou une somme $N \in \NN^{*}$ avec $N \geq i$. 
			On note $p_N(i)$ la probabilité qu'il atteigne la fortune $N$. 
			On a donc $p_N(0)=0$ et $p_N(N)=1$.
			\begin{enumerate}[label=(\roman*)]
				\item Montrer que si $N\geq 2$, $p_N(1)=p.p_N(2)$.
				\item Donner une relation entre $p_N(i-1)$, $p_N(i)$ et $p_N(i+1)$ si $1\leq i\leq N-1$. 
				En déduire la valeur de $p_N(i)$ en fonction de $p$, $i$ et $N$.
				\item Déterminer la limite de $p_N(i)$ quand $N$ tend vers $+\infty$.
			\end{enumerate}
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item C'est clair que $Z_k=\epsilon_k+1/2$ est une variable aléatoire de loi binomiale de paramètres $1$ et $p$. 
				Alors $Z = \sum_{k=1}^{n} Z_{k}$ est une variable aléatoire de loi binomiale de paramètres $n$ et $p$. 
				Comme $X_{n} = (Z+n)/2$, le rang de $X$ est $\{ -n+2j : j \in [\![ 0, n ]\!] \}$ et 
				\[     \mathbb{P}\big(\{X_{n} = -n+2j \}\big) = \mathbb{P}\big(\{Z = j \}\big) = \begin{pmatrix} n \\ j \end{pmatrix} p^{j} (1-p)^{n-j},     \]
				pour tout $j \in [\![ 0, n ]\!]$. 
				
				\item 
				\begin{enumerate}[label=(\roman*)]
					\item C'est clair que $p_N(1)$ avec $N\geq 2$ est la probabilité de l'intersection des évènement ``le jouer gagne la première partie'' et ``le jouer atteigne la fortune $N$ avec une fortune initiale $2$''. 
					Comme ces deux évènements sont indépendants, l'identité $p_N(1)=p.p_N(2)$ est immédiate. 
					En particulier, $p_{2}(1) = p$. 
					
					\item C'est clair que $p_N(i)$ avec $N \geq i+1$ et $i \in \NN^{*}$ est la probabilité de la réunion disjointe de 
					\begin{itemize} 
						\item l'intersection des évènement ``le jouer gagne la première partie'' et ``le jouer atteigne la fortune $N$ avec une fortune initiale $i+1$'';
						\item l'intersection des évènement ``le jouer perd la première partie'' et ``le jouer atteigne la fortune $N$ avec une fortune initiale $i-1$''.
					\end{itemize}
					Comme les évènements dans les intersections sont indépendants, cela implique que $p_N(i) = p . p_{N}(i+1) + (1-p) . p_{N}(i-1)$, ou, 
					de façon équivalente, 
					\[     p_N(i+1) - \frac{1}{p} . p_{N}(i) + \frac{1-p}{p} . p_{N}(i-1) = 0.     \]
					Dans ce cas, l'équation quadratique associée est 
					\[     X^{2} - \frac{1}{p} X + \frac{1-p}{p} = (X-1) \bigg(X + \frac{p-1}{p}\bigg).     \]
					Si $1-p \neq p$, cela nous dit que 
					\[     p_{N}(i) = a_{N} + b_{N} \bigg(\frac{1-p}{p}\bigg)^{i},     \]
					tandis que si $1-p \neq p$, \textit{i.e.} $p= 1/2$, alors 
					\[     p_{N}(i) = a_{N} + b_{N} i,     \]
					pour tout $i \in \NN$ tel que $N \geq i$, où $a_{N}$ et $b_{N}$ sont des constantes à déterminer (qui dépendent de $p$ et de $N$). 
					Les conditions $p_{N}(0)= 0$ dit que $a_{N} = -b_{N}$, si $1-p \neq p$, et $a_{N} = 0$, si $p=1/2$, 
					tandis que $p_{N}(N)= 1$ nous dit que 
					\[     a_{N} = \frac{1}{1 - \bigg(\frac{1-p}{p}\bigg)^{N}} = \frac{p^{N}}{p^{N} - (1-p)^{N}},     \]
					si $1-p \neq p$, et $b_{N} = 1/N$ si $p=1/2$.
					En conséquence, 
					\[     p_{N}(i) = p^{N-i} \frac{p^{i} - (1-p)^{i}}{p^{N} - (1-p)^{N}},     \]
					si $1-p \neq p$, et $p_{N}(i) = i/N$, si $p=1/2$, pour tout $i \in \NN$ tel que $N \geq i$.
					
					\item Si $p=1/2$, c'est clair que 
					\[     \underset{N \rightarrow + \infty}{\lim} p_{N}(i) = \underset{N \rightarrow + \infty}{\lim} \frac{i}{N} = 0,     \]
					pour tout $i \in \NN$. 
					En outre, si $1-p \neq p$, on voit bien que 
					\[     \underset{N \rightarrow + \infty}{\lim} a_{N} = \underset{N \rightarrow + \infty}{\lim} \frac{1}{1 - \bigg(\frac{1-p}{p}\bigg)^{N}} = \begin{cases}
					1, &\text{si $1- p < p$,}
					\\
					0, &\text{si $1- p > p$,}
					\end{cases}     \]
					pour tout $i \in \NN$, ce qui implique que 
					\[     \underset{N \rightarrow + \infty}{\lim} p_{N}(i) = \begin{cases}
					\frac{p^{i} - (1-p)^{i}}{p^{i} }, &\text{si $1- p < p$,}
					\\
					0, &\text{si $1- p > p$,}
					\end{cases}     \]
					pour tout $i \in \NN$. 
				\end{enumerate}
			\end{enumerate}
		\end{preuve}
		
	

\newpage \section{Espaces probabilisés, variables aléatoires à densité}

\def\Functionf2(#1){(#1)^2}%
\def\FunctionF2(#1){(#1)^3/3}%

		%%%%%%%%%%%%%%%
		\exo{}
		Existe-t-il $c\in \RR$ tel que 
		\[     f(x)= \begin{cases} cx(1-x), &\text{si $x \in [0,1]$,} 
		\\
		0, &\text{si $x \in \RR \setminus [0,1]$,}
		\end{cases}
		\] 
		soit une densité de probabilité ?
		Répondre la même question avec $g(x)=d e^{-x^2+4x}$.
		
		\begin{preuve}
			On remarque qu'une fonction $f : \RR \rightarrow \RR$ est une densité de probabilité si et seulement si $\operatorname{Im}(f) \subseteq \RR_{\geq 0}$, elle est intégrable et 
			\begin{equation}
			\label{eq:nor}
			\underset{\RR}{\int} f(x) dx = 1.
			\end{equation}    
			Comme la fonction $f$ est continue et à support compact, elle est intégrable sur $\RR$. 
			La condition \eqref{eq:nor} équivaut à 
			\[     1 = \int_{0}^{1} c x (1-x) dx = c \bigg[ \frac{x^{2}}{2} - \frac{x^{3}}{3} \bigg]_{0}^{1} = \frac{c}{6},     \]
			\textit{i.e.} $c = 6$. 
			On voit bien que la fonction $g$ est intégrable sur $\RR$, puisque 
			\[     \underset{\RR}{\int} d e^{-x^2+4x} dx = \underset{\RR}{\int} d e^{-(x-2)^2+4} dx = d e^{4} \underset{\RR}{\int} e^{-y^2} dy 
			= \sqrt{\pi} d e^{4}.     \]
			où l'on a fait la substitution $y = x - 2$ et on a utilisé l'exercice \textbf{19} de la fiche 3.
			En conséquence, $g$ est une densité si et seulement si $d = e^{-4}/\sqrt{\pi}$. 
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $\alpha \in \RR_{> 0}$.
		Soit $X$ une variable aléatoire de densité $f$ donnée par
		\[     f(x)= \begin{cases} x^2, &\text{si $x \in \hskip 0.6mm ] \hskip 0.6mm 0,\alpha \hskip 0.6mm [ \hskip 0.6mm$,} 
		\\
		0, &\text{si $x \in \RR \setminus \hskip 0.6mm ] \hskip 0.6mm 0,\alpha \hskip 0.6mm [ \hskip 0.6mm$.}
		\end{cases}
		\] 
		\begin{enumerate}
			\item Que vaut $\alpha$ ?
			\item Représenter $f$ et la fonction de répartition de $X$.
			\item Que vaut $\mathbb{P}(\{X>1\})$ ?
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Comme la fonction $f$ est continue par morceaux et à support compact, elle est intégrable sur $\RR$. 
				La condition \eqref{eq:nor} équivaut à 
				\[     1 = \int_{0}^{\alpha} x^{2} dx = \bigg[ \frac{x^{3}}{3} \bigg]_{0}^{\alpha} = \frac{\alpha^{3}}{3},     \]
				\textit{i.e.} $\alpha = \sqrt[3]{3} > 1$.
				
				\item On rappelle que la fonction de répartition $F$ associée à une densité $f$ est donnée par 
				\[     F(x) = \int_{- \infty}^{x} f(s) ds,     \]
				pour tout $x \in \RR$.
				Dans ce cas, on trouve alors 
				\[     F(x)= \begin{cases}
				0, &\text{si $x \in \RR_{\leq 0}$},
				\\
				\frac{x^{3}}{3}, &\text{si $x \in \hskip 0.6mm ] \hskip 0.6mm 0,\alpha \hskip 0.6mm [ \hskip 0.6mm$,} 
				\\
				1, &\text{si $x \in \RR_{\geq \alpha}$.}
				\end{cases}
				\] 
				La représentation graphique de $f$ et de $F$ est 
				
				\begin{tikzpicture}
				\begin{axis}[
				axis y line=center,
				axis x line=middle, 
				axis on top=true,
				xmin=-1,
				xmax=3,
				ymin=-1,
				ymax=3,
				height=4.0cm,
				width=4.0cm,
				%        grid,
				xtick={-1,...,3},
				ytick={-1,...,3},
				]
				\addplot [domain=0:1.442, samples=50, mark=none, thick, blue] {\Functionf2(x)};
				\node [left, blue] at (axis cs: 1,1.5) {$f$};
				\addplot [domain=0:1.442, samples=50, mark=none, thick, red] {\FunctionF2(x)};
				\node [left, red] at (axis cs: 2,0.5) {$F$};
				\draw[red,thick] (axis cs:1.442,1) -- (axis cs:3,1);
				\draw[blue,thick] (axis cs:1.442,0) -- (axis cs:3,0);
				\draw[red,thick] (axis cs:-1,0) -- (axis cs:0,0);
				\draw[blue,thick] (axis cs:-1,0) -- (axis cs:0,0);
				\end{axis}
				\end{tikzpicture}
				
				\item On rappelle que 
				\[     \mathbb{P}\big(\{X > x \}\big) = 1 - \mathbb{P}\big(\{X\leq x \}\big) = 1 - F(x),     \]
				pour tout $x \in \RR$.
				Comme $1 \in \hskip 0.6mm ] \hskip 0.6mm 0,\alpha \hskip 0.6mm [ \hskip 0.6mm$, $\mathbb{P}(\{X>1\}) = 1 - 1/3 = 2/3$. 
			\end{enumerate}
		\end{preuve} 
		
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $X$ de loi uniforme sur $[-2,1]$.
		Quelle est la loi de $|X|$?
		
		\begin{preuve} 
			On va montrer que $|X|$ est une variable aléatoire absolument continue, \textit{i.e.} elle admet une densité, et on va déterminer sa densité. 
			Comme $X$ est une variable aléatoire de loi uniforme sur $[-2,1]$, sa fonction de densité est $f_{X} = \mathbb{1}_{[-2,1]}/3$, \textit{i.e.} 
			un tiers de la fonction indicatrice de $[-2,1]$. 
			Or, la fonction de répartition $F_{|X|}$ de $|X|$ est donnée par 
			\[     F_{|X|}(y) = \mathbb{P}\big(\{ |X|\leq y \}\big),     \]
			pour tout $y \in \RR$. 
			C'est clair que $F_{|X|}(y) = 0$ pour tout $y < 0$, puisque $\{ |X|\leq y \} = \emptyset$, si $y < 0$. 
			En outre, si  $y \geq 0$, alors 
			\[     F_{|X|}(y) = \mathbb{P}\big(\{ -y \leq X \leq y \}\big) = \int_{-y}^{y} f_{X}(x) dx.     \]
			En conséquence, si $y \in [0,1]$, on a 
			\[     F_{|X|}(y) = \int_{-y}^{y} \frac{1}{3} dx = \frac{2 y}{3},     \]
			si $y \in [1,2]$, on a 
			\[     F_{|X|}(y) = \int_{-y}^{1} \frac{1}{3} dx = \frac{1 + y}{3},     \]
			et $F_{|X|}(y) = 1$ si $y \geq 2$. 
			
			On rappelle qu'une fonction de répartition $G : \RR \rightarrow [0,1]$ admet une densité si et seulement s'il existe une fonction $g : \RR \rightarrow \RR_{\geq 0}$ intégrable telle que 
			\[     G(x) = \int_{-\infty}^{x} g(s) ds,     \]
			pour tout $x \in \RR$. 
			Noter que la fonction $g$ n'est pas unique. 
			Dans ce cas, c'est facile à vérifier que$|X|$ admet la densité 
			\[     f_{|X|}(x)= \begin{cases} \frac{2}{3}, &\text{si $x \in [ 0, 1 \hskip 0.6mm [ \hskip 0.6mm$,} 
			\\
			\frac{1}{3}, &\text{si $x \in [ 1, 2 \hskip 0.6mm [ \hskip 0.6mm$,}
			\\
			0, &\text{si $x \in \RR \setminus [ 0,2 \hskip 0.6mm [ \hskip 0.6mm$.} 
			\end{cases}
			\] 
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $X$ une variable aléatoire de densité $f$ donnée par
		\[     f(x)= \begin{cases} x, &\text{si $x \in [ 0, 1 \hskip 0.6mm [ \hskip 0.6mm$,} 
		\\
		2 - x, &\text{si $x \in [ 1, 2 \hskip 0.6mm [ \hskip 0.6mm$,}
		\\
		0, &\text{si $x \in \RR \setminus [ 0,2 \hskip 0.6mm [ \hskip 0.6mm$.} 
		\end{cases}
		\] 
		\begin{enumerate}
			\item Représenter $f$.
			\item Calculer $\mathbb{P}(\{ X>1 \})$.
			\item Soit $0<b<1$.
			Calculer $\mathbb{P}(\{ 1-b<X\leq 1+b \})$.
			\item Montrer que les événements $\{X >1 \}$ et $\{1-b<X\leq 1+b\}$ sont indépendants.
		\end{enumerate} 
		
		\begin{preuve} 
			\begin{enumerate}
				\item On voit bien que 
				
				\begin{tikzpicture}
				\begin{axis}[
				axis y line=center,
				axis x line=middle, 
				axis on top=true,
				xmin=-1,
				xmax=3,
				ymin=-1,
				ymax=2,
				height=3.0cm,
				width=4.0cm,
				%        grid,
				xtick={-1,...,3},
				ytick={-1,...,2},
				]
				%    \addplot [domain=0:1.442, samples=50, mark=none, thick, blue] {\Functionf2(x)};
				\node [blue, blue] at (axis cs: 1,1.5) {$f$};
				\draw[blue,thick] (axis cs:0,0) -- (axis cs:1,1);
				\draw[blue,thick] (axis cs:1,1) -- (axis cs:2,0);
				\draw[blue,thick] (axis cs:-1,0) -- (axis cs:0,0);
				\draw[blue,thick] (axis cs:2,0) -- (axis cs:3,0);
				\end{axis}
				\end{tikzpicture}
				
				\item C'est clair que 
				\[     \mathbb{P}\big(\{X>1\}\big) = 1 - \mathbb{P}\big(\{X\leq1\}\big) = 1 - \int_{- \infty}^{1} f(x) dx = 1 - \int_{0}^{1} x dx = 1 - \frac{1}{2} = \frac{1}{2}.     \]
				
				\item On voit bien que 
				\begin{align*}
				\mathbb{P}\big(\{1-b < X \leq 1 + b \}\big) &= \mathbb{P}\big(\{X\leq1+b\}\big) - \mathbb{P}\big(\{X\leq1-b\}\big) = \int_{1-b}^{1+b} f(x) dx 
				\\ 
				&= \int_{1-b}^{1} x dx + \int_{1}^{1+b} (2-x) dx = \int_{1-b}^{1} x dx + \int_{1-b}^{1} y dy 
				\\
				&= 2 \int_{1-b}^{1} x dx = \bigg[ x^{2} \bigg]_{1-b}^{1} = b(2-b),
				\end{align*}          
				où l'on a fait la substitution $y = 2 - x$.
				
				\item Il faut montrer que $\mathbb{P}(\{X >1 \} \cap \{1-b<X\leq 1+b\}) = \mathbb{P}(\{X >1 \}) \mathbb{P}(\{1-b<X\leq 1+b\})$. 
				Or, comme $\{X >1 \} \cap \{1-b<X\leq 1+b\} = \{1<X\leq 1+b\}$, 
				\begin{align*}
				\mathbb{P}\big(\{X >1 \} &\cap \{1-b<X\leq 1+b\}\big) = \mathbb{P}\big(\{X\leq1+b\}\big) - \mathbb{P}\big(\{X\leq1 \}\big) 
				= \int_{1}^{1+b} f(x) dx
				\\
				&= \int_{1}^{1+b} (2-x) dx = \int_{1-b}^{1} y dy = 2 \bigg[ \frac{y^{2}}{2} \bigg]_{1-b}^{1} = \frac{b(2-b)}{2},
				\end{align*}
				où l'on a fait la substitution $y = 2 - x$. 
				En outre, d'après les items précédents 
				\[     \mathbb{P}\big(\{X >1 \}\big) \mathbb{P}\big(\{1-b<X\leq 1+b\}\big) = \frac{b(2-b)}{2},     \]
				comme on voulait démontrer. 
			\end{enumerate}
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $(X,Y)$ un couple de variables aléatoires réelles à valeurs dans l'ensemble $D=\{(x,y) \in \RR^2 : 0 \leq x\leq y\}$ 
		et dont la fonction de répartition est donnée par 
		\[     F_{X,Y}(x,y)=1-e^{-x}-xe^{-y},     \]
		si $(x,y)\in D$. 
		\begin{enumerate}
			\item Déterminer $F_X$ et $F_Y$.
			\item $(X,Y)$ admet-il une densité sur $\RR^2$ ?
			\item Déterminer les lois marginales de $(X,Y)$.
			\item Calculer $\mathbb{P}(\{X \leq 1\} | \{ Y>2\})$.
			\item Calculer $\mathbb{P}(\{Y\leq 2X\})$.
		\end{enumerate} 
		
		\begin{preuve} 
			\begin{enumerate}
				\item L'énoncé nous dit que 
				\[     \mathbb{P}\Big(\big\{(X,Y) \in E \big\} \Big) = 0,     \]
				pour tout $E \subseteq \RR^{2}$ tel que $E \cap D = \emptyset$. 
				Cela implique immédiatement que 
				\[     F_{X,Y}(x,y) = \mathbb{P}\big(\{ X \leq x ,Y \leq y \} \big) = 0,     \]
				pour tout $(x,y) \in \RR^{2}$ tel que $y < 0$, ou $x < 0$. 
				En plus, si $(x,y) \in \RR^{2}$ avec $y \leq x$, alors 
				\[     \RR_{\leq x} \times \RR_{\leq y} =  (\RR_{\leq y} \times \RR_{\leq y}) \sqcup \underset{A}{\underbrace{(\hskip 0.6mm ] \hskip 0.6mm y,x] \times \RR_{\leq y})}}.     \]
				Comme $A \cap D = \emptyset$, alors $\mathbb{P}((X,Y) \in A) = 0$, ce qui implique que 
				\[     F_{X,Y}(x,y) = \mathbb{P}\big(\{ X \leq x ,Y \leq y \} \big) = \mathbb{P}\big(\{ X \leq y ,Y \leq y \} \big) = F_{X,Y}(y,y),     \]
				pour tout $(x,y) \in \RR^{2}$ avec $y \leq x$. 
				
				Pour calculer $F_{X}$, on utilise que 
				\[     F_{X}(x) = \underset{y \rightarrow + \infty}{\lim} F_{X,Y}(x,y),     \]
				pour tout $x\in \RR$. 
				Cela implique que $F_{X}(x) = 0$ si $x < 0$.
				En outre, si $x \geq 0$, on a 
				\[     F_{X}(x) = \underset{y \rightarrow + \infty}{\lim} F_{X,Y}(x,y) = \underset{\text{\begin{tiny}$\begin{matrix}y \rightarrow + \infty\\ x \leq y\end{matrix}$\end{tiny}}}{\lim} F_{X,Y}(x,y) = \underset{\text{\begin{tiny}$\begin{matrix}y \rightarrow + \infty\\ x \leq y \end{matrix}$\end{tiny}}}{\lim} 1-e^{-x}-xe^{-y} = 1 - e^{-x}.   \]
				Pour calculer $F_{Y}$, on utilise que 
				\[     F_{Y}(y) = \underset{x \rightarrow + \infty}{\lim} F_{X,Y}(x,y),     \]
				pour tout $y \in \RR$. 
				Cela implique que $F_{Y}(y) = 0$ si $y < 0$.
				En outre, si $y \geq 0$, on a 
				\[     F_{Y}(y) = \underset{x \rightarrow + \infty}{\lim} F_{X,Y}(x,y) = \underset{\text{\begin{tiny}$\begin{matrix}x \rightarrow + \infty\\ y \leq x\end{matrix}$\end{tiny}}}{\lim} F_{X,Y}(x,y) = \underset{\text{\begin{tiny}$\begin{matrix}x \rightarrow + \infty\\ y \leq x\end{matrix}$\end{tiny}}}{\lim} F_{X,Y}(y,y) = 1-e^{-y}- y e^{-y}.  \]
				
				\item On rappelle que $F_{X,Y}$ admet une densité s'il existe une fonction $f_{X,Y} : \RR^{2} \rightarrow \RR_{\geq 0}$ 
				intégrable telle que
				\[     F_{X,Y}(x,y) = \int_{- \infty}^{x} \int_{- \infty}^{y} f_{X,Y}(s,t) dt ds,     \]
				pour tout $(x,y) \in \RR^{2}$.  
				Dans ce cas, on voit bien que 
				\[     f_{X,Y}(x,y)= \begin{cases} e^{-y} , &\text{si $(x,y) \in D$,} 
				\\
				0, &\text{si $(x,y) \in \RR^{2} \setminus D$,} 
				\end{cases}
				\] 
				est une densité associée à la fonction de répartition $F_{X,Y}$, 
				
				\item C'est facile à voir que 
				\[     f_{X}(x)= \begin{cases} e^{-x}, &\text{si $x \in \RR_{\geq 0}$,} 
				\\
				0, &\text{si $x \in \RR_{< 0}$,} 
				\end{cases}
				\] 
				est une densité associée à la fonction de répartition $F_{X}$, tandis que 
				\[     f_{Y}(y)= \begin{cases} y e^{-y}, &\text{si $y \in \RR_{\geq 0}$,} 
				\\
				0, &\text{si $y \in \RR_{< 0}$,} 
				\end{cases}
				\] 
				est une densité associée à la fonction de répartition $F_{Y}$. 
				
				\item On voit bien que 
				\[     \mathbb{P}\big(\{X \leq 1, Y>2\}\big) = \mathbb{P}\big(\{X \leq 1\}\big) - \mathbb{P}\big(\{X \leq 1, Y \leq 2\}\big) 
				= F_{X}(1) - F_{X,Y}(1,2) = e^{-2}      \]
				et 
				\[     \mathbb{P}\big(\{Y>2\}\big) = 1 - \mathbb{P}\big(\{Y \leq 2\}\big) = 1 - F_{Y}(2) = 3 e^{-2}.      \] 
				Cela nous dit que 
				\[     \mathbb{P}\big(\{X \leq 1 \} | \{ Y>2\}\big) = \frac{\mathbb{P}\big(\{X \leq 1, Y>2\}\big)}{\mathbb{P}\big(\{Y>2\}\big)} = \frac{1}{3}.     \]
				
				\item Soit $B = \{ (x,y) \in \RR^{2} : y \leq 2 x \}$. 
				On veut calculer $\mathbb{P}((X,Y) \in B)$, \textit{i.e.} 
				\begin{align*}
				\mathbb{P}\big(\{Y \leq 2 X \}\big) &= \underset{B}{\iint} f_{X,Y}(x,y) dx dy  = \int_{0}^{+ \infty} \int_{x}^{2x} e^{-y} dy dx = \int_{0}^{+ \infty} (e^{-x} - e^{-2x}) dx 
				\\
				&= \underset{A \rightarrow + \infty}{\lim} \bigg[ \frac{e^{-2 x}}{2} -e^{-x} \bigg]_{0}^{A} = \frac{1}{2}.    
				\end{align*}      
			\end{enumerate}
		\end{preuve}  
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $(X,Y)$ un couple de variables aléatoires réelles à valeurs dans l'ensemble 
		$D=\{(x,y) \in \RR^2 : 0\leq x\leq y\leq 1\}$ admettant une densité donnée par 
		\[     f(x,y)=\frac{\mathbb{1}_D(x,y)}{y}.     \]
		\begin{enumerate}
			\item Les variables aléatoires $X$ et $Y$ sont-elles indépendantes ?
			\item Soit $U=X/Y$ et $V=Y$.
			Déterminer la fonction de répartition de $(U,V)$.
			\item Les variables aléatoires $U$ et $V$ sont-elles indépendantes ?
		\end{enumerate}
		
		\begin{preuve} 
			\begin{enumerate}
				\item On remarque d'abord que $f$ est intégrable sur $\RR^{2}$. 
				En effet, on voit bien que $x \mapsto f(x,y)$ est intégrable sur $\RR$, pour tout $y \in \RR$, et 
				la fonction $y \mapsto \int_{\RR} f(x,y) dx = \mathbb{1}_{[0,1]}(y)$ est absolument intégrable sur $\RR$. 
				Le théorème de Fubini implique alors que $f$ est intégrable sur $\RR^{2}$. 
				
				Comme $f$ est intégrable sur $\RR^{2}$, le théorème de Fubini nous dit que 
				les densités de $X$ et $Y$ existent et elles sont données par 
				\[     f_{X}(x) = \int_{- \infty}^{+ \infty} f_{X,Y}(x,y) dy \text{ et } f_{Y}(y) = \int_{- \infty}^{+ \infty} f_{X,Y}(x,y) dx,     \]
				respectivement, pour tous $x,y \in \RR \setminus Z$, où $Z$ est un ensemble négligeable. 
				Dans ce cas, $f_{X}(x) = 0$ si $x \in \RR \setminus [ 0,1 ]$ et 
				\[     f_{X}(x) = \int_{x}^{1} \frac{1}{y} dy = - \ln (x),      \]
				pour tout $x \in \hskip 0.6mm ] \hskip 0.6mm 0,1 ]$. 
				On pose $f_{X}(1) = 0$. 
				La fonction $f_{X}$ est intégrable par le théorème de Fubini. 
				En outre, $f_{Y}(y) = 0$ si $y \in \RR \setminus [ 0,1 ]$ et
				\[     f_{Y}(y) = \int_{0}^{y} \frac{1}{y} dx = 1,     \]
				pour tout $y \in [0,1]$. 
				
				Les variables $X$ et $Y$ sont indépendantes si et seulement si $f_{X,Y}(x,y) = f_{X}(x) f_{Y}(y)$, pour tout $(x,y) \in \RR^{2} \setminus Z'$, 
				où $Z$ est un ensemble négligeable. 
				Dans ce cas, $f_{X}(x) f_{Y}(y) = - \ln(x)$, pour tout $(x,y) \in [0,1]^{2}\setminus D$, ce qui est impossible. 
				Comme $[0,1]^{2}\setminus D$ a mesure positive, on conclut que $X$ et $Y$ ne sont pas indépendantes. 
				
				\item On voit bien que 
				\[     F_{U,V}(u,v) = \mathbb{P}\big(\{U \leq u, V \leq v\}\big) = \mathbb{P}\big(\{Y \leq u X, Y \leq v\}\big) = \underset{D_{u,v}}{\iint} f(x,y) dx dy,     \]
				où $D_{u,v} = \{ (x,y) \in \RR_{\geq 0}^{2} : y \leq u x, y \leq v \}$. 
				Cela implique que $F_{U,V}(u,v) = 0$ si $u \leq 1$ ou $v \leq 0$. 
				Si $u > 1$ et $v > 0$, alors 
				\begin{align*}
				\hskip -0.6cm     F_{U,V}(u,v) = \underset{D_{u,v}}{\iint}  dx dy = \int_{0}^{\min(v,1)} \int_{y/u}^{y} \frac{1}{y} dx dy = \int_{0}^{\min(v,1)} \frac{u-1}{u} dy = \frac{(u-1)\min(v,1)}{u}.  
				\end{align*}
				
				\item On voit bien que 
				\[     F_{U}(u) = \mathbb{P}\big(\{U \leq u\}\big) = \mathbb{P}\big(\{Y \leq u X\}\big) = \underset{D_{u}}{\iint} f(x,y) dx dy,     \]
				où $D_{u} = \{ (x,y) \in \RR_{\geq 0}^{2} : y \leq u x \}$. 
				Cela implique que $F_{U}(u) = 0$ si $u \leq 1$. 
				Si $u > 1$, alors 
				\begin{align*}
				F_{U}(u) = \underset{D_{u}}{\iint}  dx dy = \int_{0}^{1} \int_{y/u}^{y} \frac{1}{y} dx dy = \int_{0}^{1} \frac{u-1}{u} dy = \frac{u-1}{u}.  
				\end{align*}
				En outre, on sait que $F_{V}(v) = F_{Y}(v) = 0$ si $y \leq 0$, et 
				\[     F_{V}(v) = \int_{0}^{v} f_{Y}(y) dy = \int_{0}^{\min(v,1)} 1 dy = \min(v,1),     \]  
				si $y \geq 0$.
				Cela nous dit que $F_{U,V}(u,v) = F_{U}(u) F_{V}(v)$, pour tout $(u,v) \in \RR^{2}$, ce qui nous dit que $U$ et $V$ 
				sont indépendantes.
			\end{enumerate}
		\end{preuve}   
		
		%%%%%%%%%%%%%%%
		\exo{Loi de Bendford}
		Soit $E=\{1,\dots,9\}$.
		Pour $k\in E$, soit $p_k=\log_{10}(1+1/k)$.
		\begin{enumerate}
			\item Montrer que $p_1,\dots ,p_9$ définissent une probabilité sur $E$.
			
			\item Dessiner son histogramme et sa fonction de répartition.
			
			\item Relever dans le journal $1000$ nombres correspondant aux chiffres de la bourse et calculer la fréquence du premier chiffre de ces nombres. 
			Comparer avec les probabilités précédentes.
			
			\item Soit $(X,Y)$ la variable aléatoire à valeurs dans $E\times (E\cup\{0\})$ définie par 
			\[     \mathbb{P}\Big(\big\{ (X,Y)=(x,y) \big\}\Big) = \log_{10}\bigg(1+\frac{1}{10x+y}\bigg),     \]
			pour tout $(x,y) \in E\times (E\cup\{0\})$. 
			Donner la loi de $X$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item C'est clair que 
				\[      \sum_{i=1}^{9} p_{i} = \sum_{i=1}^{9} \log_{10}\bigg(\frac{k+1}{k}\bigg) = \log_{10}\bigg(\prod_{i=1}^{9} \frac{k+1}{k}\bigg) = 
				\log_{10}\bigg(\frac{10}{1}\bigg) = 1.     \]
				
				\item La fonction de répartition est donné par 
				\[     F(x) = \begin{cases}
				0, &\text{si $x \in \RR_{<0}$},
				\\
				\sum_{i=1}^{\lfloor x \rfloor} p_{i}, &\text{si $x \in \RR_{\geq 0}$},
				\end{cases}
				\]
				où $\lfloor x \rfloor$ est la partie entière de $x$. 
				On laisse à l'étudiant le dessin de l'histogramme de la fonction de masse et de la fonction de répartition.
				
				\item On laisse cette partie de l'exercice à l'étudiant. 
				
				\item On voit bien que  
				\begin{align*}     \mathbb{P}\big( \{ X=x \} \big) &= \sum_{y=0}^{9} \mathbb{P}\Big(\big\{ (X,Y)=(x,y) \big\}\Big) = \sum_{y=0}^{9} \log_{10}\bigg(1+\frac{1}{10x+y}\bigg) 
				\\
				&= \log_{10}\bigg(\prod_{i=0}^{9} \frac{10 x + y + 1}{10x+y} \bigg) = \log_{10}\bigg(\frac{10 (x + 1)}{10x} \bigg) =  \log_{10}\bigg(\frac{x + 1}{x} \bigg),      
				\end{align*}
				pour tout $x \in E$, et zéro sinon. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Loi de Benford (suite)}
		Soit $x$ un nombre réel positif.
		\begin{enumerate}
			\item Montrer qu'il peut s'écrire de façon unique sous la forme $x=y 10^n$, où $n \in \Z$ et $1\leq y<10$. 
			On parle d'écriture scientifique.
			
			\item Montrer que $n$ est donné par la partie entière de $\log_{10} x$ et donc que $\log_{10} y$ est la partie fractionnaire de 
			$\log_{10} x$, \textit{i.e.} le nombre moins sa partie entière.
			
			\item On suppose que $X$ est une variable aléatoire à valeurs dans $\RR_{> 0}$.
			Soit $X=Y10^N$ son écriture scientifique.
			On dira que la loi de $X$ est \emph{invariante par changement d'échelle} si pour tout $\lambda >0$, si $\lambda X=Y'10^{N'}$ est l'écriture scientifique de $\lambda X$, alors $Y$ et $Y'$ ont la même loi.
			On peut montrer que ceci est réalisé si et seulement si $\log_{10} Y$ suit la loi uniforme sur $[0,1]$.
			On va montrer le sens facile.
			\begin{enumerate}[label=(\roman*)]
				\item Soit $\lambda = \lambda_{0} 10^{n_{0}}$ l'écriture scientifique associée. 
				Exprimer $\log_{10} Y'$ en fonction de $\lambda_{0}$ et de $\log_{10} Y$.
				\item Soit $a \in [0,1]$ et soit $Z$ une variable aléatoire de loi uniforme sur $[0,1]$.
				Montrer que la partie fractionnaire de $U = a+Z$ suit la loi uniforme sur $[0,1]$. 
				\\
				\textbf{Indication :} calculer la fonction de répartition de la partie fractionnaire de $U$.
				
				\item On suppose que $\log_{10}(Y)$ suit la loi uniforme sur $[0,1]$. 
				Soit $\lambda > 0$ et $\lambda X=Y'10^{N'}$ comme ci-dessous. 
				Montrer que $\log_{10}(Y')$ suit la loi uniforme sur $[0,1]$. 
				
				\item Montrer que si $\log_{10}(Y)$ suit la loi uniforme sur $[0,1]$, alors la partie entière de $Y$ suit la loi de Benford.
			\end{enumerate}
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On montre d'abord l'unicité. 
				Soient $n, n' \in \Z$ et $y, y' \in [1,10 \hskip 0.6 mm [ \hskip 0.6mm$ tels que $x = y 10^{n} = y' 10^{n'}$. 
				On suppose sans perte de généralité que $n < n'$. 
				Cela implique que $y = y' 10^{n'-n} \geq 10^{n'-n} \geq 10$, ce qui est absurde. 
				En conséquence $n=n'$, ce qui implique que $y = y'$. 
				
				Pour montrer l'existence, on considère l'ensemble $A_{x} = \{ m \in \Z : x.10^{-m} < 10 \}$. 
				C'est clair que $A_{x}$ n'est pas vide, puisque $x/10^{m}$ tend vers zéro quand $m$ tend vers $+ \infty$, et que $A_{x}$ est minoré, puisque $x/10^{m}$ tend vers $+ \infty$ quand $m$ tend vers $- \infty$. 
				Soit $n = \min \{ m \in \Z : x.10^{-m} < 10 \}$. 
				Alors $x.10^{-n} < 10$. 
				Si $x.10^{-n} < 1$, alors $x.10^{-(n-1)} < 10$, ce qui implique que $n$ n'est pas le minimum de $A_{x}$. 
				En conséquence, $x.10^{-n} \geq 1$. 
				Soit $y = x.10^{-n}$. 
				On conclut que $x = y 10^{n}$, avec $y \in [1,10 \hskip 0.6 mm [ \hskip 0.6mm$ et $n \in \Z$. 
				
				\item Si $x = y 10^{n}$, alors $\log_{10}(x) = \log_{10}(y) + n$, avec $\log_{10}(y) \in [0,1 \hskip 0.6 mm [ \hskip 0.6mm$. 
				En conséquence, $n = \lfloor \log_{10}(x) \rfloor$. 
				Cela implique que $\log_{10}(y) = \log_{10}(x) - \lfloor \log_{10}(x) \rfloor$ est la partie fractionnaire de $\log_{10} x$. 
				
				\item 
				\begin{enumerate}[label=(\roman*)]
					\item Comme $Y' 10^{N'} = \lambda X = \lambda Y 10^{N} = \lambda_{0} Y 10^{N + n_{0}}$, on voit que 
					\[     \log_{10}(Y') + N' = \log_{10}(\lambda_{0})+ \log_{10}(Y) + N + n_{0},     \]
					ce qui implique que $\log_{10}(Y') = \lfloor \log_{10}(\lambda_{0})+ \log_{10}(Y) \rfloor$. 
					
					\item Soit $M = \lfloor U \rfloor$ et $V = U - M$.
					C'est clair que la fonction de répartition $F_{V}$ de $V$ satisfait que $F_{V}(v) = 0$ si $v < 0$ et $F_{V}(v)=1$ si $v \geq 1$. 
					Soit $v \in [0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
					On voit bien que $\{ V \leq v \} = \{ 1 - a \leq Z \leq 1-a+v \} \sqcup \{ Z \leq v - a \}$. 
					L'union est disjointe puisque $v < 1$. 
					Cela implique que 
					\begin{align*}
					F_{V}(v) &= \mathbb{P}\big( \{ 1 - a \leq Z \leq 1-a+v \} \big) + \mathbb{P}\big(\{ Z \leq v -a \} \big) 
					\\
					&= \min(1+v-a,1) - (1-a) + \max(v-a,0) = v.    
					\end{align*} 
					En conséquence, $V$ suit la loi uniforme sur $[0,1]$. 
					
					\item Comme $\log_{10}(\lambda_{0}) \in [0,1]$ et $\log_{10}(Y) \rfloor$ suit la loi uniforme sur $[0,1]$, l'item précédent nous dit que 
					$\log_{10}(Y') = \lfloor \log_{10}(\lambda_{0})+ \log_{10}(Y) \rfloor$ suit la loi uniforme sur $[0,1]$. 
					
					\item Soit $i \in [\![ 1, 9 ]\!]$. 
					On voit bien que 
					\[     \mathbb{P}\big( \{ \lfloor Y \rfloor \leq i \} \big) = \mathbb{P}\big( \{ Y < i \} \big) = \mathbb{P}\big( \{ \log_{10}(Y) < \log_{10}(i) \} \big) = 
					\log_{10}(i) = p_{i},     \]
					où l'on tuilise la notation de l'exercice précédent. 
					Comme $p_{1} + \dots + p_{9} = 1$, $\lfloor Y \rfloor$ est une variable aléatoire discrète qui suit la loi de Benford. 
				\end{enumerate}
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Loi sans mémoire}
		Soit $X$ une variable aléatoire à valeurs dans $\RR_{> 0}$ telle que 
		\[     \mathbb{P}\big(\{X>x+y\} | \{X>x\}\big)=\mathbb{P}\big(\{X>y\}\big),     \]
		pour tous $x,y \in \RR_{> 0}$.  
		
		\begin{enumerate}
			\item Montrer que 
			\[     \mathbb{P}\big(\{X>x+y\}\big) =\mathbb{P}\big(\{X>x\}\big)\mathbb{P}\big(\{X>y\}\big),     \]
			pour tous $x,y \in \RR_{> 0}$.  
			
			\item Soit $\varphi : \RR_{\geq 0} \rightarrow \RR$ définie par $\varphi(x)=\mathbb{P}(\{X>x\})$.
			Notons $a =\varphi(1)$. 
			On a donc $a\in [0,1]$.
			Montrer que pour tout $n\in\NN^{*}$, $\varphi(n)=a^n$ et que pour tout $q\in\Q_{\geq 0}$, $\varphi(q)=a^q$.
			Peut-on avoir $a=0$ ? Et $a = 1$ ?
			
			\item Montrer que $\varphi$ est une fonction décroissante.
			En utilisant le fait que tout réel est limite d'une suite décroissante (respectivement, croissante) de rationnels, 
			déterminer $\varphi(x)$ pour $x\in\RR_{\geq 0}$.
			
			\item Déterminer la loi de $X$.
		\end{enumerate}
		
		\begin{preuve} 
			\begin{enumerate}
				\item On voit bien que 
				\[     \mathbb{P}\big(\{X>x+y\}\big) = \mathbb{P}\big(\{X>x+y\} | \{X>x\}\big)\mathbb{P}\big(\{X>x\}\big)= \mathbb{P}\big(\{X>y\}\big)\mathbb{P}\big(\{X>y\}\big),     \]
				pour tous $x,y \in \RR_{> 0}$.  
				
				\item On remarque d'abord que, comme $X$ est une variable aléatoire à valeurs dans $\RR_{> 0}$, 
				$\varphi(0) = \mathbb{P}(\{ X > 0 \}) = 1$. 
				%En particulier, $\varphi(0) = 1 = a^{0}$. 
				L'item précédent nous dit que 
				\begin{equation}
				\label{eq:rec}  
				\varphi((n+1)x) = \varphi(nx) \varphi(x), 
				\end{equation}   
				pour tout $x \in \RR_{>0}$ et $n \in \NN^{*}$. 
				En particulier, un argument par récurrence nous dit que 
				\begin{equation}
				\label{eq:rec2}  
				\varphi(nx) = \varphi(x)^{n},  
				\end{equation}   
				pour tout $x \in \RR_{>0}$ et $n \in \NN^{*}$. 
				Si $x = 1$, on trouve que $\varphi(n)=a^n$, pour tout $n\in\NN^{*}$. 
				
				Par ailleurs, si $q = m/n \in \Q$, avec $(m, n) \in \NN \times \NN^{*}$, alors 
				\[     \varphi(q)^{n} = \varphi(n q) = \varphi(m) = \varphi(1)^{m},     \]
				ce qui nous dit que $\varphi(q) = \varphi(1)^{m/n}$. 
				
				Le cas $a = 0$ est impossible. 
				En effet, par définition de $X$, l'espace probabilisé sous-jacent $\Omega$ satisfait que 
				\[     \Omega = \bigcup_{n \in  \NN^{*}} \{ X > 1/n \}.     \] 
				Comme la suite d'évènements $(\{ X > n \})_{n \in  \NN^{*}}$ est croissante, alors
				\[    1 = \mathbb{P}(\Omega) = \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\bigg(\{ X > 1/n \} \bigg) 
				= \underset{n \rightarrow + \infty}{\lim} \varphi(1/n) = \underset{n \rightarrow + \infty}{\lim} 0^{1/n} = 0,     \]
				ce qui est absurde. 
				On va montrer aussi que $a \neq 1$. 
				En effet, si $a = 1$, $\varphi(n)=1$, pour tout $n\in\NN^{*}$. 
				La suite d'évènements $(\{ X > n \})_{n \in  \NN^{*}}$ étant décroissante, on conclut que
				\[    1 = \underset{n \rightarrow + \infty}{\lim} 1 = \underset{n \rightarrow + \infty}{\lim} \mathbb{P}\big( \{ X > n \} \big) = \mathbb{P}\bigg(\bigcap_{n \in \NN^{*}} \{ X > n \} \bigg) = \mathbb{P}(\emptyset) = 0,     \]
				ce qui est absurde. 
				En conséquence, $0 < a < 1$. 
				
				\item Comme $\{ X > x\} \subseteq \{ X > y \}$ si $x \geq y$, on voit bien que $\varphi$ est décroissante. 
				Soit $x \in \RR_{>0}$ et soient $(x_{n})_{n \in \NN}, (y_{n})_{n \in \NN} \in \Q_{>0}^{\NN}$ deux suites telles que $x_{n} \leq x \leq y_{n}$ 
				pour tout $n \in \NN$ et $|y_{n} - x_{n}|$ tend vers zéro quand $n$ tend vers $+ \infty$. 
				Alors, $\varphi(y_{n}) \leq \varphi(x) \leq \varphi(x_{n})$ pour tout $n \in \NN$ nous dit que 
				\[      \underset{n \rightarrow + \infty}{\lim} a^{y_{n}} = \underset{n \rightarrow + \infty}{\lim} \varphi(y_{n}) \leq \varphi(x) \leq \underset{n \rightarrow + \infty}{\lim} \varphi(x_{n}) = \underset{n \rightarrow + \infty}{\lim} a^{x_{n}},     \] 
				ce qui implique que $\varphi(x) = a^{x}$. 
				
				\item On conclut que $F_{X}(x) = \mathbb{P}(\{ X \leq x \}) = 1 - \mathbb{P}(\{ X > x \}) = 1 - \varphi(x) = 1 - a^{x} = 1 - e^{-\lambda x}$, 
				pour tout $x > 0$ et $F_{X}(x)=0$ si $x < 0$, où $\lambda = \ln(-a) \in \RR_{>0}$. 
				Alors, $X$ admet une densité donnée par $f_{X} = \lambda e^{-\lambda x} \mathbb{1}_{\RR_{>0}}(x)$, pour tout $x \in \RR$, 
				\textit{i.e.} $X$ est une variable aléatoire de loi exponentielle de paramètre $\lambda$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $F$ une fonction strictement croissante et continue d'un intervalle ouvert $I$ dans $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
		Soit $U$ une variable aléatoire de loi uniforme sur $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$. 
		Déterminer la loi de $X = F^{-1}(U)$.
		Utiliser ceci pour construire à partir d'une loi uniforme une variable aléatoire $Y$ de loi exponentielle de paramètre $\lambda>0$.
		
		\begin{preuve} 
			On note d'abord que, comme $F : I \rightarrow \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$ est strictement monotone et continue, elle est bijective. 
			On définit la fonction $\bar{R} : \RR \rightarrow [0,1]$ donnée par $\bar{F}(x) = 0$ si $x \leq \inf I$, $\bar{F}(x) = F(x)$ si $x \in I$, 
			et $\bar{F}(x) = 1$ si $x \geq \sup I$, où la première (resp., troisième) condition est vide si $\inf I = - \infty$ (resp., $\sup I = + \infty$). 
			En outre, $F$ étant strictement croissante, on a $\{ F^{-1}(U) \leq x \} = \{ U \leq F(x) \}$, pour tout $x \in I$, ce qui dit que 
			\[     F_{X}(x) = \mathbb{P}\big(\{ F^{-1}(U) \leq x \} \big) = \mathbb{P}\big(\{ U \leq F(x) \} \big) = F(x),     \]
			pour tout $x \in I$, où $F_{X}$ est la fonction de répartition de $X$. 
			Par définition de fonction de répartition, on voit que $F_{X}(x) = 0$ si $x \leq \inf I$ et $\bar{F}(x) = 1$ si $x \geq \sup I$. 
			En conséquence, $F_{X} = \bar{F}$. 
			
			Pour construire une variable aléatoire $Y$ de loi exponentielle de paramètre $\lambda>0$ à partir de al variable aléatoire $U$ de loi uniforme $\hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$, on pose $F : \RR_{> 0} \rightarrow \hskip 0.6mm ] \hskip 0.6mm 0,1 \hskip 0.6mm [ \hskip 0.6mm$ via $F(x) = 1 - e^{-\lambda x}$, pour tout $x > 0$. 
			C'est clair que $F$ est différentiable (et, en particulier, continue) et $F'(x) = \lambda e^{- \lambda x} > 0$, pour tout $x > 0$, ce qui nous dit que $F$ est strictement croissante. 
			Alors $Y = F^{-1}(U)$ a fonction de répartition $\bar{R} : \RR \rightarrow [0,1]$ donnée par $\bar{F}(x) = 0$ si $x \leq 0$, et 
			$\bar{F}(x) = F(x)$ si $x > 0$, \textit{i.e.} $Y$ est une variable aléatoire de loi exponentielle de paramètre $\lambda>0$. 
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{Une formule pour l'espérance}
		Soit $X$ une variable aléatoire positive admettant une espérance. 
		On suppose que la loi de $X$ admet une densité $f$ et on note $F$ sa fonction de répartition.
		Montrer que si $x>0$, 
		\[     x\big(1-F(x)\big) \leq \int_x^{+\infty} tf(t)dt.     \]
		En déduire que $x(1-F(x))$ tend vers 0 quand $x$ tend vers $+\infty$.
		Montrer que 
		\[     E[X]=\int_0^{+\infty}\big(1-F(t)\big)dt.     \] 
		%\\
		%\textbf{Indication :} penser à la formule d'intégration par parties.
		
		\begin{preuve} 
			On note d'abord que $f(x) = 0$ si $x < 0$. 
			On rappelle que $X$ admet une espérance si et seulement si $x \mapsto x f(x)$ est une fonction intégrable sur $\RR_{\geq 0}$. 
			On voit bien que 
			\[     \int_x^{+\infty} tf(t)dt \geq x \int_x^{+\infty} f(t)dt = x\bigg(1-\int_{-\infty}^{x} f(t)dt\bigg) = x\big(1-F(x)\big) \geq 0,     \] 
			pour tout $x>0$. 
			Comme $X$ admet une espérance, \textit{i.e.} $\int_{0}^{+B} t f(t)dt$ converge quand $B$ tend vers $+ \infty$, alors $\int_x^{+\infty} tf(t)dt$ tend vers zéro quand $x$ tend vers $+ \infty$. 
			L'inégalité précédente nous dit alors que $x(1-F(x))$ tend vers 0 quand $x$ tend vers $+\infty$. 
			
			Finalement, 
			\begin{align*}
			E[X] &= \int_0^{+\infty} xf(x) dx = \int_0^{+\infty} \int_{0}^{x} f(x) dy dx = \underset{D}{\iint} f(x) dx dy 
			\\
			&= \int_0^{+\infty} \int_y^{+\infty} f(x) dx dy = \int_0^{+\infty} \big(1-F(y)\big)dy,     
			\end{align*}
			où $D = \{ (x,y) \in \RR^{2} : 0 < y < x \}$ et on a utilisé le théorème de Fubini. 
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} 
		Soit $X$ une variable aléatoire de loi uniforme sur $[-1,1]$.
		Donner la loi de $X^2$, son espérance et sa variance.
		
		\begin{preuve}
			On voit bien que la fonction de répartition de $X^{2}$ satisfait que $F_{X^{2}}(y) = 0$ si $y < 0$, puisque $\{ X^{2} \leq y \} = \emptyset$ si $y < 0$. 
			En outre, 
			\[     F_{X^{2}}(y) = \mathbb{P}\big( \{ X^{2} \leq y \} \big) = \mathbb{P}\big( \{ -\sqrt{y} \leq X \leq \sqrt{y} \} \big) = \int_{-\sqrt{y}}^{\sqrt{y}} f_{X}(x) dx,     \]
			pour tout $y \in \RR_{\geq 0}$, où $f_{X}$ est la densité de $X$. 
			Cela nous dit que 
			\[     F_{X^{2}}(y) = \begin{cases}
			0, &\text{si $y \in \RR_{<0}$},
			\\
			\sqrt{y}, &\text{si $y \in [0,1]$},
			\\
			1, &\text{si $y \in \RR_{\geq 1}$}.
			\end{cases}
			\]
			En conséquence, la densité de $X^{2}$ est 
			\[     f_{X^{2}}(y) = \frac{1}{2\sqrt{y}} \mathbb{1}_{\hskip 0.6mm]\hskip 0.6mm 0,1\hskip 0.6mm[\hskip 0.6mm}(y),     \]
			pour tout $y \in \RR$. 
			
			Par ailleurs, 
			\[     E[X^{2}] = \int_{\RR} y f_{X^{2}}(y) dy = \int_{0}^{1} \frac{\sqrt{y}}{2} dy = \bigg[ \frac{\sqrt{y^{3}}}{3} \bigg]_{0}^{1} = \frac{1}{3}.     \]
			En outre, 
			\begin{align*}
			\operatorname{Var}[X^{2}] &= \int_{\RR} \bigg(y - \frac{1}{3}\bigg)^{2} f_{X^{2}}(y) dy = \int_{0}^{1} \bigg(\frac{\sqrt{y^{3}}}{2} - \frac{\sqrt{y}}{3} + \frac{1}{18 \sqrt{y}} \bigg) dy 
			\\
			&= \bigg[ \frac{\sqrt{y}}{45} (9 y^{2} - 10 y + 5) \bigg]_{0}^{1} = \frac{4}{45}.     
			\end{align*}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Paradoxe de Bertrand : Choisir une corde au hasard}
		\begin{enumerate}
			\item Choisir un point $P$ au hasard à l'intérieur d'un cercle de rayon $a$.
			Soit $X$ la longueur de la corde dont $P$ est le milieu.
			Montrer que $\mathbb{P}(\{X>\sqrt{3}a\})=\frac{1}{4}$.
			
			\item On choisit deux points $A$ et $B$ au hasard et indépendamment l'un de l'autre sur un cercle de rayon $a$. 
			Soit $X$ la longueur de la corde $AB$.
			Montrer que $\mathbb{P}(\{X>\sqrt{3}a\})=\frac{1}{3}$. 
		\end{enumerate}
		
		\begin{preuve}
			On considère le disque $D = \{ (x,y) \in \RR^{2} : x^{2} + y^{2} \leq a^{2} \}$ et sa frontière $S = \{ (x,y) \in \RR^{2} : x^{2} + y^{2} = a^{2} \}$. 
			Une corde quelconque du cercle $S$ est donnée par le segment $L_{A,B} = \{ t A + (1-t) B : t \in [0,1] \}$, avec $A, B \in S$. 
			C'est clair que $L_{A,B} = L_{A',B'}$ si et seulement si $(A,B) = (A',B')$ ou $(A,B) = (B',A')$. 
			On note $\Omega = \{ L_{A,B} : A, B \in S \}$ l'ensemble formé des cordes du cercle $S$ et 
			$\Omega_{0} = \{ L_{A,-A} : A \in S \} \subseteq \Omega$. 
			On considère l'application $X : \Omega \rightarrow \RR$ donnée par $X(L_{A,B}) = || B - A ||$, où l'on a utilisé la norme euclidienne de $\RR^{2}$. 
			Comme $X(L_{A,B}) = X(L_{B,A})$, $X$ est bien définie. 
			
			\begin{enumerate}
				\item Soit $\varphi : \Omega \rightarrow D$ l'application donnée par $\varphi(L_{A,B}) = (A+B)/2$. 
				Comme $\varphi(L_{A,B}) = \varphi(L_{B,A})$, $\varphi$ est bien définie. 
				En plus, c'est facile à voir que $\varphi(\Omega_{0}) = (0,0)$ et que la restriction 
				$\varphi|_{\Omega \setminus \Omega_{0}}$ est injective avec image $D \setminus \{ (0,0) \}$, que l'on notera $\dot{D}$. 
				Noter que $\varphi$ est une application surjective.  
				Dans ce cas, l'espace probabilisé est $\Omega$ muni de la tribu $\mathscr{B}_{1} = \{ \varphi^{-1}(E) : E \subseteq D \text{ est borélien} \}$, 
				et de la probabilité 
				\[     \mathbb{P}_{1}\big(\varphi^{-1}(E)\big) = \frac{\operatorname{Aire}(E)}{\operatorname{Aire}(D)}.     \] 
				Cela implique que l'on identifie l'action ``prendre une corde au hasard'' avec l'action ``prendre le milieu de la corde au hasard''. 
				C'est facile à vérifier que $X$ est une variable aléatoire pour cet espace probabilisé, 
				\textit{i.e.} $X^{-1}([a,b]) \in \mathscr{B}_{1}$, pour tous $a \leq b$. 
				Comme $L_{A,B}$ est orthogonal au segment formé par l'origine $(0,0)$ et $(A+B)/2$, 
				on conclut que $\varphi(\{X>\sqrt{3}a\}) = \{ (x,y) \in D : x^{2} + y^{2} < a^{2}/4 \}$, ce qui nous dit que 
				\[     \mathbb{P}_{1}\big(\{X>\sqrt{3}a\}\big) = \frac{\pi a^{2}/4}{\pi a^{2}}= \frac{1}{4}.     \]
				
				\item Soit $\psi : \RR \rightarrow S$ l'application donnée par $\psi(\theta) = (a\cos(\theta),a\sin(\theta))$. 
				C'est clair que $\psi|_{[0, 2\pi \hskip 0.6mm [ \hskip 0.6mm}$ est une bijection. 
				Soit $\Psi : [0, 2\pi \hskip 0.6mm [ \hskip 0.6mm^{2} \rightarrow \Omega$ l'application donnée par $\Psi(\theta,\theta') = L_{\psi(\theta),\psi(\theta+\theta')}$. 
				Noter que que $\Psi$ est surjective. 
				%Si $T \subseteq [0, 2\pi \hskip 0.6mm [ \hskip 0.6mm^{2}$, on définit $T^{\sigma} = \{ (x,y) \in \RR^{2} : (y,x) \in T \}$.
				Soit $\mathscr{T} = \{ T \subseteq [0, 2\pi \hskip 0.6mm [ \hskip 0.6mm^{2} : T \text{ est borélien} \}$. 
				Dans ce cas, l'espace probabilisé est $\Omega$ muni de la tribu $\mathscr{B}_{2} = \{ \Psi(T) : T \in \mathscr{T} \}$, 
				et de la probabilité 
				\[     \mathbb{P}_{2}\big(\Psi(T)\big) = \frac{\operatorname{Aire}(T)}{4 \pi^{2}}.     \] 
				On remarque que $\mathscr{B}_{2}$ est une tribu, puisque $\emptyset = \Psi(\emptyset)$, $\Psi(\cup_{i\in I}T_{i})= \cup_{i\in I} \Psi(T_{i})$ pour toute famille $(T_{i} )_{i\in I} \in \mathscr{T}^{I}$, et 
				\[     \Omega \setminus \Psi(T) = \Psi\Big([0, 2\pi \hskip 0.6mm [ \hskip 0.6mm^{2} \setminus \Psi^{-1}\big(\Psi(T)\big)\Big),      \]
				pour tout $T \in \mathscr{T}$, vu que $\Psi$ est une application surjective.
				Ce modèle donne une façon d'identifier l'action ``prendre une corde au hasard'' avec l'action ``prendre les extrémités de la corde au hasard''. 
				C'est facile à vérifier que $X$ est une variable aléatoire pour cet espace probabilisé, 
				\textit{i.e.} $X^{-1}([a,b]) \in \mathscr{B}_{2}$, pour tous $a \leq b$. 
				Dans ce cas, on voit bien que $\Psi^{-1}(\{X>\sqrt{3}a\}) = \{ (\theta,\theta') \in [0, 2\pi \hskip 0.6mm [ \hskip 0.6mm^{2} : |\theta'| < 2 \pi/3 \}$, ce qui nous dit que 
				\[     \mathbb{P}_{2}\big(\{X>\sqrt{3}a\}\big) = \frac{4 \pi^{2}/3}{4 \pi^{2}}= \frac{1}{3}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{} \textit{Vont-ils se rencontrer ?}
		Paul et Virgine arrivent dans le parc indépendamment l'un de l'autre et de manière uniforme entre $12$h et $13$h. 
		Chacun attend un quart d'heure et s'en va si l'autre n'est pas là.
		On pose la question : Quelle est la probabilité qu'ils se rencontrent ?
		En notant $X$ l'heure d'arrivée de Virginie et $Y$ l'heure d'arrivée de Paul, traduire mathématiquement sur $X$ et $Y$ les hypothèses.
		Répondre ensuite à la question.
		
		\begin{preuve}
			On suppose que $X$ et $Y$ sont variables aléatoires indépendantes qui suivent la loi uniforme sur $[12,13]$. 
			Cela implique que la fonction de densité de $(X,Y)$ est $\mathbb{1}_{[12,13] \times [12,13]}$. 
			On veut calculer $\mathbb{P}(\{ |X-Y| \leq 1/4 \})$. 
			C'est facile à voir que 
			\[     \mathbb{P}\big(\{ |X-Y| \leq 1/4 \}\big) = \underset{D}{\iint} dx dy = \operatorname{Aire}(D),     \]
			où $D = \{ (x,y) \in [12,13] \times [12,13] : |x-y| \leq 1/4 \}$. 
			C'est facile à voir que l'aire de $D$ est 
			l'aire du carré $[12,13] \times [12,13]$ moins les aires de $2$ triangles rectangles isocèles de côte de longueur $3/4$, \textit{i.e.}
			\[     \operatorname{Aire}(D) = 1 - \frac{9}{16} = \frac{7}{16}.     \]
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Quelques calculs avec la loi uniforme}
		Soient $X$ et $Y$ deux variables aléatoires réelles indépendantes de loi uniforme sur $[0,1]$.
		On définit les variables aléatoires $U=\inf(X,Y)$ et $V=\sup(X,Y)$.
		\begin{enumerate}
			\item Déterminer la fonction de répartition $F_{U,V}$ du couple $(U,V)$.
			En déduire la densité $f_{U,V}=2 \mathbb{1}_T$, où  $T= \{(u,v)\in \RR^{2} : 0\leq u\leq v\leq 1\}$. 
			Est-ce que $U$ et $V$ sont indépendantes ?
			
			\item Quelles sont les densités $f_U$ et $f_V$ des lois de $U$ et $V$ ? 
			En déduire $E[U]$ et $E[V]$. %[Réponses : $E[U]=1/3$ et $E[V]=2/3$].
			
			\item Quelle est la densité de $S=U+V$ ? $E[S]$ ? 
			Quelle est la loi de $X+Y$ ?
			
			\item Calculer $\operatorname{Var}[U]$, $\operatorname{Var}[V]$, $\operatorname{Cov}(U,V)$ et $\rho_{U,V}$.
			%[Réponses : $V(U)=V(V)=1/18$ ; $\cov(U,V)=1/36$; $\rho(U,V)=1/2$].
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On remarque d'abord que la fonction de répartition $F$ d'une variable aléatoire de loi uniforme sur $[0,1]$ est 
				$F(z) = \min(\max(z,0),1) = \max(\min(z,1),0)$, pour tout $z \in \RR$. 
				Comme 
				\begin{align*}
				\{ U \leq u, V \leq v\} &= \underset{\{ U \leq u \}}{\underbrace{\big(\{ X \leq u \} \cup \{ Y \leq u \}\big)}} \cap \underset{\{ V \leq v \}}{\underbrace{\big(\{ X \leq v \} \cap \{ Y \leq v \}\big)}} 
				\\
				&= \big\{ X \leq \min(u,v), Y \leq v \big\} \cup \big\{ X \leq v, Y \leq \min(u,v) \big\}    
				\end{align*}
				et 
				\[     \big\{ X \leq \min(u,v), Y \leq v \big\} \cap \big\{ X \leq v, Y \leq \min(u,v) \big\} = \big\{ X \leq \min(u,v), Y \leq \min(u,v) \big\},     \]
				alors 
				\begin{align*}
				F_{U,V}(u,v) &= F_{X,Y}\big(\min(u,v), v\big) + F_{X,Y}\big(v,\min(u,v)\big) - F_{X,Y}\big(\min(u,v), \min(u,v)\big) 
				\\
				&= F_{X}\big(\min(u,v)\big) F_{Y}\big(v\big) + F_{X}\big(v\big) F_{Y}\big(\min(u,v)\big) - F_{X}\big(\min(u,v)\big)^{2}
				\\
				&= 2 \max\big(\min(u,v,1),0\big) \max\big(\min(v,1),0\big) -  \max\big(\min(u,v,1),0\big)^{2},  
				\end{align*}
				où l'on a utilisé que $X$ et $Y$ sont indépendantes dans la deuxième égalité, et l'expression générique de la fonction de répartition d'une variable aléatoire de loi uniforme sur $[0,1]$ dans la dernière égalité. 
				En particulier, si $v \leq 0$ ou $u \leq 0$, $F_{U,V}(u,v) = 0$. 
				Par ailleurs, si $0\leq u, v$, alors 
				\begin{align*}
				F_{U,V}(u,v) = 2 \min(u,v,1) \min(v,1) -  \min(u,v,1)^{2},  
				\end{align*}
				ce qui dit que, si $u,v \geq 1$, $F_{U,V}(u,v) = 1$. 
				En outre, c'est clair que, si $0\leq v\leq u$, alors $F_{U,V}(u,v) = \min(v,1)^{2}$, tandis que 
				si $0\leq u \leq 1 \leq v$, $F_{U,V}(u,v) = u (2-u)$, et si $0\leq u\leq v \leq 1$, $F_{U,V}(u,v) = u (2v-u)$. 
				
				On va montrer que la densité du couple $(U,V)$ est donnée par $f_{U,V}=2 \mathbb{1}_T$, où  $T= \{(u,v)\in \RR^{2} : 0\leq u\leq v\leq 1\}$. 
				En effet, c'est clair que 
				\[     \int_{- \infty}^{u} \int_{- \infty}^{v} 2 \mathbb{1}_T(s,t) dt ds = \begin{cases}
				0, &\text{si $u \leq 0$ ou $v \leq 0$},
				\\
				u^{2} + 2 u \big(\min(v,1)-u\big), &\text{si $0 \leq u \leq v$ et $u \leq 1$},
				\\
				v^{2}, &\text{si $0 \leq v \leq u$ et $v \leq 1$},
				\\
				1, &\text{si $u, v \geq 1$},
				\end{cases} 
				\]
				ce qui coïncide avec la $F_{U,V}$ décrite ci-dessus. 
				
				Finalement, comme 
				\[     F_{U,V}(3/4,1/4) = \frac{1}{16} \neq \frac{15}{16^{2}} = \frac{15}{16} \frac{1}{16} = F_{U}(3/4) F_{V}(1/4),     \] 
				on conclut que $U$ et $V$ ne sont pas indépendantes. 
				
				\item Comme
				\[     f_U(u) = \int_{- \infty}^{+ \infty} f_{U,V}(u,v) dv,     \]
				pour tout $u \in \RR \setminus Z$, avec $Z$ négligeable, 
				c'est clair que l'application $f_{U}(u) = 0$, si $u \notin [0,1]$, et $f_{U}(u) = 2(1-u)$, si $u \in [0,1]$, est une densité de $U$. 
				Le même argument nous dit que 
				\[     f_V(v) = \int_{- \infty}^{+ \infty} f_{U,V}(u,v) du,     \]
				pour tout $v \in \RR \setminus Z'$, avec $Z'$ négligeable, 
				ce qui nous dit que l'application $f_{V}(v) = 0$, si $v \notin [0,1]$, et $f_{V}(v) = 2v$, si $v \in [0,1]$, est une densité de $V$.
				
				On voit bien que 
				\[     E[U] = \int_{0}^{1} u f_{U}(u) du = \int_{0}^{1} 2(u- u^{2}) du = \frac{1}{3}     \]
				et 
				\[     E[V] = \int_{0}^{1} v f_{V}(v) dv = \int_{0}^{1} 2v^{2} dv = \frac{2}{3}.     \]
				
				\item On voit que $S = U + V = X + Y$, ce qui nous dit que 
				\[     F_{S}(s) = \mathbb{P}\big(\{ S \leq s \}\big) = \underset{D_{s}}{\iint} f_{X,Y}(x,y) dx dy,     \]
				où $D_{s} = \{ (x,y) \in \RR^{2} : x + y \leq s \}$. 
				En conséquence, $F_{S}(s) = 0$ si $s \leq 0$, $F_{S}(s) = 1$ si $s \geq 2$, $F_{S}(s) = \frac{s^{2}}{2}$ si $s \in [0,1]$, et 
				\[     F_{S}(s) = 1 - \frac{(2-s)^{2}}{2} = 2 s - 1 - \frac{s^{2}}{2},     \]
				si $s \in [1,2]$.
				En conséquence, la densité de $S$ est 
				\[     f_{S}(s) = \begin{cases}
				0, &\text{si $s \in \RR \setminus [0,2]$},
				\\
				s, &\text{si $s \in [0,1]$},
				\\
				2-s, &\text{si $s \in [1,2]$}. 
				\end{cases}
				\]     
				Noter que $E[S] = E[U] + E[V] = 1$. 
				
				\item Comme $\operatorname{Var}[U] = E[U^{2}] - E[U]^{2}$, il suffit de calculer 
				\[     E[U^{2}] = \int_{0}^{1} u^{2} f_{U}(u) du = \int_{0}^{1} 2(u^{2}- u^{3}) du = \frac{1}{6},     \] 
				ce qui donne $\operatorname{Var}[U] = 1/18$. 
				De même, d'après $\operatorname{Var}[V] = E[V^{2}] - E[V]^{2}$, il suffit de calculer 
				\[     E[V^{2}] = \int_{0}^{1} v^{2} f_{V}(v) du = \int_{0}^{1} 2v^{3} dv = \frac{1}{2},     \] 
				ce qui donne $\operatorname{Var}[V] = 1/18$. 
				
				Par ailleurs, comme $\operatorname{Cov}(U,V) = E[U V] - E[U] E[V]$, on calcule d'abord 
				\[     E[U V] = \int_{0}^{1} \int_{0}^{1} u v f_{U,V}(u,v) du dv = 2 \int_{0}^{1} \int_{0}^{v} u v du dv = \frac{1}{4},     \] 
				ce qui implique que $\operatorname{Cov}(U,V) = 1/36$.
				
				Finalement, $\rho_{U,V} = \operatorname{Cov}(U,V)/\sqrt{\operatorname{Var}[U] \operatorname{Var}[V]} = 1/2$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Problème de l'aiguille de Buffon}
		Une aiguille de longueur $\ell$ est jetée ``au hasard" sur un plan qui est strié par des parallèles 
		(\textit{i.e.} les rainures du parquet) situées à distance $d>\ell$ les unes des autres.
		Soit $X$ la variable aléatoire donnée par la distance du milieu de l'aiguille à la parallèle la plus proche et $\Theta$ celle donnée
		par l'angle orienté entre une strie et l'aiguille.
		On traduit l'hypothèse ``jeter au hasard'' par le fait que le couple $(X,\Theta)$ suit la loi uniforme sur $[0,d/2]\times [0,\pi ]$. Quelle probabilité a-t-on que l'aiguille coupe une parallèle ?
		
		\begin{preuve}
			Il faut calculer 
			\[     \mathbb{P}\big( \{ X \leq \frac{d}{2} \sin(\Theta) \} \big) = \frac{2}{\pi d} \underset{D}{\iint} dx d\theta = \frac{2}{\pi d} \operatorname{Aire}(D),     \]
			où $D = \{ (\theta, x) \in [0,\pi ] \times [0,d/2] : 2x \leq d \sin(\theta)  \}$. 
			C'est clair que 
			\[     \operatorname{Aire}(D) = \int_{0}^{\pi} \frac{d}{2} \sin(\theta) d\theta = \frac{d}{2} \bigg[ - \cos(\theta) \bigg]_{0}^{\pi} = d.     \]
			Cela nous dit que la probabilité demandée est $2/\pi$. 
		\end{preuve}
		
		
		%%%%%%%%%%%%%%%
		\exo{Rapport de deux exponentielles}
		Soient $X$ et $Y$ deux variables aléatoires réelles indépendantes suivant les lois exponentielles de paramètres
		$\lambda > 0$ et $\mu >0$, respectivement. 
		Déterminer la fonction de répartition et la densité de la variable aléatoire réelle $U=Y/X$.
		
		\begin{preuve}
			On va calculer $F_{U}(u) = \mathbb{P}(\{U \leq u\})$, pour tout $u \in \RR$. 
			Comme $\{U \leq u\}$ est vide, pour tout $u \leq 0$, puisque $\{X \leq z\}= \{Y \leq z\} = \emptyset$ pour tout $z \leq 0$, on voit que $F_{U}(u) = 0$, si $u \in \RR_{\leq 0}$. 
			Par ailleurs, si $u > 0$, 
			\[     F_{U}(u) = \mathbb{P}\big(\{U \leq u\}\big) = \mathbb{P}\big(\{Y \leq u X\}\big) = \underset{D_{u}}{\iint} f_{X,Y}(x,y) dx dy,     \]
			où $D_{u} = \{ (x,y) \in \RR_{>0}^{2} : y \leq u x \}$. 
			Comme $X$ et $Y$ sont variables aléatoires indépendantes, $f_{X,Y}(x,y) = f_{X}(x) f_{Y}(y)$, pour tout $(x,y) \in \RR^{2}$. 
			Alors, $f_{X,Y}(x,y) = \mathbb{1}_{\RR_{>0}^{2}}(x,y) \lambda \mu e^{-\lambda x - \mu y}$, pour tout $(x,y) \in \RR^{2}$, et 
			\begin{align*}
			F_{U}(u) &= \underset{D_{u}}{\iint} \lambda \mu e^{-\lambda x - \mu y} dx dy = \lambda \mu \int_{0}^{+ \infty} \int_{y/u}^{+ \infty} e^{-\lambda x - \mu y} dx dy 
			\\
			&= \mu \int_{0}^{+ \infty} e^{-(\mu + \lambda/u) y} dy = \frac{\mu u}{\lambda + \mu u},    
			\end{align*}
			pour tout $u > 0$. 
			
			La densité de $U$ est donnée par 
			\[     f_{U}(u) = \begin{cases}
			0, &\text{si $u \in \RR_{\leq 0}$},
			\\
			\frac{\lambda \mu}{(\lambda + u \mu)^{2}}, &\text{si $u \in \RR_{> 0}$}.
			\end{cases}
			\]
		\end{preuve} 
		
		%%%%%%%%%%%%%%%
		\exo{}
		Soit $f$ la densité de probabilité d'une variable aléatoire réelle $Z>0$. 
		On pose
		\[     g(x,y)=\frac{1}{x+y}f(x+y) \mathbb{1}_{\{x>0,y>0\}}.     \]
		\begin{enumerate}
			\item Montrer que $g$ est une densité d'un couple $(X,Y)$ de variables aléatoires réelles (strictement) positives. 
			
			\item Exprimer $E[X]$, $E[Y]$, $\operatorname{Var}[X]$, $\operatorname{Var}[Y]$ et $\operatorname{cov}(X,Y)$ à l'aide de $E[Z]$ et de $E[Z^2]$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On rappelle qu'une fonction $g : \RR^{2} \rightarrow \RR$ est une densité d'un couple de variables aléatoires réelles (resp., strictement positives) si et seulement si $\operatorname{Im}(g) \subseteq \RR_{\geq 0}$, $g$ est intégrable et 
				\[     \underset{\RR^{2}}{\iint} g(x,y) dx dy = 1 \text{ \big(resp., avec $g(x,y) = 0$ pour tout $(x,y) \in \RR^{2} \setminus \RR^{2}_{>0}$\big)}.     \]
				Dans ce cas, il suffit de montrer que 
				\[     \underset{\RR^{2}}{\iint} g(x,y) dx dy = 1,     \]
				ce qui est vérifié puisque 
				\begin{align*}
				\underset{\RR^{2}}{\iint} g(x,y) dx dy &= \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{1}{x+y}f(x+y) dx dy = \frac{1}{2} \int_{0}^{+ \infty} \int_{-u}^{u} \frac{f(u)}{u} dv du 
				\\
				&= \int_{0}^{+ \infty} f(u) du = 1,     
				\end{align*}
				où l'on a utilisé le changement de variables $\Phi : U \rightarrow \RR^{2}_{>0}$ donné par 
				\[     \Phi(u,v) = \bigg(\frac{u+v}{2},\frac{u-v}{2}\bigg),     \]
				pour tout $(u,v) \in U$, avec $U = \{ (u,v) \in \RR^{2} : 0 < u, -u < v < u \}$. 
				Noter que la valeur absolue du déterminant de la matrice jacobienne de $\Phi$ en $(u,v)$ est $1/2$, pour tout $(u,v) \in U$. 
				
				\item On voit bien que 
				\begin{align*}
				E[X] &= \underset{\RR^{2}}{\iint} x g(x,y) dx dy = \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{x}{x+y}f(x+y) dx dy 
				\\
				&= \frac{1}{4} \int_{0}^{+ \infty} \int_{-u}^{u} (u+v) \frac{f(u)}{u} dv du = \frac{1}{2} \int_{0}^{+ \infty} u f(u) du  = \frac{1}{2} E[Z]     
				\end{align*}
				et 
				\begin{align*}
				E[Y] &= \underset{\RR^{2}}{\iint} y g(x,y) dx dy = \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{y}{x+y}f(x+y) dx dy 
				\\
				&= \frac{1}{4} \int_{0}^{+ \infty} \int_{-u}^{u} (u-v) \frac{f(u)}{u} dv du = \frac{1}{2} \int_{0}^{+ \infty} u f(u) du  = \frac{1}{2} E[Z],     
				\end{align*}
				où l'on a utilisé le changement de variables de l'item précédent et l'identité 
				\begin{equation}
				\label{eq:use} 
				\int_{0}^{+ \infty} \int_{-u}^{u} v h(u) dv du = \int_{0}^{+ \infty} \bigg[ v^{2} \bigg]_{-u}^{u} \frac{h(u)}{2} dv du = \int_{0}^{+ \infty} 0 \frac{h(u)}{2} du =  0,    
				\end{equation}
				pour toute fonction $h : \RR_{\geq 0} \rightarrow \RR$ intégrable. 
				
				En outre, comme $\operatorname{Var}[X] = E[X^{2}] - E[X]^{2}$ et $\operatorname{Var}[Y] = E[Y^{2}] - E[Y]^{2}$, il suffit de calculer 
				\begin{align*} 
				E[X^{2}] &= \underset{\RR^{2}}{\iint} x^{2} g(x,y) dx dy = \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{x^{2}}{x+y}f(x+y) dx dy
				\\
				&= \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u+v)^{2} \frac{f(u)}{u} dv du = \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u^{2}+2uv+v^{2}) \frac{f(u)}{u} dv du 
				\\
				&= \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u^{2} + v^{2}) \frac{f(u)}{u} dv du = 
				\frac{1}{4} \int_{0}^{+ \infty}  \bigg(u^{2}+\frac{u^{2}}{3}\bigg) f(u) du 
				\\
				&= \frac{1}{3} \int_{0}^{+ \infty}  u^{2} f(u) du = \frac{1}{3} E[Z^{2}] 
				\end{align*}
				et
				\begin{align*} 
				E[Y^{2}] &= \underset{\RR^{2}}{\iint} y^{2} g(x,y) dx dy = \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{y^{2}}{x+y}f(x+y) dx dy
				\\
				&= \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u-v)^{2} \frac{f(u)}{u} dv du = \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u^{2}-2uv+v^{2}) \frac{f(u)}{u} dv du 
				\\
				&= \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u^{2} + v^{2}) \frac{f(u)}{u} dv du = 
				\frac{1}{4} \int_{0}^{+ \infty}  \bigg(u^{2}+\frac{u^{2}}{3}\bigg) f(u) du 
				\\
				&= \frac{1}{3} \int_{0}^{+ \infty}  u^{2} f(u) du = \frac{1}{3} E[Z^{2}],
				\end{align*}
				où l'on a utilisé le changement de variables de l'item précédent et \eqref{eq:use} dans la cinquième égalité .  
				En conséquence, 
				\begin{align*}
				\operatorname{Var}[X] &= E[X^{2}] - E[X]^{2} = \operatorname{Var}[Y] =  E[Y^{2}] - E[Y]^{2} 
				\\
				&= \frac{1}{3} E[Z^{2}] - \frac{1}{4} E[Z]^{2} = \frac{1}{3} \operatorname{Var}[Z] +\frac{1}{12} E[Z]^{2}. 
				\end{align*}
				
				Finalement, comme $\operatorname{Cov}(X,Y) = E[X Y] - E[X] E[Y]$, il suffit de calculer 
				\begin{align*} 
				E[X Y] &= \underset{\RR^{2}}{\iint} x y g(x,y) dx dy = \int_{0}^{+ \infty} \int_{0}^{+ \infty} \frac{x y}{x+y}f(x+y) dx dy
				\\
				&= \frac{1}{8} \int_{0}^{+ \infty} \int_{-u}^{u} (u^{2}-v^{2}) \frac{f(u)}{u} dv du 
				= \frac{1}{4} \int_{0}^{+ \infty}  \bigg(u^{2}-\frac{u^{2}}{3}\bigg) f(u) du 
				\\
				&= \frac{1}{6} \int_{0}^{+ \infty}  u^{2} f(u) du = \frac{1}{6} E[Z^{2}], 
				\end{align*}
				ce qui nous dit que 
				\[     \operatorname{Cov}(X,Y) = E[X Y] - E[X] E[Y] = \frac{1}{6} E[Z^{2}] - \frac{1}{4} E[Z]^{2} = \frac{1}{6} \operatorname{Var}[Z] - \frac{1}{12} E[Z]^{2}.     \]
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Discrétisation}
		Soit $X : \Omega \rightarrow \RR$ une variable aléatoire à valeurs réelles. 
		On suppose que la loi de $X$ admet une densité $f$ sur $\RR$ et on note $F$ sa fonction de répartition.
		Soit $\delta>0$. 
		On définit la variable aléatoire $X_\delta  : \Omega \rightarrow \RR$ par $X_\delta(\omega)=n\delta$, si 
		$n\in \Z$ et $\omega \in \Omega$ satisfont que $(n-1)\delta<X(\omega)\leq n\delta$. 
		\begin{enumerate}
			\item Déterminer la loi de $X_\delta$. 
			Donner la fonction de répartition $F_\delta$ de $X_\delta$ et montrer que $F_\delta$ converge simplement vers $F$ quand $\delta$ tend vers $0$.
			\item On suppose que $X$ admet une espérance. 
			Montrer que $X_\delta$ admet une espérance $m_\delta$ et que $m_\delta$ tend vers $E[X]$ quand $\delta$ tend vers $0$.
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item On note d'abord que 
				\[     \mathbb{P}\big( \{ X_{\delta} = n \delta \} \big) = F_{X}(n \delta) - F_{X}\big((n-1) \delta\big),     \]
				ce qui implique que 
				\[     F_\delta(n \delta) = \sum_{m = - \infty}^{n}  \mathbb{P}\big( \{ X_{\delta} = m \delta \} \big) = \sum_{m = - \infty}^{n} \Big(F(n \delta) - F\big((n-1) \delta\big) \Big) = F(n \delta),     \]
				pour tout $n \in \Z$.
				En conséquence, 
				\[     \sum_{n \in \Z}  \mathbb{P}\big( \{ X_{\delta} = n \delta \} \big) = \underset{n \rightarrow + \infty}{\lim} F(n \delta) = 1.     \]
				Cela nous dit que $X_\delta$ est une variable aléatoire discrète. 
				
				En outre, $F_\delta(x) = F_{\delta}(n \delta) = F(n \delta)$, où $n \in \Z$ est donné par $(n-1)\delta<x\leq n\delta$. 
				Pour tout $x \in \RR$, soit $n \in \Z$ tel que $(n-1)\delta<x\leq n\delta$. 
				En conséquence, $0 \leq n \delta - x \leq \delta$ et $|F_{\delta}(x) - F(x)| = |F(n \delta) - F(x)|$. 
				Comme $F$ est continue à droite, on conclut que $|F_{\delta}(x) - F(x)| = |F(n \delta) - F(x)|$ tend vers zéro, quand $\delta$ 
				tend vers zéro, \textit{i.e.} $F_\delta$ converge simplement vers $F$ quand $\delta$ tend vers $0$.
				
				\item La variable $X_{\delta}$ est une approximation supérieure de Lebesgue de $X$. 
				Son espérance $m_{\delta}$ est une sommes supérieure de Lebesgue de $X$. 
				Comme $X$ admet une espérance, par définition, cette somme supérieure de Lebesgue $m_{\delta}$ 
				est finie à partir d'un certain rang de $\delta$ et elle converge vers $E[X]$ quand $\delta$ tend vers $0$.
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Régression linéaire}
		Soit $X$ et $Y$ deux variables aléatoires réelles  de variances non nulles.
		
		On pose 
		\[     \varrho_{X,Y}=\frac{\operatorname{Cov}(X,Y)}{\sigma_X\sigma_Y}, \hskip 1mm \bar{X}=X-E[X] \text{ et } \bar{Y}=Y-E[Y],     \]
		où $\sigma_X = \sqrt{\operatorname{Var}[X]}$ et $\sigma_Y = \sqrt{\operatorname{Var}[Y]}$. 
		\begin{enumerate}
			\item Montrer que $|\operatorname{Cov}(X,Y)|=|E[\bar{X}\ \bar{Y}]|\le \sigma_X\sigma_Y$.
			En déduire que $-1\le\varrho_{X,Y}\le1$.
			
			\item Montrer que $|\varrho_{X,Y}|=1$ si et seulement s'il existe $a$ non nul et $b$ tels que $\mathbb{P}(\{Y=aX+b\})=1$.
			%\\
			%\textbf{Indication :} calculer $E[(\overline{Y}+t\overline{X})^2]$.
			
			\item Préciser la valeur de $\varrho_{X,Y}$ si $X$ et $Y$ sont indépendantes.
			
			\item On cherche la meilleure approximation de $Y$ comme fonction affine de $X$ au sens des moindres carrés, 
			c'est-à-dire que l'on cherche les valeurs de $a$ et $b$ qui minimisent $E[(aX+b-Y)^2]$.
			Notons $\Phi(a,b)=E[(aX+b-Y)^2]$. 
			Montrer que $\Phi(a,b)=E[(\bar{Y}-a\bar{X})^2]+(E[Y]-(aE[X]+b))^2$.
			
			En déduire que le couple $(a_0,b_0)$ qui minimise $\Phi$ vaut
			\[     a_0=\varrho_{X,Y}\sigma_Y/\sigma_X \text{ et } b_0=E[Y]-a_0E[X].     \]
			On appelle la droite d'équation $y=a_0x+b_0$ la \emph{droite de régression linéaire} de $Y$ en $X$.
			
			\item On suppose que $(X,Y)$ suit la loi uniforme sur un ensemble de cardinal $n$, \textit{i.e.} il existe $n$ points 
			$(x_i,y_i)$ dans le plan tels que $\mathbb{P}(\{X=x_i,Y=y_i\})=1/n$ pour tout $1\leq i\leq n$ .
			Déterminer la droite de régression linéaire  de $Y$ en $X$ dans ce cas. 
		\end{enumerate}
		
		\begin{preuve}
			\begin{enumerate}
				\item Si $\sigma_X$ ou $\sigma_Y$ est infini, il n'y a rien à démontrer. 
				On suppose désormais que $\sigma_X$ ou $\sigma_Y$ sont des nombres réels. 
				Le résultat demandé s'agit d'une version du théorème de Cauchy-Schwarz, pour la forme bilinéaire positive (mais pas forcément définie) 
				$(U,V) \mapsto E(U V)$. 
				En effet, si $V$ est une variable aléatoire qui satisfait que $E[V^{2}]\neq0$, on pose $c = E[UV]/E[V^{2}]$ et on considère 
				\[     0 \leq E[(U - cV)^2] =  E[U^{2}] - \frac{E[U V]^{2}}{E[V^{2}]},     \]
				ce qui nous dit que $|E[U V]| \leq \sqrt{E[U^{2}]} \sqrt{E[V^{2}]}$. 
				C'est facile à vérifier que $c$ est en fait le minimum de la fonction quadratique $c \mapsto E[(U - cV)^2]$. 
				Si l'on utilise l'identité précédente pour $U = \bar{X}$ et $V = \bar{Y}$, on conclut que 
				$|\operatorname{Cov}(X,Y)| \leq \sigma_X\sigma_Y$ et $-1 \leq \varrho_{X,Y} \leq 1$. 
				
				\item Le résultat demandé est aussi une conséquence du théorème de Cauchy-Schwarz, pour la forme bilinéaire positive définie ci-dessus. 
				En effet, avec les hypothèse précédentes, $|E[U V]| = \sqrt{E[U^{2}]} \sqrt{E[V^{2}]}$ si et seulement si $E[(U - cV)^2] = 0$, 
				\textit{i.e.} $\mathbb{P}(\{ U = cV \})=1$. 
				Si l'on utilise l'identité précédente pour $U = \bar{X}$ et $V = \bar{Y}$, on conclut que 
				$|\operatorname{Cov}(X,Y)| = \sigma_X\sigma_Y$ si et seulement si 
				$\mathbb{P}(\{ \bar{X} = c \bar{Y} \})=1$, avec $c = \rho_{X,Y} \sigma_{X} \neq 0$. 
				Cette dernière condition est équivalente à l'existence de coefficients $a$ non nul et $b$ tels que $\mathbb{P}(\{Y=aX+b\})=1$. 
				En effet, si $\mathbb{P}(\{ \bar{X} = c \bar{Y} \})=1$, on prend $a = 1/c$ et $b = E[X]/c - E[Y]$.
				La réciproque est immédiate. 
				
				\item Si $X$ et $Y$ sont indépendantes, alors $E[X Y] = E[X] E[Y]$, ce qui implique que $\operatorname{Cov}(X,Y = 0)$, 
				et en conséquence $\varrho_{X,Y} = 0$. 
				
				\item  C'est clair que 
				\begin{align*} 
				\Phi(a,b)&=E\big[(aX+b-Y)^2\big] = E\Big[\big(a\bar{X}-\bar{Y} + b + a E[X] - E[Y]\big)^2\Big]
				\\
				&= E\big[(\bar{Y}-a\bar{X})^2\big]+E\Big[\big(E[Y]-(aE[X]+b)\big)^2\Big] 
				\\
				&\phantom{=}+ 2E\Big[\big(\bar{Y}-a\bar{X}\big)\big(E[Y]-(aE[X]+b)\big)\Big] 
				\\
				&= E\big[(\bar{Y}-a\bar{X})^2\big]+\big(E[Y]-(aE[X]+b)\big)^2.  
				\end{align*}
				C'est clair que la valeur $a_{0}$ minimise $E[(\bar{Y}-a\bar{X})^2]$, d'après l'argument dans l'item (i), 
				tandis que la valeur $b_{0}$ annule l'opérande $(E[Y]-(aE[X]+b))^{2}$. 
				
				\item Soit $R_{X} = \{ x_{i} : i \in [\![ 1 , n ]\!] \}$ et $R_{Y} = \{ y_{i} : i \in [\![ 1 , n ]\!] \}$. 
				Alors, 
				\[     \mathbb{P}\big( \{ X = x \} \big) = \sum_{\text{\begin{tiny}$\begin{matrix}i \in [\![ 1 , n ]\!] \\ x_{i} = x \end{matrix}$\end{tiny}}} \mathbb{P}\big(\{X=x_i,Y=y_i\}\big) = \frac{\#\big(\{ i \in [\![ 1 , n ]\!] : x_{i} = x \}\big)}{n}     \]
				et 
				\[     \mathbb{P}\big( \{ Y = y \} \big) = \sum_{\text{\begin{tiny}$\begin{matrix}i \in [\![ 1 , n ]\!] \\ y_{i} = y \end{matrix}$\end{tiny}}} \mathbb{P}\big(\{X=x_i,Y=y_i\}\big) = \frac{\#\big(\{ i \in [\![ 1 , n ]\!] : y_{i} = y \}\big)}{n}.     \]
				En conséquence, 
				\[     E[X] = \sum_{x \in R_{X}} x \mathbb{P}\big( \{ X = x \} \big) = \frac{\sum_{i}^{n} x_{i}}{n} \text{ et } E[Y] = \sum_{iy \in R_{Y}}^{n} y \mathbb{P}\big( \{ Y = y \} \big) = \frac{\sum_{i}^{n} y_{i}}{n}.     \]
				En outre, 
				\[     \operatorname{Var}[X] = \sum_{x \in R_{X}} x^{2} \mathbb{P}\big( \{ X = x \} \big) - E[X]^{2} = \frac{\sum_{i}^{n} x_{i}^{2}}{n} - E[X]^{2}     \]
				et  
				\[     \operatorname{Var}[Y] = \sum_{y \in R_{Y}} y^{2} \mathbb{P}\big( \{ Y = y \} \big) - E[Y]^{2} = \frac{\sum_{i}^{n} y_{i}^{2}}{n} - E[Y]^{2}.     \]
				En plus, 
				\[     E[X Y] = \sum_{i=1}^{n} x_{i} y_{i} \mathbb{P}\big(\{X=x_i,Y=y_i\}\big) = \frac{\sum_{i}^{n} x_{i} y_{i}}{n}.     \]
				Cela permet d'exprimer $a_{0}$ et $b_{0}$ en fonction de points $(x_{i},y_{i})$, pour $i \in [\![ 1 , n ]\!]$. 
			\end{enumerate}
		\end{preuve}
		
		%%%%%%%%%%%%%%%
		\exo{Une loi normale dans $ \RR^2$}
		Soit $(X,Y)$ un couple de variables aléatoires à valeurs réelles possédant une densité sur $ \RR^2$ donnée par    
		\[     f(x,y)=\frac{1}{2\pi \sqrt{1-\alpha^2}}e^{-\frac{x^2+y^2-2\alpha xy}{2(1-\alpha^2)}}.     \]
		On suppose que $-1<\alpha<1$.
		\begin{enumerate}
			\item Donner les lois de $X$ et de $Y$. 
			Calculer leur espérance et leur variance.
			\item Calculer la covariance de $(X,Y)$.
			\item Soit $Z_1=X+Y$ et $Z_2=X-Y$. 
			Donner l'espérance et la variance de $Z_1$ et $Z_2$. 
			Donner leur covariance.
			\item Calculer la loi du couple $(Z_1,Z_2)$. 
			Montrer que $Z_1$ et $Z_2$ sont indépendantes et donner leurs lois respectives.
		\end{enumerate}                                      
		
		\begin{preuve}
			\begin{enumerate}
				\item On remarque d'abord que $f$ est intégrable sur $\RR^{2}$. 
				En effet, on voit bien que $y \mapsto f(x,y)$ est intégrable sur $\RR$ pour tout $x \in \RR$, 
				puisque 
				\[     \frac{e^{-\frac{x^2}{2}}}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} e^{-\frac{(y-\alpha x)^{2}}{2(1-\alpha^2)}} dy 
				= \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}},      \]
				où l'on a utilisé que 
				\begin{equation}
				\label{eq:intga}
				\int_{- \infty}^{+ \infty} e^{-\frac{z^{2}}{2 c}} dy = \sqrt{2 \pi c},
				\end{equation}          
				pour tout $c > 0$ (voir l'exercice \textbf{19} de la fiche 3), 
				et la fonction $x \mapsto \int_{\RR} f(x,y) dy = (2 \pi)^{-1/2} e^{-x^2/2}$ est absolument intégrable sur $\RR$, d'après l'exercice mentionné. 
				Le théorème de Fubini implique alors que $f$ est intégrable sur $\RR^{2}$. 
				
				Comme $f$ est intégrable sur $\RR^{2}$, le théorème de Fubini nous dit que 
				les densités de $X$ et $Y$ existent et elles sont données par 
				\[     f_{X}(x) = \int_{- \infty}^{+ \infty} f_{X,Y}(x,y) dy \text{ et } f_{Y}(y) = \int_{- \infty}^{+ \infty} f_{X,Y}(x,y) dx,     \]
				respectivement, pour tous $x,y \in \RR \setminus Z$, où $Z$ est un ensemble négligeable. 
				Dans ce cas $Z = \emptyset$, puisque, comme on a déjà indiqué, 
				\[     f_{X}(x) = \frac{e^{-\frac{x^2}{2}}}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} e^{-\frac{(y-\alpha x)^{2}}{2(1-\alpha^2)}} dy 
				= \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}},      \]
				pour tout $x \in \RR$, et 
				\[     f_{Y}(y) = \frac{e^{-\frac{y^2}{2}}}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} e^{-\frac{(x-\alpha y)^{2}}{2(1-\alpha^2)}} dx 
				= \frac{1}{\sqrt{2 \pi}} e^{-\frac{y^2}{2}},      \]
				pour tout $y \in \RR$. 
				
				Comme $X$ et $Y$ ont la même fonction de densité, on conclut que $E[X] = E[Y]$ et $\operatorname{Var}[X] = \operatorname{Var}[Y]$. 
				En plus, c'est clair que 
				\[    E[Y] = E[X] = \int_{- \infty}^{+ \infty} \frac{x}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} dx = 0,     \]
				puisque l'intégrande est une fonctions impaires. 
				En outre, dans ce cas $\operatorname{Var}[Y] = \operatorname{Var}[X] = E[X^{2}]$, qui est donné par 
				\begin{equation}
				\label{eq:intga2}
				E[X^{2}] = \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{+ \infty} x^{2} e^{-\frac{x^2}{2}} dx = \frac{2}{\sqrt{2 \pi}} \frac{d}{dc}\bigg( \sqrt{2 \pi c} \bigg)(1) = 1,     
				\end{equation}
				où l'on a utilisé l'identité \eqref{eq:intga} dans la deuxième égalité. 
				
				\item Comme $E[X] = E[Y] = 0$, on voit bien que 
				\begin{align*}
				\operatorname{Cov}(X,Y) &= E[X Y] = \frac{1}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} \int_{- \infty}^{+ \infty} x y e^{-\frac{x^2+y^2-2\alpha xy}{2(1-\alpha^2)}} dx dy 
				\\
				&= \frac{1}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} \int_{- \infty}^{+ \infty} \big(uv + \alpha u^{2}\big) e^{-\frac{u^2}{2}} e^{-\frac{v^2}{2(1-\alpha^2)}} du dv 
				\\
				&= \frac{\alpha}{2\pi \sqrt{1-\alpha^2}} \int_{- \infty}^{+ \infty} \int_{- \infty}^{+ \infty} u^{2} e^{-\frac{u^2}{2}} e^{-\frac{v^2}{2(1-\alpha^2)}} du dv,      
				\\
				&=\frac{\alpha}{\sqrt{2\pi  (1-\alpha^2)}} \int_{- \infty}^{+ \infty} e^{-\frac{v^2}{2(1-\alpha^2)}} dv = \alpha,      
				\end{align*}
				où l'on a utilisé le changement de variables $u = x$ et $v = y - \alpha x$ dans la deuxième égalité, 
				\eqref{eq:intga2} dans la quatrième égalité, et \eqref{eq:intga} dans la dernière. 
				
				\item C'est clair que 
				\[     E[Z_{1}] = E[X+Y] = E[X] + E[Y] = 0 \text{ et } E[Z_{2}] = E[X-Y] = E[X] - E[Y] = 0.     \]
				En outre, 
				\[     \operatorname{Var}[Z_{i}] = E[Z_{i}^{2}] - E[Z_{i}]^{2} = E[X^{2}] -(-1)^{i} 2 E[X Y] + E[Y^{2}] = 2 - (-1)^{i}2 \alpha,      \]
				pour $i \in \{ 1, 2\}$. 
				Finalement, 
				\[     \operatorname{Cov}(Z_{1},Z_{2}) = E[Z_{1} Z_{2}] - E[Z_{1}] E[Z_{2}] = E[X^{2} - Y^{2}] = 0.     \]
				
				\item Comme $X = (Z_{1} + Z_{2})/2$ et $Y = (Z_{1} - Z_{2})/2$, le théorème de changement de variable nous dit que 
				\[     f_{Z_{1},Z_{2}}(z_{1},z_{2}) = \frac{1}{2} f_{X,Y}\bigg(\frac{z_{1} + z_{2}}{2}, \frac{z_{1} - z_{2}}{2}\bigg) 
				= \frac{e^{-\frac{z_{1}^2}{4(1+\alpha)}}}{2 \sqrt{\pi (1+\alpha)}} \frac{e^{-\frac{z_{2}^2}{4(1-\alpha)}}}{2 \sqrt{\pi (1-\alpha)}},     \]
				pour tous $z_{1}, z_{2} \in \RR$. 
				En employant \eqref{eq:intga}, on conclut que 
				\[     f_{Z_{1}}(z_{1}) = \int_{- \infty}^{\infty} f_{Z_{1},Z_{2}}(z_{1},z_{2}) dz_{1} = \frac{e^{-\frac{z_{1}^2}{4(1+\alpha)}}}{2 \sqrt{\pi (1+\alpha)}}   \]
				et 
				\[     f_{Z_{2}}(z_{2}) = \int_{- \infty}^{\infty} f_{Z_{1},Z_{2}}(z_{1},z_{2}) dz_{2} = \frac{e^{-\frac{z_{2}^2}{4(1-\alpha)}}}{2 \sqrt{\pi (1-\alpha)}},     \]
				pour tous $z_{1}, z_{2} \in \RR$. 
				Comme $ f_{Z_{1},Z_{2}}(z_{1},z_{2}) = f_{Z_{1}}(z_{1}) f_{Z_{2}}(z_{2})$, pour tous $z_{1}, z_{2} \in \RR$, $Z_1$ et $Z_2$ sont indépendantes. 
			\end{enumerate} 
		\end{preuve}
		
	


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
